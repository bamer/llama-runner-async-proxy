2025-11-06 22:13:12,897 - INFO - root - App file logging to: F:\llm\llama-runner-async-proxy\llama_runner\..\config\app.log
2025-11-06 22:13:12,898 - INFO - root - Development mode: forcing headless mode for better debugging
2025-11-06 22:13:12,911 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-06 22:13:12,913 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-06 22:13:12,914 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-06 22:13:12,914 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-06 22:13:12,915 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-06 22:13:12,915 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-06 22:13:12,915 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-06 22:13:12,916 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-06 22:13:12,916 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-06 22:13:12,917 - INFO - llama_runner.headless_service_manager - Waiting for all services to start...
2025-11-06 22:13:13,017 - INFO - root - Application exited with code 0.
2025-11-06 22:13:13,170 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-06 22:13:13,170 - ERROR - asyncio - Task exception was never retrieved
future: <Task finished name='Task-1' coro=<main.<locals>.run_app() done, defined at F:\llm\llama-runner-async-proxy\main.py:158> exception=SystemExit(1)>
Traceback (most recent call last):
  File "C:\ProgramData\miniconda3\Lib\site-packages\uvicorn\server.py", line 164, in startup
    server = await loop.create_server(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 1630, in create_server
    raise OSError(err.errno, msg) from None
OSError: [Errno 10048] error while attempting to bind on address ('127.0.0.1', 11434): [winerror 10048] une seule utilisation de chaque adresse de socket (protocole/adresse réseau/port) est habituellement autorisée

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\main.py", line 212, in main
    asyncio.run(run_app())
  File "C:\ProgramData\miniconda3\Lib\asyncio\runners.py", line 194, in run
    with Runner(debug=debug, loop_factory=loop_factory) as runner:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\miniconda3\Lib\asyncio\runners.py", line 62, in __exit__
    self.close()
  File "C:\ProgramData\miniconda3\Lib\asyncio\runners.py", line 70, in close
    _cancel_all_tasks(loop)
  File "C:\ProgramData\miniconda3\Lib\asyncio\runners.py", line 206, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 712, in run_until_complete
    self.run_forever()
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 683, in run_forever
    self._run_once()
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 2050, in _run_once
    handle._run()
  File "C:\ProgramData\miniconda3\Lib\asyncio\events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
  File "F:\llm\llama-runner-async-proxy\main.py", line 184, in run_app
    await hsm.start_services()
  File "F:\llm\llama-runner-async-proxy\llama_runner\headless_service_manager.py", line 163, in start_services
    await asyncio.gather(*self.running_tasks)
  File "C:\ProgramData\miniconda3\Lib\asyncio\runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\ProgramData\miniconda3\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 712, in run_until_complete
    self.run_forever()
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 683, in run_forever
    self._run_once()
  File "C:\ProgramData\miniconda3\Lib\asyncio\base_events.py", line 2050, in _run_once
    handle._run()
  File "C:\ProgramData\miniconda3\Lib\asyncio\events.py", line 89, in _run
    self._context.run(self._callback, *self._args)
  File "C:\ProgramData\miniconda3\Lib\site-packages\uvicorn\server.py", line 71, in serve
    await self._serve(sockets)
  File "C:\ProgramData\miniconda3\Lib\site-packages\uvicorn\server.py", line 86, in _serve
    await self.startup(sockets=sockets)
  File "C:\ProgramData\miniconda3\Lib\site-packages\uvicorn\server.py", line 174, in startup
    sys.exit(1)
SystemExit: 1
2025-11-08 06:27:58,108 - INFO - root - App file logging to: ./config\app.log
2025-11-08 06:27:58,693 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 06:27:58,695 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 06:27:59,052 - INFO - root - LM Studio Proxy listening on http://127.0.0.1:1234
2025-11-08 06:27:59,100 - INFO - root - Ollama Proxy listening on http://127.0.0.1:11434
2025-11-08 06:43:49,775 - INFO - root - App file logging to: ./config\app.log
2025-11-08 06:43:49,807 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 06:43:49,809 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 06:43:50,086 - INFO - root - LM Studio Proxy listening on http://127.0.0.1:1234
2025-11-08 06:43:50,115 - INFO - root - Ollama Proxy listening on http://127.0.0.1:11434
2025-11-08 06:44:33,992 - INFO - root - App file logging to: ./config\app.log
2025-11-08 06:44:33,995 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 06:44:33,997 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 06:44:33,997 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 06:44:33,998 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 06:44:33,998 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 06:44:33,998 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 06:44:33,999 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 06:44:33,999 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 06:44:33,999 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 06:44:34,000 - INFO - llama_runner.headless_service_manager - Waiting for all services to start...
2025-11-08 06:44:57,157 - INFO - root - Cache miss or invalid for Chroma1-HD-Flash-Q4_K_S (size: 5032612000). Extracting metadata...
2025-11-08 06:44:57,157 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf
2025-11-08 06:44:57,234 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:44:57,235 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[643]
2025-11-08 06:44:57,235 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:44:57,235 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[102 108 117 120]
2025-11-08 06:44:57,235 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:44:57,235 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[14]
2025-11-08 06:44:57,236 - DEBUG - root - Raw 'general.file_type' value: 14, Type: <class 'int'>
2025-11-08 06:44:57,236 - WARNING - root - Architecture-specific context_length key 'flux.context_length' not found for F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf. Using default 4096.
2025-11-08 06:44:57,236 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf
2025-11-08 06:44:57,237 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf: {'id': 'Chroma1-HD-Flash-Q4_K_S.gguf', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'flux', 'compatibility_type': 'gguf', 'quantization': 'Q4_K_S', 'max_context_length': 4096, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([643], dtype=uint64), 'GGUF.kv_count': memmap([3], dtype=uint64), 'general.architecture': memmap([102, 108, 117, 120], dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([14], dtype=uint32)}}
2025-11-08 06:44:57,239 - INFO - root - Saved metadata to cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000) at ./config\metadata_cache\Chroma1-HD-Flash-Q4_K_S_5032612000.json
2025-11-08 06:44:57,240 - INFO - root - Cache miss or invalid for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216). Extracting metadata...
2025-11-08 06:44:57,240 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M-GGUF\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[399]
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[37]
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  51]
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(25,), Value=[ 68 101 101 112 115 101 101 107  45  82  49  45  48  53  50  56  45  81
 119 101 110  51  45  56  66]
2025-11-08 06:45:04,698 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(25,), Value=[ 68 101 101 112 115 101 101 107  45  82  49  45  48  53  50  56  45  81
 119 101 110  51  45  56  66]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'general.quantized_by': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[ 85 110 115 108 111 116 104]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[56 66]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'general.repo_url': Type=<class 'numpy.memmap'>, Shape=(30,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47 117 110 115 108 111 116 104]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[36]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[131072]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4096]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[12288]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32]
2025-11-08 06:45:04,699 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8]
2025-11-08 06:45:04,701 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.key_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.value_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.scaling.type': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[121  97 114 110]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.scaling.factor': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4.]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.scaling.original_context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32768]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:04,702 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151654]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_eos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(5265,), Value=[123  37  45 ...  32  37 125]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[15]
2025-11-08 06:45:04,703 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.file': Type=<class 'numpy.memmap'>, Shape=(50,), Value=[ 68 101 101 112  83 101 101 107  45  82  49  45  48  53  50  56  45  81
 119 101 110  51  45  56  66  45  71  71  85  70  47 105 109  97 116 114
 105 120  95 117 110 115 108 111 116 104  46 100  97 116]
2025-11-08 06:45:04,704 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.dataset': Type=<class 'numpy.memmap'>, Shape=(49,), Value=[117 110 115 108 111 116 104  95  99  97 108 105  98 114  97 116 105 111
 110  95  68 101 101 112  83 101 101 107  45  82  49  45  48  53  50  56
  45  81 119 101 110  51  45  56  66  46 116 120 116]
2025-11-08 06:45:04,704 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.entries_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[252]
2025-11-08 06:45:04,704 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.chunks_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[713]
2025-11-08 06:45:04,704 - DEBUG - root - Raw 'general.file_type' value: 15, Type: <class 'int'>
2025-11-08 06:45:04,704 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M-GGUF\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf
2025-11-08 06:45:04,706 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M-GGUF\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf: {'id': 'Deepseek-R1-0528-Qwen3-8B', 'object': 'model', 'type': 'llm', 'publisher': 'Unsloth', 'arch': 'qwen3', 'compatibility_type': 'gguf', 'quantization': 'Q4_K_M', 'max_context_length': 131072, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([399], dtype=uint64), 'GGUF.kv_count': memmap([37], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  51], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 68, 101, 101, 112, 115, 101, 101, 107,  45,  82,  49,  45,  48,
         53,  50,  56,  45,  81, 119, 101, 110,  51,  45,  56,  66],
       dtype=uint8), 'general.basename': memmap([ 68, 101, 101, 112, 115, 101, 101, 107,  45,  82,  49,  45,  48,
         53,  50,  56,  45,  81, 119, 101, 110,  51,  45,  56,  66],
       dtype=uint8), 'general.quantized_by': memmap([ 85, 110, 115, 108, 111, 116, 104], dtype=uint8), 'general.size_label': memmap([56, 66], dtype=uint8), 'general.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47, 117, 110, 115,
        108, 111, 116, 104], dtype=uint8), 'qwen3.block_count': memmap([36], dtype=uint32), 'qwen3.context_length': memmap([131072], dtype=uint32), 'qwen3.embedding_length': memmap([4096], dtype=uint32), 'qwen3.feed_forward_length': memmap([12288], dtype=uint32), 'qwen3.attention.head_count': memmap([32], dtype=uint32), 'qwen3.attention.head_count_kv': memmap([8], dtype=uint32), 'qwen3.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen3.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'qwen3.attention.key_length': memmap([128], dtype=uint32), 'qwen3.attention.value_length': memmap([128], dtype=uint32), 'qwen3.rope.scaling.type': memmap([121,  97, 114, 110], dtype=uint8), 'qwen3.rope.scaling.factor': memmap([4.], dtype=float32), 'qwen3.rope.scaling.original_context_length': memmap([32768], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.bos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151654], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.ggml.add_eos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  32,  37, 125], shape=(5265,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([15], dtype=uint32), 'quantize.imatrix.file': memmap([ 68, 101, 101, 112,  83, 101, 101, 107,  45,  82,  49,  45,  48,
         53,  50,  56,  45,  81, 119, 101, 110,  51,  45,  56,  66,  45,
         71,  71,  85,  70,  47, 105, 109,  97, 116, 114, 105, 120,  95,
        117, 110, 115, 108, 111, 116, 104,  46, 100,  97, 116],
       dtype=uint8), 'quantize.imatrix.dataset': memmap([117, 110, 115, 108, 111, 116, 104,  95,  99,  97, 108, 105,  98,
        114,  97, 116, 105, 111, 110,  95,  68, 101, 101, 112,  83, 101,
        101, 107,  45,  82,  49,  45,  48,  53,  50,  56,  45,  81, 119,
        101, 110,  51,  45,  56,  66,  46, 116, 120, 116], dtype=uint8), 'quantize.imatrix.entries_count': memmap([252], dtype=uint32), 'quantize.imatrix.chunks_count': memmap([713], dtype=uint32)}}
2025-11-08 06:45:04,840 - INFO - root - Saved metadata to cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216) at ./config\metadata_cache\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M_5027785216.json
2025-11-08 06:45:04,840 - INFO - root - Cache miss or invalid for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760). Extracting metadata...
2025-11-08 06:45:04,840 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[398]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[29]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  51]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[ 77 111 100 101 108]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'general.quantized_by': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[ 85 110 115 108 111 116 104]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[52 46 48 66]
2025-11-08 06:45:11,693 - DEBUG - root - Extracted numpy array metadata for key 'general.repo_url': Type=<class 'numpy.memmap'>, Shape=(30,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47 117 110 115 108 111 116 104]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[117 110 115 108 111 116 104]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[36]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[262144]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2560]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[9728]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[5.e+06]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:45:11,694 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.key_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.value_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(5057,), Value=[123  37  45 ...  32  37 125]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:11,695 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[7]
2025-11-08 06:45:11,696 - DEBUG - root - Raw 'general.file_type' value: 7, Type: <class 'int'>
2025-11-08 06:45:11,696 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf
2025-11-08 06:45:11,697 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf: {'id': 'Model', 'object': 'model', 'type': 'llm', 'publisher': 'Unsloth', 'arch': 'qwen3', 'compatibility_type': 'gguf', 'quantization': 'Q8_0', 'max_context_length': 262144, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([398], dtype=uint64), 'GGUF.kv_count': memmap([29], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  51], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 77, 111, 100, 101, 108], dtype=uint8), 'general.quantized_by': memmap([ 85, 110, 115, 108, 111, 116, 104], dtype=uint8), 'general.size_label': memmap([52, 46, 48, 66], dtype=uint8), 'general.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47, 117, 110, 115,
        108, 111, 116, 104], dtype=uint8), 'general.tags': memmap([117, 110, 115, 108, 111, 116, 104], dtype=uint8), 'qwen3.block_count': memmap([36], dtype=uint32), 'qwen3.context_length': memmap([262144], dtype=uint32), 'qwen3.embedding_length': memmap([2560], dtype=uint32), 'qwen3.feed_forward_length': memmap([9728], dtype=uint32), 'qwen3.attention.head_count': memmap([32], dtype=uint32), 'qwen3.attention.head_count_kv': memmap([8], dtype=uint32), 'qwen3.rope.freq_base': memmap([5.e+06], dtype=float32), 'qwen3.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'qwen3.attention.key_length': memmap([128], dtype=uint32), 'qwen3.attention.value_length': memmap([128], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.bos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  32,  37, 125], shape=(5057,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([7], dtype=uint32)}}
2025-11-08 06:45:11,833 - INFO - root - Saved metadata to cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760) at ./config\metadata_cache\Dorado-WebSurf_Tool-ext.Q8_0_4280405760.json
2025-11-08 06:45:11,834 - INFO - root - Cache miss or invalid for gpt-oss-20b-MXFP4 (size: 12109565632). Extracting metadata...
2025-11-08 06:45:11,834 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\gpt-oss-20b-MXFP4-GGUF\gpt-oss-20b-MXFP4.gguf
2025-11-08 06:45:26,102 - WARNING - root - Non-blocking error extracting GGUF metadata from F:\llm\llama\models\gpt-oss-20b-MXFP4-GGUF\gpt-oss-20b-MXFP4.gguf: np.uint32(39) is not a valid GGMLQuantizationType
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 161, in extract_gguf_metadata
    reader = GGUFReader(model_path, 'r')
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\gguf\gguf_reader.py", line 182, in __init__
    self._build_tensors(offs, tensors_fields)
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\gguf\gguf_reader.py", line 325, in _build_tensors
    ggml_type = GGMLQuantizationType(raw_dtype[0])
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\enum.py", line 757, in __call__
    return cls.__new__(cls, value)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\enum.py", line 1171, in __new__
    raise ve_exc
ValueError: np.uint32(39) is not a valid GGMLQuantizationType

2025-11-08 06:45:26,102 - DEBUG - root - Metadata extraction failed for F:\llm\llama\models\gpt-oss-20b-MXFP4-GGUF\gpt-oss-20b-MXFP4.gguf. Returning minimal metadata structure.
2025-11-08 06:45:26,383 - INFO - root - Saved metadata to cache for gpt-oss-20b-MXFP4 (size: 12109565632) at ./config\metadata_cache\gpt-oss-20b-MXFP4_12109565632.json
2025-11-08 06:45:26,385 - INFO - root - Cache miss or invalid for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952). Extracting metadata...
2025-11-08 06:45:26,385 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen2.5-7B-Instruct-Q4_K_M-GGUF\Qwen2.5-7B-Instruct-Q4_K_M.gguf
2025-11-08 06:45:32,996 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[339]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[34]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(19,), Value=[ 81 119 101 110  50  46  53  32  55  66  32  73 110 115 116 114 117  99
 116]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.finetune': Type=<class 'numpy.memmap'>, Shape=(8,), Value=[ 73 110 115 116 114 117  99 116]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[ 81 119 101 110  50  46  53]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[55 66]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.license': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[ 97 112  97  99 104 101  45  50  46  48]
2025-11-08 06:45:32,997 - DEBUG - root - Extracted numpy array metadata for key 'general.license.link': Type=<class 'numpy.memmap'>, Shape=(65,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  50  46  53  45
  55  66  45  73 110 115 116 114 117  99 116  47  98 108 111  98  47 109
  97 105 110  47  76  73  67  69  78  83  69]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.name': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[ 81 119 101 110  50  46  53  32  55  66]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.organization': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 81 119 101 110]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.repo_url': Type=<class 'numpy.memmap'>, Shape=(38,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  50  46  53  45
  55  66]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 99 104  97 116]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'general.languages': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[101 110]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32768]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3584]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[18944]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:45:32,998 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[15]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:45:32,999 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(2507,), Value=[123  37  45 ...  37 125  10]
2025-11-08 06:45:33,000 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:33,000 - DEBUG - root - Raw 'general.file_type' value: 15, Type: <class 'int'>
2025-11-08 06:45:33,000 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-7B-Instruct-Q4_K_M-GGUF\Qwen2.5-7B-Instruct-Q4_K_M.gguf
2025-11-08 06:45:33,002 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-7B-Instruct-Q4_K_M-GGUF\Qwen2.5-7B-Instruct-Q4_K_M.gguf: {'id': 'Qwen2.5 7B Instruct', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'qwen2', 'compatibility_type': 'gguf', 'quantization': 'Q4_K_M', 'max_context_length': 32768, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([339], dtype=uint64), 'GGUF.kv_count': memmap([34], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  50], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 81, 119, 101, 110,  50,  46,  53,  32,  55,  66,  32,  73, 110,
        115, 116, 114, 117,  99, 116], dtype=uint8), 'general.finetune': memmap([ 73, 110, 115, 116, 114, 117,  99, 116], dtype=uint8), 'general.basename': memmap([ 81, 119, 101, 110,  50,  46,  53], dtype=uint8), 'general.size_label': memmap([55, 66], dtype=uint8), 'general.license': memmap([ 97, 112,  97,  99, 104, 101,  45,  50,  46,  48], dtype=uint8), 'general.license.link': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  50,  46,  53,  45,  55,  66,  45,
         73, 110, 115, 116, 114, 117,  99, 116,  47,  98, 108, 111,  98,
         47, 109,  97, 105, 110,  47,  76,  73,  67,  69,  78,  83,  69],
       dtype=uint8), 'general.base_model.count': memmap([1], dtype=uint32), 'general.base_model.0.name': memmap([ 81, 119, 101, 110,  50,  46,  53,  32,  55,  66], dtype=uint8), 'general.base_model.0.organization': memmap([ 81, 119, 101, 110], dtype=uint8), 'general.base_model.0.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  50,  46,  53,  45,  55,  66],
       dtype=uint8), 'general.tags': memmap([ 99, 104,  97, 116], dtype=uint8), 'general.languages': memmap([101, 110], dtype=uint8), 'qwen2.block_count': memmap([28], dtype=uint32), 'qwen2.context_length': memmap([32768], dtype=uint32), 'qwen2.embedding_length': memmap([3584], dtype=uint32), 'qwen2.feed_forward_length': memmap([18944], dtype=uint32), 'qwen2.attention.head_count': memmap([28], dtype=uint32), 'qwen2.attention.head_count_kv': memmap([4], dtype=uint32), 'qwen2.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen2.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'general.file_type': memmap([15], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.bos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  37, 125,  10], shape=(2507,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32)}}
2025-11-08 06:45:33,138 - INFO - root - Saved metadata to cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952) at ./config\metadata_cache\Qwen2.5-7B-Instruct-Q4_K_M_4683073952.json
2025-11-08 06:45:33,138 - INFO - root - Cache miss or invalid for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056). Extracting metadata...
2025-11-08 06:45:33,138 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen2.5-Coder-1.5B-Q8_0-GGUF\Qwen2.5-Coder-1.5B-Q8_0.gguf
2025-11-08 06:45:40,306 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:45:40,306 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[338]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(18,), Value=[ 81 119 101 110  50  46  53  32  67 111 100 101 114  32  49  46  53  66]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(13,), Value=[ 81 119 101 110  50  46  53  45  67 111 100 101 114]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[49 46 53 66]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.license': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[ 97 112  97  99 104 101  45  50  46  48]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.license.link': Type=<class 'numpy.memmap'>, Shape=(64,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  50  46  53  45
  67 111 100 101 114  45  49  46  53  66  47  98 108 111  98  47 109  97
 105 110  47  76  73  67  69  78  83  69]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:40,307 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.name': Type=<class 'numpy.memmap'>, Shape=(12,), Value=[ 81 119 101 110  50  46  53  32  49  46  53  66]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.organization': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 81 119 101 110]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.repo_url': Type=<class 'numpy.memmap'>, Shape=(40,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  50  46  53  45
  49  46  53  66]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 99 111 100 101]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'general.languages': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[101 110]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32768]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1536]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8960]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[12]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:40,308 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[7]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(2507,), Value=[123  37  45 ...  37 125  10]
2025-11-08 06:45:40,309 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:40,310 - DEBUG - root - Raw 'general.file_type' value: 7, Type: <class 'int'>
2025-11-08 06:45:40,310 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-Coder-1.5B-Q8_0-GGUF\Qwen2.5-Coder-1.5B-Q8_0.gguf
2025-11-08 06:45:40,312 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-Coder-1.5B-Q8_0-GGUF\Qwen2.5-Coder-1.5B-Q8_0.gguf: {'id': 'Qwen2.5 Coder 1.5B', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'qwen2', 'compatibility_type': 'gguf', 'quantization': 'Q8_0', 'max_context_length': 32768, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([338], dtype=uint64), 'GGUF.kv_count': memmap([33], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  50], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 81, 119, 101, 110,  50,  46,  53,  32,  67, 111, 100, 101, 114,
         32,  49,  46,  53,  66], dtype=uint8), 'general.basename': memmap([ 81, 119, 101, 110,  50,  46,  53,  45,  67, 111, 100, 101, 114],
       dtype=uint8), 'general.size_label': memmap([49, 46, 53, 66], dtype=uint8), 'general.license': memmap([ 97, 112,  97,  99, 104, 101,  45,  50,  46,  48], dtype=uint8), 'general.license.link': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  50,  46,  53,  45,  67, 111, 100,
        101, 114,  45,  49,  46,  53,  66,  47,  98, 108, 111,  98,  47,
        109,  97, 105, 110,  47,  76,  73,  67,  69,  78,  83,  69],
       dtype=uint8), 'general.base_model.count': memmap([1], dtype=uint32), 'general.base_model.0.name': memmap([ 81, 119, 101, 110,  50,  46,  53,  32,  49,  46,  53,  66],
       dtype=uint8), 'general.base_model.0.organization': memmap([ 81, 119, 101, 110], dtype=uint8), 'general.base_model.0.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  50,  46,  53,  45,  49,  46,  53,
         66], dtype=uint8), 'general.tags': memmap([ 99, 111, 100, 101], dtype=uint8), 'general.languages': memmap([101, 110], dtype=uint8), 'qwen2.block_count': memmap([28], dtype=uint32), 'qwen2.context_length': memmap([32768], dtype=uint32), 'qwen2.embedding_length': memmap([1536], dtype=uint32), 'qwen2.feed_forward_length': memmap([8960], dtype=uint32), 'qwen2.attention.head_count': memmap([12], dtype=uint32), 'qwen2.attention.head_count_kv': memmap([2], dtype=uint32), 'qwen2.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen2.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'general.file_type': memmap([7], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.bos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  37, 125,  10], shape=(2507,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32)}}
2025-11-08 06:45:40,445 - INFO - root - Saved metadata to cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056) at ./config\metadata_cache\Qwen2.5-Coder-1.5B-Q8_0_1646573056.json
2025-11-08 06:45:40,446 - INFO - root - Cache miss or invalid for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064). Extracting metadata...
2025-11-08 06:45:40,446 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M-GGUF\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M.gguf
2025-11-08 06:45:47,415 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:45:47,415 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[339]
2025-11-08 06:45:47,415 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[38]
2025-11-08 06:45:47,415 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:47,415 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:45:47,415 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(54,), Value=[ 77 111 100 101 108 115  32  72 117 105 104 117 105  32  65 105  32  81
 119 101 110  50  46  53  32  67 111 100 101 114  32  55  66  32  73 110
 115 116 114 117  99 116  32  65  98 108 105 116 101 114  97 116 101 100]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.finetune': Type=<class 'numpy.memmap'>, Shape=(20,), Value=[ 73 110 115 116 114 117  99 116  45  97  98 108 105 116 101 114  97 116
 101 100]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(30,), Value=[109 111 100 101 108 115  45 104 117 105 104 117 105  45  97 105  45  81
 119 101 110  50  46  53  45  67 111 100 101 114]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[55 66]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.license': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[ 97 112  97  99 104 101  45  50  46  48]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.license.link': Type=<class 'numpy.memmap'>, Shape=(88,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47 104 117 105 104 117 105  45  97 105  47  81 119 101
 110  50  46  53  45  67 111 100 101 114  45  55  66  45  73 110 115 116
 114 117  99 116  45  97  98 108 105 116 101 114  97 116 101 100  47  98
 108 111  98  47 109  97 105 110  47  76  73  67  69  78  83  69]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.name': Type=<class 'numpy.memmap'>, Shape=(25,), Value=[ 81 119 101 110  50  46  53  32  67 111 100 101 114  32  55  66  32  73
 110 115 116 114 117  99 116]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.organization': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 81 119 101 110]
2025-11-08 06:45:47,416 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.repo_url': Type=<class 'numpy.memmap'>, Shape=(53,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  50  46  53  45
  67 111 100 101 114  45  55  66  45  73 110 115 116 114 117  99 116]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 99 104  97 116]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'general.languages': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[101 110]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32768]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3584]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[18944]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'qwen2.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[17]
2025-11-08 06:45:47,417 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(2509,), Value=[123  37  45 ...  37 125  10]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.file': Type=<class 'numpy.memmap'>, Shape=(56,), Value=[ 46  47  81 119 101 110  50  46  53  45  67 111 100 101 114  45  55  66
  45  73 110 115 116 114 117  99 116  45  97  98 108 105 116 101 114  97
 116 101 100  45  71  71  85  70  95 105 109  97 116 114 105 120  46 100
  97 116]
2025-11-08 06:45:47,418 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.dataset': Type=<class 'numpy.memmap'>, Shape=(12,), Value=[103 114 111 117 112  95  52  48  46 116 120 116]
2025-11-08 06:45:47,419 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.entries_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[196]
2025-11-08 06:45:47,419 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.chunks_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[68]
2025-11-08 06:45:47,419 - DEBUG - root - Raw 'general.file_type' value: 17, Type: <class 'int'>
2025-11-08 06:45:47,419 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M-GGUF\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M.gguf
2025-11-08 06:45:47,421 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M-GGUF\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M.gguf: {'id': 'Models Huihui Ai Qwen2.5 Coder 7B Instruct Abliterated', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'qwen2', 'compatibility_type': 'gguf', 'quantization': 'Q5_K_M', 'max_context_length': 32768, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([339], dtype=uint64), 'GGUF.kv_count': memmap([38], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  50], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 77, 111, 100, 101, 108, 115,  32,  72, 117, 105, 104, 117, 105,
         32,  65, 105,  32,  81, 119, 101, 110,  50,  46,  53,  32,  67,
        111, 100, 101, 114,  32,  55,  66,  32,  73, 110, 115, 116, 114,
        117,  99, 116,  32,  65,  98, 108, 105, 116, 101, 114,  97, 116,
        101, 100], dtype=uint8), 'general.finetune': memmap([ 73, 110, 115, 116, 114, 117,  99, 116,  45,  97,  98, 108, 105,
        116, 101, 114,  97, 116, 101, 100], dtype=uint8), 'general.basename': memmap([109, 111, 100, 101, 108, 115,  45, 104, 117, 105, 104, 117, 105,
         45,  97, 105,  45,  81, 119, 101, 110,  50,  46,  53,  45,  67,
        111, 100, 101, 114], dtype=uint8), 'general.size_label': memmap([55, 66], dtype=uint8), 'general.license': memmap([ 97, 112,  97,  99, 104, 101,  45,  50,  46,  48], dtype=uint8), 'general.license.link': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47, 104, 117, 105,
        104, 117, 105,  45,  97, 105,  47,  81, 119, 101, 110,  50,  46,
         53,  45,  67, 111, 100, 101, 114,  45,  55,  66,  45,  73, 110,
        115, 116, 114, 117,  99, 116,  45,  97,  98, 108, 105, 116, 101,
        114,  97, 116, 101, 100,  47,  98, 108, 111,  98,  47, 109,  97,
        105, 110,  47,  76,  73,  67,  69,  78,  83,  69], dtype=uint8), 'general.base_model.count': memmap([1], dtype=uint32), 'general.base_model.0.name': memmap([ 81, 119, 101, 110,  50,  46,  53,  32,  67, 111, 100, 101, 114,
         32,  55,  66,  32,  73, 110, 115, 116, 114, 117,  99, 116],
       dtype=uint8), 'general.base_model.0.organization': memmap([ 81, 119, 101, 110], dtype=uint8), 'general.base_model.0.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  50,  46,  53,  45,  67, 111, 100,
        101, 114,  45,  55,  66,  45,  73, 110, 115, 116, 114, 117,  99,
        116], dtype=uint8), 'general.tags': memmap([ 99, 104,  97, 116], dtype=uint8), 'general.languages': memmap([101, 110], dtype=uint8), 'qwen2.block_count': memmap([28], dtype=uint32), 'qwen2.context_length': memmap([32768], dtype=uint32), 'qwen2.embedding_length': memmap([3584], dtype=uint32), 'qwen2.feed_forward_length': memmap([18944], dtype=uint32), 'qwen2.attention.head_count': memmap([28], dtype=uint32), 'qwen2.attention.head_count_kv': memmap([4], dtype=uint32), 'qwen2.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen2.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'general.file_type': memmap([17], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.bos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  37, 125,  10], shape=(2509,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'quantize.imatrix.file': memmap([ 46,  47,  81, 119, 101, 110,  50,  46,  53,  45,  67, 111, 100,
        101, 114,  45,  55,  66,  45,  73, 110, 115, 116, 114, 117,  99,
        116,  45,  97,  98, 108, 105, 116, 101, 114,  97, 116, 101, 100,
         45,  71,  71,  85,  70,  95, 105, 109,  97, 116, 114, 105, 120,
         46, 100,  97, 116], dtype=uint8), 'quantize.imatrix.dataset': memmap([103, 114, 111, 117, 112,  95,  52,  48,  46, 116, 120, 116],
       dtype=uint8), 'quantize.imatrix.entries_count': memmap([196], dtype=int32), 'quantize.imatrix.chunks_count': memmap([68], dtype=int32)}}
2025-11-08 06:45:47,555 - INFO - root - Saved metadata to cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064) at ./config\metadata_cache\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M_5444832064.json
2025-11-08 06:45:47,556 - INFO - root - Cache miss or invalid for Qwen2.5-Omni-3B (size: 3616087360). Extracting metadata...
2025-11-08 06:45:47,556 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen2.5-Omni-3B-GGUF\Qwen2.5-Omni-3B-Q8_0.gguf
2025-11-08 06:45:54,572 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:45:54,572 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[435]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[27]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[113 119 101 110  50 118 108]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[51 46 52 66]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.license': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[111 116 104 101 114]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.license.name': Type=<class 'numpy.memmap'>, Shape=(13,), Value=[113 119 101 110  45 114 101 115 101  97 114  99 104]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.license.link': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[76 73 67 69 78 83 69]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[109 117 108 116 105 109 111 100  97 108]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'general.languages': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[101 110]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[36]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32768]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2048]
2025-11-08 06:45:54,573 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[11008]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[16]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[7]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.rope.dimension_sections': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[16]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:45:54,574 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:45:54,575 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:45:54,575 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:45:54,575 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(1281,), Value=[123  37  32 ...  32  37 125]
2025-11-08 06:45:54,575 - DEBUG - root - Raw 'general.file_type' value: 7, Type: <class 'int'>
2025-11-08 06:45:54,575 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-Omni-3B-GGUF\Qwen2.5-Omni-3B-Q8_0.gguf
2025-11-08 06:45:54,576 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-Omni-3B-GGUF\Qwen2.5-Omni-3B-Q8_0.gguf: {'id': 'Qwen2.5-Omni-3B-Q8_0.gguf', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'qwen2vl', 'compatibility_type': 'gguf', 'quantization': 'Q8_0', 'max_context_length': 32768, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([435], dtype=uint64), 'GGUF.kv_count': memmap([27], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  50, 118, 108], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.size_label': memmap([51, 46, 52, 66], dtype=uint8), 'general.license': memmap([111, 116, 104, 101, 114], dtype=uint8), 'general.license.name': memmap([113, 119, 101, 110,  45, 114, 101, 115, 101,  97, 114,  99, 104],
       dtype=uint8), 'general.license.link': memmap([76, 73, 67, 69, 78, 83, 69], dtype=uint8), 'general.tags': memmap([109, 117, 108, 116, 105, 109, 111, 100,  97, 108], dtype=uint8), 'general.languages': memmap([101, 110], dtype=uint8), 'qwen2vl.block_count': memmap([36], dtype=uint32), 'qwen2vl.context_length': memmap([32768], dtype=uint32), 'qwen2vl.embedding_length': memmap([2048], dtype=uint32), 'qwen2vl.feed_forward_length': memmap([11008], dtype=uint32), 'qwen2vl.attention.head_count': memmap([16], dtype=uint32), 'qwen2vl.attention.head_count_kv': memmap([2], dtype=uint32), 'qwen2vl.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen2vl.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'general.file_type': memmap([7], dtype=uint32), 'qwen2vl.rope.dimension_sections': memmap([16], dtype=int32), 'general.quantization_version': memmap([2], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151643], dtype=uint32), 'tokenizer.chat_template': memmap([123,  37,  32, ...,  32,  37, 125], shape=(1281,), dtype=uint8)}}
2025-11-08 06:45:54,711 - INFO - root - Saved metadata to cache for Qwen2.5-Omni-3B (size: 3616087360) at ./config\metadata_cache\Qwen2.5-Omni-3B_3616087360.json
2025-11-08 06:45:54,712 - INFO - root - Cache miss or invalid for Qwen2.5-VL-7B-Instruct (size: 4457767808). Extracting metadata...
2025-11-08 06:45:54,712 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf
2025-11-08 06:46:01,961 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:46:01,961 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[339]
2025-11-08 06:46:01,961 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32]
2025-11-08 06:46:01,961 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[113 119 101 110  50 118 108]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(22,), Value=[ 81 119 101 110  50  46  53  45  86 108  45  55  66  45  73 110 115 116
 114 117  99 116]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.finetune': Type=<class 'numpy.memmap'>, Shape=(8,), Value=[ 73 110 115 116 114 117  99 116]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(22,), Value=[ 81 119 101 110  50  46  53  45  86 108  45  55  66  45  73 110 115 116
 114 117  99 116]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.quantized_by': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[ 85 110 115 108 111 116 104]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[55 66]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'general.repo_url': Type=<class 'numpy.memmap'>, Shape=(30,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47 117 110 115 108 111 116 104]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128000]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3584]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[18944]
2025-11-08 06:46:01,962 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'qwen2vl.rope.dimension_sections': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[16]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151654]
2025-11-08 06:46:01,963 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(1017,), Value=[123  37  32 ...  32  37 125]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[14]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.file': Type=<class 'numpy.memmap'>, Shape=(47,), Value=[ 81 119 101 110  50  46  53  45  86  76  45  55  66  45  73 110 115 116
 114 117  99 116  45  71  71  85  70  47 105 109  97 116 114 105 120  95
 117 110 115 108 111 116 104  46 100  97 116]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.dataset': Type=<class 'numpy.memmap'>, Shape=(46,), Value=[117 110 115 108 111 116 104  95  99  97 108 105  98 114  97 116 105 111
 110  95  81 119 101 110  50  46  53  45  86  76  45  55  66  45  73 110
 115 116 114 117  99 116  46 116 120 116]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.entries_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[196]
2025-11-08 06:46:01,964 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.chunks_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[691]
2025-11-08 06:46:01,964 - DEBUG - root - Raw 'general.file_type' value: 14, Type: <class 'int'>
2025-11-08 06:46:01,964 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf
2025-11-08 06:46:01,967 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen2.5-VL-7B-Instruct-GGUF\Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf: {'id': 'Qwen2.5-Vl-7B-Instruct', 'object': 'model', 'type': 'llm', 'publisher': 'Unsloth', 'arch': 'qwen2vl', 'compatibility_type': 'gguf', 'quantization': 'Q4_K_S', 'max_context_length': 128000, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([339], dtype=uint64), 'GGUF.kv_count': memmap([32], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  50, 118, 108], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 81, 119, 101, 110,  50,  46,  53,  45,  86, 108,  45,  55,  66,
         45,  73, 110, 115, 116, 114, 117,  99, 116], dtype=uint8), 'general.finetune': memmap([ 73, 110, 115, 116, 114, 117,  99, 116], dtype=uint8), 'general.basename': memmap([ 81, 119, 101, 110,  50,  46,  53,  45,  86, 108,  45,  55,  66,
         45,  73, 110, 115, 116, 114, 117,  99, 116], dtype=uint8), 'general.quantized_by': memmap([ 85, 110, 115, 108, 111, 116, 104], dtype=uint8), 'general.size_label': memmap([55, 66], dtype=uint8), 'general.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47, 117, 110, 115,
        108, 111, 116, 104], dtype=uint8), 'qwen2vl.block_count': memmap([28], dtype=uint32), 'qwen2vl.context_length': memmap([128000], dtype=uint32), 'qwen2vl.embedding_length': memmap([3584], dtype=uint32), 'qwen2vl.feed_forward_length': memmap([18944], dtype=uint32), 'qwen2vl.attention.head_count': memmap([28], dtype=uint32), 'qwen2vl.attention.head_count_kv': memmap([4], dtype=uint32), 'qwen2vl.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen2vl.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'qwen2vl.rope.dimension_sections': memmap([16], dtype=int32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151654], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  32, ...,  32,  37, 125], shape=(1017,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([14], dtype=uint32), 'quantize.imatrix.file': memmap([ 81, 119, 101, 110,  50,  46,  53,  45,  86,  76,  45,  55,  66,
         45,  73, 110, 115, 116, 114, 117,  99, 116,  45,  71,  71,  85,
         70,  47, 105, 109,  97, 116, 114, 105, 120,  95, 117, 110, 115,
        108, 111, 116, 104,  46, 100,  97, 116], dtype=uint8), 'quantize.imatrix.dataset': memmap([117, 110, 115, 108, 111, 116, 104,  95,  99,  97, 108, 105,  98,
        114,  97, 116, 105, 111, 110,  95,  81, 119, 101, 110,  50,  46,
         53,  45,  86,  76,  45,  55,  66,  45,  73, 110, 115, 116, 114,
        117,  99, 116,  46, 116, 120, 116], dtype=uint8), 'quantize.imatrix.entries_count': memmap([196], dtype=int32), 'quantize.imatrix.chunks_count': memmap([691], dtype=int32)}}
2025-11-08 06:46:02,108 - INFO - root - Saved metadata to cache for Qwen2.5-VL-7B-Instruct (size: 4457767808) at ./config\metadata_cache\Qwen2.5-VL-7B-Instruct_4457767808.json
2025-11-08 06:46:02,110 - INFO - root - Cache miss or invalid for Qwen3-1.7B-Q8_0 (size: 1834426016). Extracting metadata...
2025-11-08 06:46:02,110 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen3-1.7B-Q8_0-GGUF\Qwen3-1.7B-Q8_0.gguf
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[310]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  51]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(19,), Value=[ 81 119 101 110  51  32  49  46  55  66  32  73 110 115 116 114 117  99
 116]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'general.finetune': Type=<class 'numpy.memmap'>, Shape=(8,), Value=[ 73 110 115 116 114 117  99 116]
2025-11-08 06:46:09,105 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[ 81 119 101 110  51]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[49 46 55 66]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[28]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[40960]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2048]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[6144]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[16]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+06]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.key_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.value_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:09,106 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(4100,), Value=[123  37  45 ...  32  37 125]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:46:09,107 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[7]
2025-11-08 06:46:09,107 - DEBUG - root - Raw 'general.file_type' value: 7, Type: <class 'int'>
2025-11-08 06:46:09,107 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-1.7B-Q8_0-GGUF\Qwen3-1.7B-Q8_0.gguf
2025-11-08 06:46:09,109 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-1.7B-Q8_0-GGUF\Qwen3-1.7B-Q8_0.gguf: {'id': 'Qwen3 1.7B Instruct', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'qwen3', 'compatibility_type': 'gguf', 'quantization': 'Q8_0', 'max_context_length': 40960, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([310], dtype=uint64), 'GGUF.kv_count': memmap([28], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  51], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 81, 119, 101, 110,  51,  32,  49,  46,  55,  66,  32,  73, 110,
        115, 116, 114, 117,  99, 116], dtype=uint8), 'general.finetune': memmap([ 73, 110, 115, 116, 114, 117,  99, 116], dtype=uint8), 'general.basename': memmap([ 81, 119, 101, 110,  51], dtype=uint8), 'general.size_label': memmap([49, 46, 55, 66], dtype=uint8), 'qwen3.block_count': memmap([28], dtype=uint32), 'qwen3.context_length': memmap([40960], dtype=uint32), 'qwen3.embedding_length': memmap([2048], dtype=uint32), 'qwen3.feed_forward_length': memmap([6144], dtype=uint32), 'qwen3.attention.head_count': memmap([16], dtype=uint32), 'qwen3.attention.head_count_kv': memmap([8], dtype=uint32), 'qwen3.rope.freq_base': memmap([1.e+06], dtype=float32), 'qwen3.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'qwen3.attention.key_length': memmap([128], dtype=uint32), 'qwen3.attention.value_length': memmap([128], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.bos_token_id': memmap([151643], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  32,  37, 125], shape=(4100,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([7], dtype=uint32)}}
2025-11-08 06:46:09,242 - INFO - root - Saved metadata to cache for Qwen3-1.7B-Q8_0 (size: 1834426016) at ./config\metadata_cache\Qwen3-1.7B-Q8_0_1834426016.json
2025-11-08 06:46:09,243 - INFO - root - Cache miss or invalid for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952). Extracting metadata...
2025-11-08 06:46:09,244 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\Qwen3-4B-Thinking-2507-Q4_K_S.gguf
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[398]
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[42]
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  51]
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(22,), Value=[ 81 119 101 110  51  45  52  66  45  84 104 105 110 107 105 110 103  45
  50  53  48  55]
2025-11-08 06:46:16,336 - DEBUG - root - Extracted numpy array metadata for key 'general.version': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[50 53 48 55]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.finetune': Type=<class 'numpy.memmap'>, Shape=(8,), Value=[ 84 104 105 110 107 105 110 103]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(22,), Value=[ 81 119 101 110  51  45  52  66  45  84 104 105 110 107 105 110 103  45
  50  53  48  55]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.quantized_by': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[ 85 110 115 108 111 116 104]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[52 66]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.license': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[ 97 112  97  99 104 101  45  50  46  48]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.license.link': Type=<class 'numpy.memmap'>, Shape=(68,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  51  45  52  66
  45  84 104 105 110 107 105 110 103  45  50  53  48  55  47  98 108 111
  98  47 109  97 105 110  47  76  73  67  69  78  83  69]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.repo_url': Type=<class 'numpy.memmap'>, Shape=(30,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47 117 110 115 108 111 116 104]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.name': Type=<class 'numpy.memmap'>, Shape=(22,), Value=[ 81 119 101 110  51  32  52  66  32  84 104 105 110 107 105 110 103  32
  50  53  48  55]
2025-11-08 06:46:16,337 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.version': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[50 53 48 55]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.organization': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 81 119 101 110]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.repo_url': Type=<class 'numpy.memmap'>, Shape=(50,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  51  45  52  66
  45  84 104 105 110 107 105 110 103  45  50  53  48  55]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[117 110 115 108 111 116 104]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[36]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[262144]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2560]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[9728]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[5.e+06]
2025-11-08 06:46:16,338 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.key_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'qwen3.attention.value_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151654]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(4060,), Value=[123  37  45 ...  32  37 125]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:46:16,339 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[14]
2025-11-08 06:46:16,340 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.file': Type=<class 'numpy.memmap'>, Shape=(48,), Value=[ 81 119 101 110  51  45  52  66  45  84 104 105 110 107 105 110 103  45
  50  53  48  55  45  71  71  85  70  47 105 109  97 116 114 105 120  95
 117 110 115 108 111 116 104  46 103 103 117 102]
2025-11-08 06:46:16,340 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.dataset': Type=<class 'numpy.memmap'>, Shape=(46,), Value=[117 110 115 108 111 116 104  95  99  97 108 105  98 114  97 116 105 111
 110  95  81 119 101 110  51  45  52  66  45  84 104 105 110 107 105 110
 103  45  50  53  48  55  46 116 120 116]
2025-11-08 06:46:16,340 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.entries_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[252]
2025-11-08 06:46:16,340 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.chunks_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[79]
2025-11-08 06:46:16,340 - DEBUG - root - Raw 'general.file_type' value: 14, Type: <class 'int'>
2025-11-08 06:46:16,340 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\Qwen3-4B-Thinking-2507-Q4_K_S.gguf
2025-11-08 06:46:16,343 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\Qwen3-4B-Thinking-2507-Q4_K_S.gguf: {'id': 'Qwen3-4B-Thinking-2507', 'object': 'model', 'type': 'llm', 'publisher': 'Unsloth', 'arch': 'qwen3', 'compatibility_type': 'gguf', 'quantization': 'Q4_K_S', 'max_context_length': 262144, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([398], dtype=uint64), 'GGUF.kv_count': memmap([42], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  51], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 81, 119, 101, 110,  51,  45,  52,  66,  45,  84, 104, 105, 110,
        107, 105, 110, 103,  45,  50,  53,  48,  55], dtype=uint8), 'general.version': memmap([50, 53, 48, 55], dtype=uint8), 'general.finetune': memmap([ 84, 104, 105, 110, 107, 105, 110, 103], dtype=uint8), 'general.basename': memmap([ 81, 119, 101, 110,  51,  45,  52,  66,  45,  84, 104, 105, 110,
        107, 105, 110, 103,  45,  50,  53,  48,  55], dtype=uint8), 'general.quantized_by': memmap([ 85, 110, 115, 108, 111, 116, 104], dtype=uint8), 'general.size_label': memmap([52, 66], dtype=uint8), 'general.license': memmap([ 97, 112,  97,  99, 104, 101,  45,  50,  46,  48], dtype=uint8), 'general.license.link': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  51,  45,  52,  66,  45,  84, 104,
        105, 110, 107, 105, 110, 103,  45,  50,  53,  48,  55,  47,  98,
        108, 111,  98,  47, 109,  97, 105, 110,  47,  76,  73,  67,  69,
         78,  83,  69], dtype=uint8), 'general.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47, 117, 110, 115,
        108, 111, 116, 104], dtype=uint8), 'general.base_model.count': memmap([1], dtype=uint32), 'general.base_model.0.name': memmap([ 81, 119, 101, 110,  51,  32,  52,  66,  32,  84, 104, 105, 110,
        107, 105, 110, 103,  32,  50,  53,  48,  55], dtype=uint8), 'general.base_model.0.version': memmap([50, 53, 48, 55], dtype=uint8), 'general.base_model.0.organization': memmap([ 81, 119, 101, 110], dtype=uint8), 'general.base_model.0.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  51,  45,  52,  66,  45,  84, 104,
        105, 110, 107, 105, 110, 103,  45,  50,  53,  48,  55],
       dtype=uint8), 'general.tags': memmap([117, 110, 115, 108, 111, 116, 104], dtype=uint8), 'qwen3.block_count': memmap([36], dtype=uint32), 'qwen3.context_length': memmap([262144], dtype=uint32), 'qwen3.embedding_length': memmap([2560], dtype=uint32), 'qwen3.feed_forward_length': memmap([9728], dtype=uint32), 'qwen3.attention.head_count': memmap([32], dtype=uint32), 'qwen3.attention.head_count_kv': memmap([8], dtype=uint32), 'qwen3.rope.freq_base': memmap([5.e+06], dtype=float32), 'qwen3.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'qwen3.attention.key_length': memmap([128], dtype=uint32), 'qwen3.attention.value_length': memmap([128], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151654], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  37,  45, ...,  32,  37, 125], shape=(4060,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([14], dtype=uint32), 'quantize.imatrix.file': memmap([ 81, 119, 101, 110,  51,  45,  52,  66,  45,  84, 104, 105, 110,
        107, 105, 110, 103,  45,  50,  53,  48,  55,  45,  71,  71,  85,
         70,  47, 105, 109,  97, 116, 114, 105, 120,  95, 117, 110, 115,
        108, 111, 116, 104,  46, 103, 103, 117, 102], dtype=uint8), 'quantize.imatrix.dataset': memmap([117, 110, 115, 108, 111, 116, 104,  95,  99,  97, 108, 105,  98,
        114,  97, 116, 105, 111, 110,  95,  81, 119, 101, 110,  51,  45,
         52,  66,  45,  84, 104, 105, 110, 107, 105, 110, 103,  45,  50,
         53,  48,  55,  46, 116, 120, 116], dtype=uint8), 'quantize.imatrix.entries_count': memmap([252], dtype=uint32), 'quantize.imatrix.chunks_count': memmap([79], dtype=uint32)}}
2025-11-08 06:46:16,473 - INFO - root - Saved metadata to cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952) at ./config\metadata_cache\Qwen3-4B-Thinking-2507-Q4_K_S_2383309952.json
2025-11-08 06:46:16,475 - INFO - root - Cache miss or invalid for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448). Extracting metadata...
2025-11-08 06:46:16,475 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[579]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[44]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(8,), Value=[113 119 101 110  51 109 111 101]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'general.name': Type=<class 'numpy.memmap'>, Shape=(28,), Value=[ 81 119 101 110  51  45  67 111 100 101 114  45  51  48  66  45  65  51
  66  45  73 110 115 116 114 117  99 116]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'general.finetune': Type=<class 'numpy.memmap'>, Shape=(8,), Value=[ 73 110 115 116 114 117  99 116]
2025-11-08 06:46:23,527 - DEBUG - root - Extracted numpy array metadata for key 'general.basename': Type=<class 'numpy.memmap'>, Shape=(28,), Value=[ 81 119 101 110  51  45  67 111 100 101 114  45  51  48  66  45  65  51
  66  45  73 110 115 116 114 117  99 116]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.quantized_by': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[ 85 110 115 108 111 116 104]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[51 48 66 45 65 51 66]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.license': Type=<class 'numpy.memmap'>, Shape=(10,), Value=[ 97 112  97  99 104 101  45  50  46  48]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.license.link': Type=<class 'numpy.memmap'>, Shape=(74,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  51  45  67 111
 100 101 114  45  51  48  66  45  65  51  66  45  73 110 115 116 114 117
  99 116  47  98 108 111  98  47 109  97 105 110  47  76  73  67  69  78
  83  69]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.repo_url': Type=<class 'numpy.memmap'>, Shape=(30,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47 117 110 115 108 111 116 104]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.name': Type=<class 'numpy.memmap'>, Shape=(28,), Value=[ 81 119 101 110  51  32  67 111 100 101 114  32  51  48  66  32  65  51
  66  32  73 110 115 116 114 117  99 116]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.organization': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[ 81 119 101 110]
2025-11-08 06:46:23,528 - DEBUG - root - Extracted numpy array metadata for key 'general.base_model.0.repo_url': Type=<class 'numpy.memmap'>, Shape=(56,), Value=[104 116 116 112 115  58  47  47 104 117 103 103 105 110 103 102  97  99
 101  46  99 111  47  81 119 101 110  47  81 119 101 110  51  45  67 111
 100 101 114  45  51  48  66  45  65  51  66  45  73 110 115 116 114 117
  99 116]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'general.tags': Type=<class 'numpy.memmap'>, Shape=(7,), Value=[117 110 115 108 111 116 104]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.block_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[48]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.context_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[262144]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.embedding_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2048]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[5472]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.attention.head_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.attention.head_count_kv': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.rope.freq_base': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e+07]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.attention.layer_norm_rms_epsilon': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.expert_used_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.attention.key_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:23,529 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.attention.value_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.expert_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.expert_feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[768]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'qwen3moe.expert_shared_feed_forward_length': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[0]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.model': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[103 112 116  50]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.pre': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[113 119 101 110  50]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.tokens': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[33]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.token_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.merges': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[196 160  32 196 160]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.padding_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151654]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.ggml.add_bos_token': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'tokenizer.chat_template': Type=<class 'numpy.memmap'>, Shape=(6896,), Value=[123  35  32 ...  32  35 125]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:46:23,530 - DEBUG - root - Extracted numpy array metadata for key 'general.file_type': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[14]
2025-11-08 06:46:23,531 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.file': Type=<class 'numpy.memmap'>, Shape=(54,), Value=[ 81 119 101 110  51  45  67 111 100 101 114  45  51  48  66  45  65  51
  66  45  73 110 115 116 114 117  99 116  45  71  71  85  70  47 105 109
  97 116 114 105 120  95 117 110 115 108 111 116 104  46 103 103 117 102]
2025-11-08 06:46:23,531 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.dataset': Type=<class 'numpy.memmap'>, Shape=(52,), Value=[117 110 115 108 111 116 104  95  99  97 108 105  98 114  97 116 105 111
 110  95  81 119 101 110  51  45  67 111 100 101 114  45  51  48  66  45
  65  51  66  45  73 110 115 116 114 117  99 116  46 116 120 116]
2025-11-08 06:46:23,531 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.entries_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[384]
2025-11-08 06:46:23,531 - DEBUG - root - Extracted numpy array metadata for key 'quantize.imatrix.chunks_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[154]
2025-11-08 06:46:23,531 - DEBUG - root - Raw 'general.file_type' value: 14, Type: <class 'int'>
2025-11-08 06:46:23,531 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf
2025-11-08 06:46:23,535 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf: {'id': 'Qwen3-Coder-30B-A3B-Instruct', 'object': 'model', 'type': 'llm', 'publisher': 'Unsloth', 'arch': 'qwen3moe', 'compatibility_type': 'gguf', 'quantization': 'Q4_K_S', 'max_context_length': 262144, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([579], dtype=uint64), 'GGUF.kv_count': memmap([44], dtype=uint64), 'general.architecture': memmap([113, 119, 101, 110,  51, 109, 111, 101], dtype=uint8), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.name': memmap([ 81, 119, 101, 110,  51,  45,  67, 111, 100, 101, 114,  45,  51,
         48,  66,  45,  65,  51,  66,  45,  73, 110, 115, 116, 114, 117,
         99, 116], dtype=uint8), 'general.finetune': memmap([ 73, 110, 115, 116, 114, 117,  99, 116], dtype=uint8), 'general.basename': memmap([ 81, 119, 101, 110,  51,  45,  67, 111, 100, 101, 114,  45,  51,
         48,  66,  45,  65,  51,  66,  45,  73, 110, 115, 116, 114, 117,
         99, 116], dtype=uint8), 'general.quantized_by': memmap([ 85, 110, 115, 108, 111, 116, 104], dtype=uint8), 'general.size_label': memmap([51, 48, 66, 45, 65, 51, 66], dtype=uint8), 'general.license': memmap([ 97, 112,  97,  99, 104, 101,  45,  50,  46,  48], dtype=uint8), 'general.license.link': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  51,  45,  67, 111, 100, 101, 114,
         45,  51,  48,  66,  45,  65,  51,  66,  45,  73, 110, 115, 116,
        114, 117,  99, 116,  47,  98, 108, 111,  98,  47, 109,  97, 105,
        110,  47,  76,  73,  67,  69,  78,  83,  69], dtype=uint8), 'general.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47, 117, 110, 115,
        108, 111, 116, 104], dtype=uint8), 'general.base_model.count': memmap([1], dtype=uint32), 'general.base_model.0.name': memmap([ 81, 119, 101, 110,  51,  32,  67, 111, 100, 101, 114,  32,  51,
         48,  66,  32,  65,  51,  66,  32,  73, 110, 115, 116, 114, 117,
         99, 116], dtype=uint8), 'general.base_model.0.organization': memmap([ 81, 119, 101, 110], dtype=uint8), 'general.base_model.0.repo_url': memmap([104, 116, 116, 112, 115,  58,  47,  47, 104, 117, 103, 103, 105,
        110, 103, 102,  97,  99, 101,  46,  99, 111,  47,  81, 119, 101,
        110,  47,  81, 119, 101, 110,  51,  45,  67, 111, 100, 101, 114,
         45,  51,  48,  66,  45,  65,  51,  66,  45,  73, 110, 115, 116,
        114, 117,  99, 116], dtype=uint8), 'general.tags': memmap([117, 110, 115, 108, 111, 116, 104], dtype=uint8), 'qwen3moe.block_count': memmap([48], dtype=uint32), 'qwen3moe.context_length': memmap([262144], dtype=uint32), 'qwen3moe.embedding_length': memmap([2048], dtype=uint32), 'qwen3moe.feed_forward_length': memmap([5472], dtype=uint32), 'qwen3moe.attention.head_count': memmap([32], dtype=uint32), 'qwen3moe.attention.head_count_kv': memmap([4], dtype=uint32), 'qwen3moe.rope.freq_base': memmap([1.e+07], dtype=float32), 'qwen3moe.attention.layer_norm_rms_epsilon': memmap([1.e-06], dtype=float32), 'qwen3moe.expert_used_count': memmap([8], dtype=uint32), 'qwen3moe.attention.key_length': memmap([128], dtype=uint32), 'qwen3moe.attention.value_length': memmap([128], dtype=uint32), 'qwen3moe.expert_count': memmap([128], dtype=uint32), 'qwen3moe.expert_feed_forward_length': memmap([768], dtype=uint32), 'qwen3moe.expert_shared_feed_forward_length': memmap([0], dtype=uint32), 'tokenizer.ggml.model': memmap([103, 112, 116,  50], dtype=uint8), 'tokenizer.ggml.pre': memmap([113, 119, 101, 110,  50], dtype=uint8), 'tokenizer.ggml.tokens': memmap([33], dtype=uint8), 'tokenizer.ggml.token_type': memmap([1], dtype=int32), 'tokenizer.ggml.merges': memmap([196, 160,  32, 196, 160], dtype=uint8), 'tokenizer.ggml.eos_token_id': memmap([151645], dtype=uint32), 'tokenizer.ggml.padding_token_id': memmap([151654], dtype=uint32), 'tokenizer.ggml.add_bos_token': memmap([False]), 'tokenizer.chat_template': memmap([123,  35,  32, ...,  32,  35, 125], shape=(6896,), dtype=uint8), 'general.quantization_version': memmap([2], dtype=uint32), 'general.file_type': memmap([14], dtype=uint32), 'quantize.imatrix.file': memmap([ 81, 119, 101, 110,  51,  45,  67, 111, 100, 101, 114,  45,  51,
         48,  66,  45,  65,  51,  66,  45,  73, 110, 115, 116, 114, 117,
         99, 116,  45,  71,  71,  85,  70,  47, 105, 109,  97, 116, 114,
        105, 120,  95, 117, 110, 115, 108, 111, 116, 104,  46, 103, 103,
        117, 102], dtype=uint8), 'quantize.imatrix.dataset': memmap([117, 110, 115, 108, 111, 116, 104,  95,  99,  97, 108, 105,  98,
        114,  97, 116, 105, 111, 110,  95,  81, 119, 101, 110,  51,  45,
         67, 111, 100, 101, 114,  45,  51,  48,  66,  45,  65,  51,  66,
         45,  73, 110, 115, 116, 114, 117,  99, 116,  46, 116, 120, 116],
       dtype=uint8), 'quantize.imatrix.entries_count': memmap([384], dtype=uint32), 'quantize.imatrix.chunks_count': memmap([154], dtype=uint32)}}
2025-11-08 06:46:23,669 - INFO - root - Saved metadata to cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448) at ./config\metadata_cache\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S_17456012448.json
2025-11-08 06:46:23,670 - INFO - root - Cache miss or invalid for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544). Extracting metadata...
2025-11-08 06:46:23,670 - DEBUG - root - Attempting to extract GGUF metadata from: F:\llm\llama\models\Qwen3-VL-8B-Thinking.Q4_K-GGUF\Qwen3-VL-8B-Thinking.Q4_K.gguf
2025-11-08 06:46:23,705 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[399]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'GGUF.kv_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[21]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'general.architecture': Type=<class 'numpy.memmap'>, Shape=(20,), Value=[ 81 119 101 110  51  45  86  76  45  56  66  45  84 104 105 110 107 105
 110 103]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[3]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'tensor_count': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[399]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'general.type': Type=<class 'numpy.memmap'>, Shape=(5,), Value=[109 111 100 101 108]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'general.size_label': Type=<class 'numpy.memmap'>, Shape=(2,), Value=[56 66]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'bos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151643]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'eos_token_id': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151645]
2025-11-08 06:46:23,706 - DEBUG - root - Extracted numpy array metadata for key 'hidden_act': Type=<class 'numpy.memmap'>, Shape=(4,), Value=[115 105 108 117]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'hidden_size': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[4096]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'intermediate_size': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[12288]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'max_position_embeddings': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[262144]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'num_attention_heads': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[32]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'num_hidden_layers': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[36]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'num_key_value_heads': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[8]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'rms_norm_eps': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[1.e-06]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'rope_theta': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[5.e+06]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'attention_bias': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'head_dim': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[128]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'tie_word_embeddings': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[False]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'vocab_size': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[151936]
2025-11-08 06:46:23,707 - DEBUG - root - Extracted numpy array metadata for key 'general.quantization_version': Type=<class 'numpy.memmap'>, Shape=(1,), Value=[2]
2025-11-08 06:46:23,708 - DEBUG - root - Raw 'general.file_type' value: None, Type: <class 'NoneType'>
2025-11-08 06:46:23,708 - WARNING - root - Architecture-specific context_length key 'Qwen3-VL-8B-Thinking.context_length' not found for F:\llm\llama\models\Qwen3-VL-8B-Thinking.Q4_K-GGUF\Qwen3-VL-8B-Thinking.Q4_K.gguf. Using default 4096.
2025-11-08 06:46:23,708 - INFO - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-VL-8B-Thinking.Q4_K-GGUF\Qwen3-VL-8B-Thinking.Q4_K.gguf
2025-11-08 06:46:23,709 - DEBUG - root - Successfully extracted metadata for F:\llm\llama\models\Qwen3-VL-8B-Thinking.Q4_K-GGUF\Qwen3-VL-8B-Thinking.Q4_K.gguf: {'id': 'Qwen3-VL-8B-Thinking.Q4_K.gguf', 'object': 'model', 'type': 'llm', 'publisher': 'local', 'arch': 'Qwen3-VL-8B-Thinking', 'compatibility_type': 'gguf', 'quantization': 'Unknown', 'max_context_length': 4096, 'raw_metadata': {'GGUF.version': memmap([3], dtype=uint32), 'GGUF.tensor_count': memmap([399], dtype=uint64), 'GGUF.kv_count': memmap([21], dtype=uint64), 'general.architecture': memmap([ 81, 119, 101, 110,  51,  45,  86,  76,  45,  56,  66,  45,  84,
        104, 105, 110, 107, 105, 110, 103], dtype=uint8), 'version': memmap([3], dtype=uint32), 'tensor_count': memmap([399], dtype=uint32), 'general.type': memmap([109, 111, 100, 101, 108], dtype=uint8), 'general.size_label': memmap([56, 66], dtype=uint8), 'bos_token_id': memmap([151643], dtype=uint32), 'eos_token_id': memmap([151645], dtype=uint32), 'hidden_act': memmap([115, 105, 108, 117], dtype=uint8), 'hidden_size': memmap([4096], dtype=uint32), 'intermediate_size': memmap([12288], dtype=uint32), 'max_position_embeddings': memmap([262144], dtype=uint32), 'num_attention_heads': memmap([32], dtype=uint32), 'num_hidden_layers': memmap([36], dtype=uint32), 'num_key_value_heads': memmap([8], dtype=uint32), 'rms_norm_eps': memmap([1.e-06], dtype=float32), 'rope_theta': memmap([5.e+06], dtype=float32), 'attention_bias': memmap([False]), 'head_dim': memmap([128], dtype=uint32), 'tie_word_embeddings': memmap([False]), 'vocab_size': memmap([151936], dtype=uint32), 'general.quantization_version': memmap([2], dtype=uint32)}}
2025-11-08 06:46:23,711 - INFO - root - Saved metadata to cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544) at ./config\metadata_cache\Qwen3-VL-8B-Thinking.Q4_K_4608376544.json
2025-11-08 06:47:54,736 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:47:54,737 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:47:54,739 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:47:54,747 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:47:54,758 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:47:54,759 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:47:54,760 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:47:54,769 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:47:54,771 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:47:54,772 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:47:54,773 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:47:54,784 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:47:54,786 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:48:12,504 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:48:12,504 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:48:12,505 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:48:12,506 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:48:12,507 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:48:12,507 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:48:12,508 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:48:12,508 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:48:12,508 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:48:12,509 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:48:12,509 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:48:12,510 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:48:12,511 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:48:28,668 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:48:28,669 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:48:28,669 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:48:28,669 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:48:28,670 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:48:28,670 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:48:28,670 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:48:28,671 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:48:28,671 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:48:28,672 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:48:28,672 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:48:28,673 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:48:28,673 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:48:36,887 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:48:36,887 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:48:36,888 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:48:36,889 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:48:36,889 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:48:36,890 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:48:36,891 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:48:36,891 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:48:36,891 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:48:36,892 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:48:36,892 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:48:36,893 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:48:36,893 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:48:50,597 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:48:50,598 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:48:50,598 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:48:50,599 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:48:50,600 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:48:50,600 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:48:50,601 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:48:50,602 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:48:50,603 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:48:50,604 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:48:50,604 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:48:50,605 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:48:50,605 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:50:46,668 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:50:46,669 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:50:46,669 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:50:46,670 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:50:46,670 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:50:46,671 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:50:46,671 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:50:46,672 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:50:46,672 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:50:46,673 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:50:46,673 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:50:46,674 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:50:46,674 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:50:46,675 - INFO - root - Runner for gpt-oss-20b-MXFP4 not running. Requesting startup.
2025-11-08 06:50:46,675 - INFO - llama_runner.services.runner_service - Starting Llama runner for gpt-oss-20b-MXFP4
2025-11-08 06:50:46,675 - INFO - llama_runner.services.runner_service - First runner will use port 8585 for web UI routing
2025-11-08 06:50:46,675 - INFO - llama_runner.metrics - Runner started
2025-11-08 06:50:46,676 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Started gpt-oss-20b-MXFP4
2025-11-08 06:50:46,676 - INFO - root - Starting llama.cpp server with command: F:\llm\llama\llama-server.exe --model F:\llm\llama\models\gpt-oss-20b-MXFP4-GGUF\gpt-oss-20b-MXFP4.gguf --alias gpt-oss-20b-MXFP4 --host 127.0.0.1 --port 8585 --ctx-size 32000 --temp 1.0 --batch-size 1024 --ubatch-size 512 --threads 10 --mlock --no-mmap --flash-attn on --n-gpu-layers 85 --top-p 1.0 --top-k 0 --jinja
2025-11-08 06:50:47,635 - INFO - root - Process started with PID: 213868
2025-11-08 06:50:56,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2025-11-08 06:50:56,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2025-11-08 06:50:56,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ggml_cuda_init: found 1 CUDA devices:
2025-11-08 06:50:56,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Device 0: NVIDIA GeForce GTX 1070, compute capability 6.1, VMM: yes
2025-11-08 06:50:56,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_backend: loaded CUDA backend from F:\llm\llama\ggml-cuda.dll
2025-11-08 06:50:56,369 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_backend: loaded RPC backend from F:\llm\llama\ggml-rpc.dll
2025-11-08 06:50:57,510 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ggml_vulkan: Found 2 Vulkan devices:
2025-11-08 06:50:57,512 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none
2025-11-08 06:50:57,515 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ggml_vulkan: 1 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none
2025-11-08 06:50:57,515 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_backend: loaded Vulkan backend from F:\llm\llama\ggml-vulkan.dll
2025-11-08 06:50:57,781 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_backend: loaded CPU backend from F:\llm\llama\ggml-cpu-haswell.dll
2025-11-08 06:50:57,782 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)
2025-11-08 06:50:57,782 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: build: 6975 (16bcc1259) with clang version 19.1.5 for x86_64-pc-windows-msvc
2025-11-08 06:50:57,783 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: system info: n_threads = 10, n_threads_batch = 10, total_threads = 12
2025-11-08 06:50:57,783 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:50:57,783 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: system_info: n_threads = 10 (n_threads_batch = 10) / 12 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |
2025-11-08 06:50:57,783 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:50:57,783 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: binding port with default address family
2025-11-08 06:50:57,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: HTTP server is listening, hostname: 127.0.0.1, port: 8585, http threads: 11
2025-11-08 06:50:57,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: loading model
2025-11-08 06:50:57,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv    load_model: loading model 'F:\llm\llama\models\gpt-oss-20b-MXFP4-GGUF\gpt-oss-20b-MXFP4.gguf'
2025-11-08 06:50:57,886 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_load_from_file_impl: skipping device Vulkan0 (NVIDIA GeForce GTX 1070) with id 0000:01:00.0 - already using device CUDA0 (NVIDIA GeForce GTX 1070) with the same id
2025-11-08 06:50:57,886 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1070) (0000:01:00.0) - 7215 MiB free
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: loaded meta data with 33 key-value pairs and 459 tensors from F:\llm\llama\models\gpt-oss-20b-MXFP4-GGUF\gpt-oss-20b-MXFP4.gguf (version GGUF V3 (latest))
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   1:                               general.type str              = model
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   2:                               general.name str              = Openai_Gpt Oss 20b
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   3:                           general.basename str              = openai_gpt-oss
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   4:                         general.size_label str              = 20B
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   5:                        gpt-oss.block_count u32              = 24
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   6:                     gpt-oss.context_length u32              = 131072
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   7:                   gpt-oss.embedding_length u32              = 2880
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   8:                gpt-oss.feed_forward_length u32              = 2880
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv   9:               gpt-oss.attention.head_count u32              = 64
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  10:            gpt-oss.attention.head_count_kv u32              = 8
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  11:                     gpt-oss.rope.freq_base f32              = 150000.000000
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  12:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
2025-11-08 06:50:57,958 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  13:                       gpt-oss.expert_count u32              = 32
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  14:                  gpt-oss.expert_used_count u32              = 4
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  15:               gpt-oss.attention.key_length u32              = 64
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  16:             gpt-oss.attention.value_length u32              = 64
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  17:           gpt-oss.attention.sliding_window u32              = 128
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  18:         gpt-oss.expert_feed_forward_length u32              = 2880
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  19:                  gpt-oss.rope.scaling.type str              = yarn
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  20:                gpt-oss.rope.scaling.factor f32              = 32.000000
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  21: gpt-oss.rope.scaling.original_context_length u32              = 4096
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
2025-11-08 06:50:57,959 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = gpt-4o
2025-11-08 06:50:58,013 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-11-08 06:50:58,033 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 199998
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 200002
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 199999
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {#-\n  In addition to the normal input...
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  31:               general.quantization_version u32              = 2
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - kv  32:                          general.file_type u32              = 38
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - type  f32:  289 tensors
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - type q8_0:   98 tensors
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_model_loader: - type mxfp4:   72 tensors
2025-11-08 06:50:58,169 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: file format = GGUF V3 (latest)
2025-11-08 06:50:58,170 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: file type   = MXFP4 MoE
2025-11-08 06:50:58,170 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: file size   = 11.27 GiB (4.63 BPW)
2025-11-08 06:50:58,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load: printing all EOG tokens:
2025-11-08 06:50:58,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load:   - 199999 ('<|endoftext|>')
2025-11-08 06:50:58,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load:   - 200002 ('<|return|>')
2025-11-08 06:50:58,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load:   - 200007 ('<|end|>')
2025-11-08 06:50:58,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load:   - 200012 ('<|call|>')
2025-11-08 06:50:58,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load: special_eog_ids contains both '<|return|>' and '<|call|>' tokens, removing '<|end|>' token from EOG list
2025-11-08 06:50:58,444 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load: special tokens cache size = 21
2025-11-08 06:50:58,491 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load: token to piece cache size = 1.3332 MB
2025-11-08 06:50:58,491 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: arch             = gpt-oss
2025-11-08 06:50:58,491 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: vocab_only       = 0
2025-11-08 06:50:58,491 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_ctx_train      = 131072
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_embd           = 2880
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_layer          = 24
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_head           = 64
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_head_kv        = 8
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_rot            = 64
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_swa            = 128
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: is_swa_any       = 1
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_embd_head_k    = 64
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_embd_head_v    = 64
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_gqa            = 8
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_embd_k_gqa     = 512
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_embd_v_gqa     = 512
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: f_norm_eps       = 0.0e+00
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: f_norm_rms_eps   = 1.0e-05
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: f_clamp_kqv      = 0.0e+00
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: f_max_alibi_bias = 0.0e+00
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: f_logit_scale    = 0.0e+00
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: f_attn_scale     = 0.0e+00
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_ff             = 2880
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_expert         = 32
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_expert_used    = 4
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_expert_groups  = 0
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_group_used     = 0
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: causal attn      = 1
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: pooling type     = 0
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: rope type        = 2
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: rope scaling     = yarn
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: freq_base_train  = 150000.0
2025-11-08 06:50:58,492 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: freq_scale_train = 0.03125
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_ctx_orig_yarn  = 4096
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: rope_finetuned   = unknown
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: model type       = 20B
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: model params     = 20.91 B
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: general.name     = Openai_Gpt Oss 20b
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_ff_exp         = 2880
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: vocab type       = BPE
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_vocab          = 201088
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: n_merges         = 446189
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: BOS token        = 199998 '<|startoftext|>'
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: EOS token        = 200002 '<|return|>'
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: EOT token        = 199999 '<|endoftext|>'
2025-11-08 06:50:58,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: PAD token        = 199999 '<|endoftext|>'
2025-11-08 06:50:58,495 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: EOG token        = 199999 '<|endoftext|>'
2025-11-08 06:50:58,495 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: EOG token        = 200002 '<|return|>'
2025-11-08 06:50:58,495 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: EOG token        = 200012 '<|call|>'
2025-11-08 06:50:58,495 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: print_info: max token length = 256
2025-11-08 06:50:58,495 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_tensors: loading model tensors, this can take a while... (mmap = false)
2025-11-08 06:50:59,173 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_tensors: offloading 24 repeating layers to GPU
2025-11-08 06:50:59,173 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_tensors: offloading output layer to GPU
2025-11-08 06:50:59,173 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_tensors: offloaded 25/25 layers to GPU
2025-11-08 06:50:59,173 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_tensors:        CUDA0 model buffer size = 10949.38 MiB
2025-11-08 06:50:59,173 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: load_tensors:    CUDA_Host model buffer size =   586.82 MiB
2025-11-08 06:51:28,328 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: ................................................................................
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: constructing llama_context
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: n_seq_max     = 4
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: n_ctx         = 32000
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: n_ctx_seq     = 32000
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: n_batch       = 1024
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: n_ubatch      = 512
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: causal_attn   = 1
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: flash_attn    = enabled
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: kv_unified    = true
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: freq_base     = 150000.0
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: freq_scale    = 0.03125
2025-11-08 06:51:28,337 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: n_ctx_seq (32000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
2025-11-08 06:51:28,341 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context:  CUDA_Host  output buffer size =     3.07 MiB
2025-11-08 06:51:28,342 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_kv_cache_iswa: creating non-SWA KV cache, size = 32000 cells
2025-11-08 06:51:28,348 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_kv_cache:      CUDA0 KV buffer size =   750.00 MiB
2025-11-08 06:51:28,743 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_kv_cache: size =  750.00 MiB ( 32000 cells,  12 layers,  4/1 seqs), K (f16):  375.00 MiB, V (f16):  375.00 MiB
2025-11-08 06:51:28,743 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
2025-11-08 06:51:28,743 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
2025-11-08 06:51:28,757 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
2025-11-08 06:51:28,782 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context:      CUDA0 compute buffer size =   398.38 MiB
2025-11-08 06:51:28,782 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context:  CUDA_Host compute buffer size =    70.15 MiB
2025-11-08 06:51:28,782 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: graph nodes  = 1352
2025-11-08 06:51:28,782 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_context: graph splits = 2
2025-11-08 06:51:28,784 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: common_init_from_params: added <|endoftext|> logit bias = -inf
2025-11-08 06:51:28,784 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: common_init_from_params: added <|return|> logit bias = -inf
2025-11-08 06:51:28,784 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: common_init_from_params: added <|call|> logit bias = -inf
2025-11-08 06:51:28,784 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: common_init_from_params: setting dry_penalty_last_n to ctx_size = 32000
2025-11-08 06:51:28,784 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv          init: initializing slots, n_slots = 4
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot         init: id  0 | task -1 | new slot, n_ctx = 32000
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot         init: id  1 | task -1 | new slot, n_ctx = 32000
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot         init: id  2 | task -1 | new slot, n_ctx = 32000
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot         init: id  3 | task -1 | new slot, n_ctx = 32000
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv          init: prompt cache is enabled, size limit: 8192 MiB
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv          init: use `--cache-ram 0` to disable the prompt cache
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv          init: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv          init: thinking = 0
2025-11-08 06:51:36,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: model loaded
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: chat template, chat_template: {#-
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: In addition to the normal inputs of `messages` and `tools`, this template also accepts the
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: following kwargs:
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: - "builtin_tools": A list, can contain "browser" and/or "python".
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: - "model_identity": A string that optionally describes the model identity.
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: - "reasoning_effort": A string that describes the reasoning effort, defaults to "medium".
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: #}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Tool Definition Rendering ============================================== #}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.type == "array" -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec['items'] -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec['items']['type'] == "string" -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "string[]" }}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec['items']['type'] == "number" -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "number[]" }}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec['items']['type'] == "integer" -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "number[]" }}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec['items']['type'] == "boolean" -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "boolean[]" }}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set inner_type = render_typescript_type(param_spec['items'], required_params) -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if inner_type == "object | object" or inner_type|length > 50 -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "any[]" }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- inner_type + "[]" }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.nullable -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- " | null" }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "any[]" }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.nullable -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- " | null" }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Handle array of types like ["object", "object"] from Union[dict, list] #}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.type | length > 1 -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- param_spec.type | join(" | ") }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- param_spec.type[0] }}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.oneOf -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Handle oneOf schemas - check for complex unions and fallback to any #}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set has_object_variants = false -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for variant in param_spec.oneOf -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if variant.type == "object" -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set has_object_variants = true -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if has_object_variants and param_spec.oneOf|length > 1 -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "any" }}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for variant in param_spec.oneOf -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- render_typescript_type(variant, required_params) -}}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if variant.description %}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// " + variant.description }}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if variant.default is defined %}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{ "// default: " + variant.default|tojson }}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if not loop.last %}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- " | " }}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {% endif -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.type == "string" -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.enum -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- '"' + param_spec.enum|join('" | "') + '"' -}}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "string" }}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.nullable %}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- " | null" }}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,049 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.type == "number" -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "number" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.type == "integer" -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "number" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.type == "boolean" -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "boolean" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.type == "object" -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.properties -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "{
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for prop_name, prop_spec in param_spec.properties.items() -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- prop_name -}}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if prop_name not in (param_spec.required or []) -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "?" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- ": " }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{ render_typescript_type(prop_spec, param_spec.required or []) }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if not loop.last -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{-", " }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "}" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "object" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "any" }}
2025-11-08 06:51:36,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endmacro -%}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- macro render_tool_namespace(namespace_name, tools) -%}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "## " + namespace_name + "
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "namespace " + namespace_name + " {
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for tool in tools %}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set tool = tool.function %}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// " + tool.description + "
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "type "+ tool.name + " = " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if tool.parameters and tool.parameters.properties %}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "(_: {
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for param_name, param_spec in tool.parameters.properties.items() %}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.description %}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// " + param_spec.description + "
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- param_name }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_name not in (tool.parameters.required or []) -%}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "?" }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- ": " }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- render_typescript_type(param_spec, tool.parameters.required or []) }}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.default is defined -%}
2025-11-08 06:51:36,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if param_spec.enum %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- ", // default: " + param_spec.default }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif param_spec.oneOf %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// default: " + param_spec.default }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- ", // default: " + param_spec.default|tojson }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if not loop.last %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- ",
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "}) => any;
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "() => any;
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor %}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "} // namespace " + namespace_name }}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endmacro -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- macro render_builtin_tools(browser_tool, python_tool) -%}
2025-11-08 06:51:36,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if browser_tool %}
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "## browser
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Tool for browsing.
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Cite information from the tool using the following format:
2025-11-08 06:51:36,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Do not quote more than 10 words directly from the tool output.
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// sources=web (default: web)
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "namespace browser {
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Searches for information related to `query` and displays `topn` results.
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "type search = (_: {
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "query: string,
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "topn?: number, // default: 10
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "source?: string,
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "}) => any;
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,058 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.
2025-11-08 06:51:36,058 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// If `cursor` is not provided, the most recent page is implied.
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// If `id` is a string, it is treated as a fully qualified URL associated with `source`.
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Use this function without `id` to scroll to a new location of an opened page.
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "type open = (_: {
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "id?: number | string, // default: -1
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "cursor?: number, // default: -1
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "loc?: number, // default: -1
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "num_lines?: number, // default: -1
2025-11-08 06:51:36,060 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "view_source?: boolean, // default: false
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "source?: string,
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "}) => any;
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "type find = (_: {
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "pattern: string,
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "cursor?: number, // default: -1
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "}) => any;
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "} // namespace browser
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if python_tool %}
2025-11-08 06:51:36,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "## python
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endmacro -%}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- System Message Construction ============================================ #}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- macro build_system_message() -%}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if model_identity is not defined %}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set model_identity = "You are ChatGPT, a large language model trained by OpenAI." %}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- model_identity + "
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "Knowledge cutoff: 2024-06
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "Current date: " + strftime_now("%Y-%m-%d") + "
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if reasoning_effort is not defined %}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set reasoning_effort = "medium" %}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "Reasoning: " + reasoning_effort + "
2025-11-08 06:51:36,062 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if builtin_tools %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "# Tools
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set available_builtin_tools = namespace(browser=false, python=false) %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for tool in builtin_tools %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if tool == "browser" %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set available_builtin_tools.browser = true %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif tool == "python" %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set available_builtin_tools.python = true %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor %}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "# Valid channels: analysis, commentary, final. Channel must be included for every message." }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if tools -%}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Calls to these tools must go to the commentary channel: 'functions'." }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endmacro -%}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Main Template Logic ================================================= #}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Set defaults #}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Render system message #}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>system<|message|>" }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- build_system_message() }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|end|>" }}
2025-11-08 06:51:36,063 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Extract developer message #}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if messages[0].role == "developer" or messages[0].role == "system" %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set developer_message = messages[0].content %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set loop_messages = messages[1:] %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set developer_message = "" %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set loop_messages = messages %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Render developer message #}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if developer_message or tools %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>developer<|message|>" }}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if developer_message %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "# Instructions
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- developer_message }}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if tools -%}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "# Tools
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: " }}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- render_tool_namespace("functions", tools) }}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|end|>" }}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Render messages #}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set last_tool_call = namespace(name=none) %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- for message in loop_messages -%}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- At this point only assistant/user/tool messages should remain #}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if message.role == 'assistant' -%}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Checks to ensure the messages are being passed in the format we expect #}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if "content" in message %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if false %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- raise_exception("You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.") }}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if "thinking" in message %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if "<|channel|>analysis<|message|>" in message.thinking or "<|channel|>final<|message|>" in message.thinking %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- raise_exception("You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between '<|message|>' and '<|end|>') in the 'thinking' field, and final messages (the string between '<|message|>' and '<|end|>') in the 'content' field.") }}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if "tool_calls" in message %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- We assume max 1 tool call per message, and so we infer the tool call name #}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- in "tool" messages from the most recent assistant tool call name #}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set tool_call = message.tool_calls[0] %}
2025-11-08 06:51:36,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if tool_call.function %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set tool_call = tool_call.function %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if message.content and message.thinking %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- raise_exception("Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.") }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif message.content %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.content + "<|end|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif message.thinking %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.thinking + "<|end|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>assistant to=" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "functions." + tool_call.name + "<|channel|>commentary " }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- (tool_call.content_type if tool_call.content_type is defined else "json") + "<|message|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- tool_call.arguments|tojson }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|call|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set last_tool_call.name = tool_call.name %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif loop.last and not add_generation_prompt %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- This is a situation that should only occur in training, never in inference. #}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if "thinking" in message %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>assistant<|channel|>analysis<|message|>" + message.thinking + "<|end|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- <|return|> indicates the end of generation, but <|end|> does not #}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- <|return|> should never be an input to the model, but we include it as the final token #}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- when training, so the model learns to emit it. #}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>assistant<|channel|>final<|message|>" + message.content + "<|return|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- else %}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- CoT is dropped during all previous turns, so we never render it for inference #}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>assistant<|channel|>final<|message|>" + message.content + "<|end|>" }}
2025-11-08 06:51:36,066 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- set last_tool_call.name = none %}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif message.role == 'tool' -%}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if last_tool_call.name is none %}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- raise_exception("Message has tool role, but there was no previous assistant message with a tool call!") }}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif %}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>functions." + last_tool_call.name }}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- " to=assistant<|channel|>commentary<|message|>" + message.content|tojson + "<|end|>" }}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- elif message.role == 'user' -%}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {{- "<|start|>user<|message|>" + message.content + "<|end|>" }}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endfor -%}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {#- Generation prompt #}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- if add_generation_prompt -%}
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: <|start|>assistant
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: {%- endif -%}, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Knowledge cutoff: 2024-06
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Current date: 2025-11-08
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Reasoning: medium
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: # Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: 
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
2025-11-08 06:51:36,067 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: main: server is listening on http://127.0.0.1:8585 - starting the main loop
2025-11-08 06:51:36,067 - INFO - root - llama.cpp server for gpt-oss-20b-MXFP4 is configured on port 8585
2025-11-08 06:51:36,068 - INFO - llama_runner.metrics - Runner ready
2025-11-08 06:51:36,068 - INFO - llama_runner.headless_service_manager - Port 8585 ready for gpt-oss-20b-MXFP4
2025-11-08 06:51:36,068 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:51:36,095 - INFO - llama_runner.services.runner_service - Llama runner for gpt-oss-20b-MXFP4 is ready on port 8585
2025-11-08 06:51:36,095 - INFO - root - Runner for gpt-oss-20b-MXFP4 is ready on port 8585 after startup.
2025-11-08 06:51:36,095 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:51:36,912 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:51:36,914 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216E6EFDD60>
2025-11-08 06:51:36,914 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:51:36,914 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:51:36,914 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:51:36,915 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:51:36,915 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:51:37,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:51:37,020 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
2025-11-08 06:51:37,020 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:51:37,020 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:51:37,050 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:51:37,051 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:51:37,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:51:37,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 0 | processing task
2025-11-08 06:51:37,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 9975
2025-11-08 06:51:37,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
2025-11-08 06:51:37,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 1024, batch.n_tokens = 1024, progress = 0.102657
2025-11-08 06:51:40,788 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 1024, memory_seq_rm [1024, end)
2025-11-08 06:51:40,789 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 1024, progress = 0.205313
2025-11-08 06:51:48,457 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 2048, memory_seq_rm [2048, end)
2025-11-08 06:51:48,458 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 3072, batch.n_tokens = 1024, progress = 0.307970
2025-11-08 06:51:56,549 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 3072, memory_seq_rm [3072, end)
2025-11-08 06:51:56,549 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 1024, progress = 0.410627
2025-11-08 06:52:05,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 4096, memory_seq_rm [4096, end)
2025-11-08 06:52:05,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 5120, batch.n_tokens = 1024, progress = 0.513283
2025-11-08 06:52:14,017 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 5120, memory_seq_rm [5120, end)
2025-11-08 06:52:14,017 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 1024, progress = 0.615940
2025-11-08 06:52:23,418 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 6144, memory_seq_rm [6144, end)
2025-11-08 06:52:23,418 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 7168, batch.n_tokens = 1024, progress = 0.718597
2025-11-08 06:52:33,263 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 7168, memory_seq_rm [7168, end)
2025-11-08 06:52:33,263 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 1024, progress = 0.821253
2025-11-08 06:52:43,540 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 8192, memory_seq_rm [8192, end)
2025-11-08 06:52:43,540 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 9216, batch.n_tokens = 1024, progress = 0.923910
2025-11-08 06:52:54,251 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 9216, memory_seq_rm [9216, end)
2025-11-08 06:52:54,251 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 9911, batch.n_tokens = 695, progress = 0.993584
2025-11-08 06:53:05,574 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | n_tokens = 9911, memory_seq_rm [9911, end)
2025-11-08 06:53:05,574 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 9975, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:53:05,578 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | prompt done, n_tokens = 9975, batch.n_tokens = 64
2025-11-08 06:53:07,937 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 8887, pos_max = 9910, size = 24.012 MiB)
2025-11-08 06:53:44,099 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 0 |
2025-11-08 06:53:44,099 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =   92126.73 ms /  9975 tokens (    9.24 ms per token,   108.27 tokens per second)
2025-11-08 06:53:44,099 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   34947.92 ms /    93 tokens (  375.78 ms per token,     2.66 tokens per second)
2025-11-08 06:53:44,099 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =  127074.65 ms / 10068 tokens
2025-11-08 06:53:44,099 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 0 | stop processing: n_tokens = 10067, truncated = 0
2025-11-08 06:53:44,099 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:53:44,101 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:53:44,101 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:53:44,101 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:53:44,102 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:53:44,594 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:53:44,596 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:53:44,596 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:53:44,597 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:53:44,598 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:53:44,598 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:53:44,599 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:53:44,600 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:53:44,601 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:53:44,602 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:53:44,603 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:53:44,604 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:53:44,604 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:53:44,605 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:53:44,605 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:53:44,874 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:53:44,875 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311D190>
2025-11-08 06:53:44,876 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:53:44,876 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:53:44,876 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:53:44,877 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:53:44,877 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:53:44,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:53:44,974 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.991
2025-11-08 06:53:44,974 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:53:44,974 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:53:44,974 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:53:44,975 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:53:44,976 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:53:44,976 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 104 | processing task
2025-11-08 06:53:44,976 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 104 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10027
2025-11-08 06:53:44,976 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 104 | n_tokens = 9978, memory_seq_rm [9978, end)
2025-11-08 06:53:44,976 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 104 | prompt processing progress, n_tokens = 10027, batch.n_tokens = 49, progress = 1.000000
2025-11-08 06:53:44,979 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 104 | prompt done, n_tokens = 10027, batch.n_tokens = 49
2025-11-08 06:53:44,988 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 104 | created context checkpoint 2 of 8 (pos_min = 9043, pos_max = 9977, size = 21.925 MiB)
2025-11-08 06:53:57,568 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 104 |
2025-11-08 06:53:57,568 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1015.97 ms /    49 tokens (   20.73 ms per token,    48.23 tokens per second)
2025-11-08 06:53:57,568 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   11576.11 ms /    31 tokens (  373.42 ms per token,     2.68 tokens per second)
2025-11-08 06:53:57,568 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   12592.08 ms /    80 tokens
2025-11-08 06:53:57,568 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 104 | stop processing: n_tokens = 10057, truncated = 0
2025-11-08 06:53:57,569 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:53:57,569 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:53:57,569 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:53:57,570 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:53:57,570 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:53:57,643 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:53:57,643 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:53:57,645 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:53:57,646 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:53:57,646 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:53:57,646 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:53:57,646 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:53:57,647 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:53:57,647 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:53:57,648 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:53:57,648 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:53:57,649 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:53:57,650 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:53:57,650 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:53:57,650 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:53:57,906 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:53:57,907 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311D340>
2025-11-08 06:53:57,907 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:53:57,907 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:53:57,907 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:53:57,907 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:53:57,907 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:53:57,911 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:53:58,006 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:53:58,006 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.997
2025-11-08 06:53:58,006 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:53:58,006 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:53:58,006 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:53:58,007 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:53:58,008 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:53:58,008 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 136 | processing task
2025-11-08 06:53:58,008 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10108
2025-11-08 06:53:58,008 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | n_tokens = 10030, memory_seq_rm [10030, end)
2025-11-08 06:53:58,008 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | prompt processing progress, n_tokens = 10044, batch.n_tokens = 14, progress = 0.993668
2025-11-08 06:53:58,016 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | n_tokens = 10044, memory_seq_rm [10044, end)
2025-11-08 06:53:58,016 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | prompt processing progress, n_tokens = 10108, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:53:58,020 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | prompt done, n_tokens = 10108, batch.n_tokens = 64
2025-11-08 06:53:58,518 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 136 | created context checkpoint 3 of 8 (pos_min = 9043, pos_max = 10043, size = 23.473 MiB)
2025-11-08 06:54:16,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 136 |
2025-11-08 06:54:16,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1739.26 ms /    78 tokens (   22.30 ms per token,    44.85 tokens per second)
2025-11-08 06:54:16,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   16340.14 ms /    44 tokens (  371.37 ms per token,     2.69 tokens per second)
2025-11-08 06:54:16,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   18079.41 ms /   122 tokens
2025-11-08 06:54:16,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 136 | stop processing: n_tokens = 10151, truncated = 0
2025-11-08 06:54:16,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:54:16,089 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:54:16,089 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:54:16,089 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:54:16,089 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:54:16,184 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:54:16,185 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:54:16,186 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:54:16,186 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:54:16,187 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:54:16,188 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:54:16,189 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:54:16,190 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:54:16,190 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:54:16,191 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:54:16,191 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:54:16,192 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:54:16,192 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:54:16,192 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:54:16,192 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:54:16,453 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:54:16,454 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000021813137290>
2025-11-08 06:54:16,454 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:54:16,454 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:54:16,454 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:54:16,455 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:54:16,455 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:54:16,458 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:16,458 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:16,554 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:54:16,554 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:54:16,554 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:54:16,555 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:54:16,555 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:54:16,555 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:54:16,556 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:54:16,556 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 182 | processing task
2025-11-08 06:54:16,556 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10189
2025-11-08 06:54:16,557 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | n_tokens = 10111, memory_seq_rm [10111, end)
2025-11-08 06:54:16,557 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | prompt processing progress, n_tokens = 10125, batch.n_tokens = 14, progress = 0.993719
2025-11-08 06:54:16,564 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | n_tokens = 10125, memory_seq_rm [10125, end)
2025-11-08 06:54:16,564 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | prompt processing progress, n_tokens = 10189, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:54:16,567 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | prompt done, n_tokens = 10189, batch.n_tokens = 64
2025-11-08 06:54:17,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 182 | created context checkpoint 4 of 8 (pos_min = 9127, pos_max = 10124, size = 23.402 MiB)
2025-11-08 06:54:33,540 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 182 |
2025-11-08 06:54:33,541 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1727.42 ms /    78 tokens (   22.15 ms per token,    45.15 tokens per second)
2025-11-08 06:54:33,541 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   15256.13 ms /    41 tokens (  372.10 ms per token,     2.69 tokens per second)
2025-11-08 06:54:33,541 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   16983.56 ms /   119 tokens
2025-11-08 06:54:33,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 182 | stop processing: n_tokens = 10229, truncated = 0
2025-11-08 06:54:33,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:54:33,543 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:54:33,543 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:54:33,544 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:54:33,544 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:54:33,646 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:54:33,648 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:54:33,648 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:54:33,649 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:54:33,649 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:54:33,650 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:54:33,650 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:54:33,651 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:54:33,651 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:54:33,652 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:54:33,653 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:54:33,653 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:54:33,654 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:54:33,654 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:54:33,654 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:54:33,913 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:54:33,914 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000021813136C00>
2025-11-08 06:54:33,914 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:54:33,914 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:54:33,914 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:54:33,914 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:54:33,914 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:54:33,918 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:33,918 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:33,918 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:34,017 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:54:34,017 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.992 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:54:34,017 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:54:34,018 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:54:34,018 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:54:34,018 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:54:34,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:54:34,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 225 | processing task
2025-11-08 06:54:34,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10270
2025-11-08 06:54:34,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | n_tokens = 10192, memory_seq_rm [10192, end)
2025-11-08 06:54:34,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | prompt processing progress, n_tokens = 10206, batch.n_tokens = 14, progress = 0.993768
2025-11-08 06:54:34,027 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | n_tokens = 10206, memory_seq_rm [10206, end)
2025-11-08 06:54:34,027 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | prompt processing progress, n_tokens = 10270, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:54:34,030 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | prompt done, n_tokens = 10270, batch.n_tokens = 64
2025-11-08 06:54:34,540 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 225 | created context checkpoint 5 of 8 (pos_min = 9205, pos_max = 10205, size = 23.473 MiB)
2025-11-08 06:54:51,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 225 |
2025-11-08 06:54:51,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1756.67 ms /    78 tokens (   22.52 ms per token,    44.40 tokens per second)
2025-11-08 06:54:51,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   15311.02 ms /    41 tokens (  373.44 ms per token,     2.68 tokens per second)
2025-11-08 06:54:51,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   17067.70 ms /   119 tokens
2025-11-08 06:54:51,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 225 | stop processing: n_tokens = 10310, truncated = 0
2025-11-08 06:54:51,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:54:51,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:54:51,088 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:54:51,088 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:54:51,088 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:54:51,197 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:54:51,198 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:54:51,199 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:54:51,199 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:54:51,200 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:54:51,200 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:54:51,201 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:54:51,201 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:54:51,202 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:54:51,202 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:54:51,203 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:54:51,203 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:54:51,204 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:54:51,204 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:54:51,204 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:54:51,470 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:54:51,471 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000218131654C0>
2025-11-08 06:54:51,471 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:54:51,471 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:54:51,471 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:54:51,472 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:54:51,472 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:54:51,475 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:51,475 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:51,475 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:51,475 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:54:51,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:54:51,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.997
2025-11-08 06:54:51,573 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:54:51,573 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:54:51,573 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:54:51,574 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:54:51,575 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:54:51,575 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 268 | processing task
2025-11-08 06:54:51,575 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10351
2025-11-08 06:54:51,575 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | n_tokens = 10274, memory_seq_rm [10274, end)
2025-11-08 06:54:51,575 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | prompt processing progress, n_tokens = 10287, batch.n_tokens = 13, progress = 0.993817
2025-11-08 06:54:51,582 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | n_tokens = 10287, memory_seq_rm [10287, end)
2025-11-08 06:54:51,582 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | prompt processing progress, n_tokens = 10351, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:54:51,585 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | prompt done, n_tokens = 10351, batch.n_tokens = 64
2025-11-08 06:54:52,089 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 268 | created context checkpoint 6 of 8 (pos_min = 9286, pos_max = 10286, size = 23.473 MiB)
2025-11-08 06:55:27,646 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 268 |
2025-11-08 06:55:27,646 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1743.58 ms /    77 tokens (   22.64 ms per token,    44.16 tokens per second)
2025-11-08 06:55:27,646 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   34327.20 ms /    91 tokens (  377.22 ms per token,     2.65 tokens per second)
2025-11-08 06:55:27,646 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   36070.78 ms /   168 tokens
2025-11-08 06:55:27,646 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 268 | stop processing: n_tokens = 10441, truncated = 0
2025-11-08 06:55:27,647 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:55:27,647 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:55:27,647 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:55:27,647 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:55:27,647 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:55:27,766 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:55:27,767 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:55:27,768 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:55:27,769 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:55:27,769 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:55:27,770 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:55:27,770 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:55:27,771 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:55:27,771 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:55:27,772 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:55:27,772 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:55:27,773 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:55:27,773 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:55:27,774 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:55:27,774 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:55:28,035 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:55:28,036 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000021813109280>
2025-11-08 06:55:28,036 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:55:28,037 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:55:28,037 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:55:28,037 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:55:28,037 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:55:28,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:55:28,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:55:28,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:55:28,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:55:28,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:55:28,140 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:55:28,140 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.992
2025-11-08 06:55:28,141 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:55:28,141 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:55:28,141 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:55:28,141 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:55:28,143 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:55:28,143 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 361 | processing task
2025-11-08 06:55:28,143 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10432
2025-11-08 06:55:28,143 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | n_tokens = 10354, memory_seq_rm [10354, end)
2025-11-08 06:55:28,143 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | prompt processing progress, n_tokens = 10368, batch.n_tokens = 14, progress = 0.993865
2025-11-08 06:55:28,150 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | n_tokens = 10368, memory_seq_rm [10368, end)
2025-11-08 06:55:28,150 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | prompt processing progress, n_tokens = 10432, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:55:28,154 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | prompt done, n_tokens = 10432, batch.n_tokens = 64
2025-11-08 06:55:28,670 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 361 | created context checkpoint 7 of 8 (pos_min = 9417, pos_max = 10367, size = 22.300 MiB)
2025-11-08 06:56:03,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 361 |
2025-11-08 06:56:03,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1702.92 ms /    78 tokens (   21.83 ms per token,    45.80 tokens per second)
2025-11-08 06:56:03,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   33195.34 ms /    91 tokens (  364.78 ms per token,     2.74 tokens per second)
2025-11-08 06:56:03,041 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   34898.26 ms /   169 tokens
2025-11-08 06:56:03,042 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 361 | stop processing: n_tokens = 10522, truncated = 0
2025-11-08 06:56:03,042 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:56:03,042 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:56:03,043 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:56:03,043 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:56:03,043 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:56:03,147 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:56:03,148 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:56:03,149 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:56:03,149 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:56:03,150 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:56:03,150 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:56:03,151 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:56:03,151 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:56:03,152 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:56:03,152 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:56:03,153 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:56:03,153 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:56:03,154 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:56:03,154 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:56:03,154 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:56:03,435 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:56:03,435 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000218130E4440>
2025-11-08 06:56:03,435 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:56:03,436 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:56:03,436 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:56:03,436 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:56:03,436 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:56:03,440 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:03,440 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:03,440 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:03,440 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:03,440 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:03,440 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:03,540 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:56:03,540 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.992
2025-11-08 06:56:03,540 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:56:03,540 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:56:03,541 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:56:03,541 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:56:03,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:56:03,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 454 | processing task
2025-11-08 06:56:03,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10513
2025-11-08 06:56:03,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | n_tokens = 10439, memory_seq_rm [10439, end)
2025-11-08 06:56:03,542 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | prompt processing progress, n_tokens = 10449, batch.n_tokens = 10, progress = 0.993912
2025-11-08 06:56:03,549 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | n_tokens = 10449, memory_seq_rm [10449, end)
2025-11-08 06:56:03,549 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | prompt processing progress, n_tokens = 10513, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:56:03,552 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | prompt done, n_tokens = 10513, batch.n_tokens = 64
2025-11-08 06:56:03,994 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 454 | created context checkpoint 8 of 8 (pos_min = 9498, pos_max = 10448, size = 22.300 MiB)
2025-11-08 06:56:23,680 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 454 |
2025-11-08 06:56:23,680 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1636.23 ms /    74 tokens (   22.11 ms per token,    45.23 tokens per second)
2025-11-08 06:56:23,681 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   18501.65 ms /    51 tokens (  362.78 ms per token,     2.76 tokens per second)
2025-11-08 06:56:23,681 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   20137.88 ms /   125 tokens
2025-11-08 06:56:23,681 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 454 | stop processing: n_tokens = 10563, truncated = 0
2025-11-08 06:56:23,681 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:56:23,681 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:56:23,682 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:56:23,682 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:56:23,682 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:56:23,805 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:56:23,805 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:56:23,806 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:56:23,806 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:56:23,807 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:56:23,807 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:56:23,807 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:56:23,808 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:56:23,808 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:56:23,809 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:56:23,809 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:56:23,809 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:56:23,810 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:56:23,810 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:56:23,810 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:56:24,073 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:56:24,073 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311C050>
2025-11-08 06:56:24,074 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:56:24,074 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:56:24,074 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:56:24,074 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:56:24,074 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,078 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:24,178 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:56:24,178 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:56:24,178 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:56:24,179 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:56:24,179 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:56:24,179 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:56:24,180 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:56:24,180 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 507 | processing task
2025-11-08 06:56:24,180 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10594
2025-11-08 06:56:24,180 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | n_tokens = 10520, memory_seq_rm [10520, end)
2025-11-08 06:56:24,180 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | prompt processing progress, n_tokens = 10530, batch.n_tokens = 10, progress = 0.993959
2025-11-08 06:56:24,187 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | n_tokens = 10530, memory_seq_rm [10530, end)
2025-11-08 06:56:24,187 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | prompt processing progress, n_tokens = 10594, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:56:24,191 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | prompt done, n_tokens = 10594, batch.n_tokens = 64
2025-11-08 06:56:24,191 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | erasing old context checkpoint (pos_min = 8887, pos_max = 9910, size = 24.012 MiB)
2025-11-08 06:56:24,639 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 507 | created context checkpoint 8 of 8 (pos_min = 9539, pos_max = 10529, size = 23.238 MiB)
2025-11-08 06:56:44,562 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 507 |
2025-11-08 06:56:44,562 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1635.59 ms /    74 tokens (   22.10 ms per token,    45.24 tokens per second)
2025-11-08 06:56:44,562 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   18746.16 ms /    52 tokens (  360.50 ms per token,     2.77 tokens per second)
2025-11-08 06:56:44,562 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   20381.75 ms /   126 tokens
2025-11-08 06:56:44,563 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 507 | stop processing: n_tokens = 10645, truncated = 0
2025-11-08 06:56:44,563 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:56:44,563 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:56:44,563 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:56:44,563 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:56:44,564 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:56:44,677 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:56:44,678 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:56:44,679 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:56:44,679 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:56:44,680 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:56:44,680 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:56:44,681 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:56:44,682 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:56:44,683 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:56:44,683 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:56:44,684 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:56:44,685 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:56:44,685 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:56:44,686 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:56:44,686 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:56:44,942 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:56:44,943 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FA72A150>
2025-11-08 06:56:44,943 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:56:44,944 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:56:44,944 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:56:44,944 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:56:44,944 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:44,948 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:56:45,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:56:45,048 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.995
2025-11-08 06:56:45,048 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:56:45,048 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:56:45,049 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:56:45,049 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:56:45,050 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:56:45,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 561 | processing task
2025-11-08 06:56:45,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10675
2025-11-08 06:56:45,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | n_tokens = 10597, memory_seq_rm [10597, end)
2025-11-08 06:56:45,051 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | prompt processing progress, n_tokens = 10611, batch.n_tokens = 14, progress = 0.994005
2025-11-08 06:56:45,058 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | n_tokens = 10611, memory_seq_rm [10611, end)
2025-11-08 06:56:45,058 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | prompt processing progress, n_tokens = 10675, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:56:45,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | prompt done, n_tokens = 10675, batch.n_tokens = 64
2025-11-08 06:56:45,061 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | erasing old context checkpoint (pos_min = 9043, pos_max = 9977, size = 21.925 MiB)
2025-11-08 06:56:45,581 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 561 | created context checkpoint 8 of 8 (pos_min = 9621, pos_max = 10610, size = 23.215 MiB)
2025-11-08 06:57:05,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 561 |
2025-11-08 06:57:05,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1698.32 ms /    78 tokens (   21.77 ms per token,    45.93 tokens per second)
2025-11-08 06:57:05,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   18565.01 ms /    51 tokens (  364.02 ms per token,     2.75 tokens per second)
2025-11-08 06:57:05,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   20263.33 ms /   129 tokens
2025-11-08 06:57:05,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 561 | stop processing: n_tokens = 10725, truncated = 0
2025-11-08 06:57:05,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:57:05,316 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:57:05,316 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:57:05,316 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:57:05,316 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:57:05,430 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:57:05,431 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:57:05,431 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:57:05,432 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:57:05,433 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:57:05,433 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:57:05,434 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:57:05,434 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:57:05,435 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:57:05,435 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:57:05,436 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:57:05,436 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:57:05,437 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:57:05,437 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:57:05,437 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:57:05,698 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:57:05,699 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FA7294C0>
2025-11-08 06:57:05,699 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:57:05,699 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:57:05,699 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:57:05,699 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:57:05,699 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:05,807 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:57:05,807 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:57:05,807 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:57:05,808 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:57:05,808 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:57:05,808 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:57:05,810 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:57:05,810 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 614 | processing task
2025-11-08 06:57:05,810 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10756
2025-11-08 06:57:05,810 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | n_tokens = 10678, memory_seq_rm [10678, end)
2025-11-08 06:57:05,810 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | prompt processing progress, n_tokens = 10692, batch.n_tokens = 14, progress = 0.994050
2025-11-08 06:57:05,817 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | n_tokens = 10692, memory_seq_rm [10692, end)
2025-11-08 06:57:05,817 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | prompt processing progress, n_tokens = 10756, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:57:05,821 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | prompt done, n_tokens = 10756, batch.n_tokens = 64
2025-11-08 06:57:05,821 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | erasing old context checkpoint (pos_min = 9043, pos_max = 10043, size = 23.473 MiB)
2025-11-08 06:57:06,328 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 614 | created context checkpoint 8 of 8 (pos_min = 9701, pos_max = 10691, size = 23.238 MiB)
2025-11-08 06:57:26,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 614 |
2025-11-08 06:57:26,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1688.85 ms /    78 tokens (   21.65 ms per token,    46.19 tokens per second)
2025-11-08 06:57:26,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   18809.32 ms /    52 tokens (  361.72 ms per token,     2.76 tokens per second)
2025-11-08 06:57:26,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   20498.16 ms /   130 tokens
2025-11-08 06:57:26,309 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 614 | stop processing: n_tokens = 10807, truncated = 0
2025-11-08 06:57:26,309 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:57:26,309 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:57:26,309 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:57:26,309 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:57:26,310 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:57:26,446 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:57:26,447 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:57:26,447 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:57:26,448 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:57:26,448 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:57:26,449 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:57:26,449 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:57:26,450 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:57:26,450 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:57:26,451 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:57:26,451 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:57:26,452 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:57:26,452 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:57:26,453 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:57:26,453 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:57:26,713 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:57:26,713 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181310BE60>
2025-11-08 06:57:26,713 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:57:26,714 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:57:26,714 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:57:26,714 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:57:26,714 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:26,822 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:57:26,822 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:57:26,823 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:57:26,823 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:57:26,823 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:57:26,823 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:57:26,825 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:57:26,825 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 668 | processing task
2025-11-08 06:57:26,825 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10837
2025-11-08 06:57:26,825 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | n_tokens = 10759, memory_seq_rm [10759, end)
2025-11-08 06:57:26,825 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | prompt processing progress, n_tokens = 10773, batch.n_tokens = 14, progress = 0.994094
2025-11-08 06:57:26,832 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | n_tokens = 10773, memory_seq_rm [10773, end)
2025-11-08 06:57:26,832 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | prompt processing progress, n_tokens = 10837, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:57:26,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | prompt done, n_tokens = 10837, batch.n_tokens = 64
2025-11-08 06:57:26,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | erasing old context checkpoint (pos_min = 9127, pos_max = 10124, size = 23.402 MiB)
2025-11-08 06:57:27,342 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 668 | created context checkpoint 8 of 8 (pos_min = 9783, pos_max = 10772, size = 23.215 MiB)
2025-11-08 06:57:46,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 668 |
2025-11-08 06:57:46,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1696.91 ms /    78 tokens (   21.76 ms per token,    45.97 tokens per second)
2025-11-08 06:57:46,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   18450.79 ms /    51 tokens (  361.78 ms per token,     2.76 tokens per second)
2025-11-08 06:57:46,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   20147.70 ms /   129 tokens
2025-11-08 06:57:46,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 668 | stop processing: n_tokens = 10887, truncated = 0
2025-11-08 06:57:46,973 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:57:46,974 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:57:46,974 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:57:46,975 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:57:46,975 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:57:47,058 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:57:47,058 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:57:47,059 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:57:47,059 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:57:47,060 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:57:47,060 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:57:47,061 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:57:47,061 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:57:47,062 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:57:47,062 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:57:47,063 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:57:47,063 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:57:47,064 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:57:47,064 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:57:47,064 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:57:47,324 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:57:47,324 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AD130>
2025-11-08 06:57:47,326 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:57:47,326 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:57:47,326 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:57:47,326 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:57:47,326 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,331 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:57:47,434 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:57:47,434 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:57:47,434 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:57:47,434 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:57:47,434 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:57:47,434 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:57:47,435 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:57:47,435 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 721 | processing task
2025-11-08 06:57:47,435 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10918
2025-11-08 06:57:47,435 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | n_tokens = 10840, memory_seq_rm [10840, end)
2025-11-08 06:57:47,435 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | prompt processing progress, n_tokens = 10854, batch.n_tokens = 14, progress = 0.994138
2025-11-08 06:57:47,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | n_tokens = 10854, memory_seq_rm [10854, end)
2025-11-08 06:57:47,443 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | prompt processing progress, n_tokens = 10918, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:57:47,447 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | prompt done, n_tokens = 10918, batch.n_tokens = 64
2025-11-08 06:57:47,447 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | erasing old context checkpoint (pos_min = 9205, pos_max = 10205, size = 23.473 MiB)
2025-11-08 06:57:47,960 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 721 | created context checkpoint 8 of 8 (pos_min = 9863, pos_max = 10853, size = 23.238 MiB)
2025-11-08 06:58:13,525 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 721 |
2025-11-08 06:58:13,526 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1707.86 ms /    78 tokens (   21.90 ms per token,    45.67 tokens per second)
2025-11-08 06:58:13,526 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   24381.51 ms /    65 tokens (  375.10 ms per token,     2.67 tokens per second)
2025-11-08 06:58:13,526 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   26089.37 ms /   143 tokens
2025-11-08 06:58:13,526 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 721 | stop processing: n_tokens = 10982, truncated = 0
2025-11-08 06:58:13,526 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:58:13,526 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:58:13,527 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:58:13,527 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:58:13,527 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:58:13,618 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:58:13,619 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:58:13,619 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:58:13,620 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:58:13,620 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:58:13,621 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:58:13,621 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:58:13,622 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:58:13,622 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:58:13,623 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:58:13,623 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:58:13,624 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:58:13,625 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:58:13,625 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:58:13,625 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:58:13,897 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:58:13,899 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AF290>
2025-11-08 06:58:13,899 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:58:13,900 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:58:13,900 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:58:13,900 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:58:13,900 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:13,904 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:14,029 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:58:14,029 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.995
2025-11-08 06:58:14,029 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:58:14,030 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:58:14,030 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:58:14,030 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:58:14,032 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:58:14,032 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 788 | processing task
2025-11-08 06:58:14,033 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 10999
2025-11-08 06:58:14,033 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | n_tokens = 10925, memory_seq_rm [10925, end)
2025-11-08 06:58:14,033 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | prompt processing progress, n_tokens = 10935, batch.n_tokens = 10, progress = 0.994181
2025-11-08 06:58:14,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | n_tokens = 10935, memory_seq_rm [10935, end)
2025-11-08 06:58:14,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | prompt processing progress, n_tokens = 10999, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:58:14,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | prompt done, n_tokens = 10999, batch.n_tokens = 64
2025-11-08 06:58:14,052 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | erasing old context checkpoint (pos_min = 9286, pos_max = 10286, size = 23.473 MiB)
2025-11-08 06:58:14,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 788 | created context checkpoint 8 of 8 (pos_min = 9958, pos_max = 10934, size = 22.910 MiB)
2025-11-08 06:58:33,304 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 788 |
2025-11-08 06:58:33,304 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1698.75 ms /    74 tokens (   22.96 ms per token,    43.56 tokens per second)
2025-11-08 06:58:33,304 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   17572.36 ms /    46 tokens (  382.01 ms per token,     2.62 tokens per second)
2025-11-08 06:58:33,304 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   19271.11 ms /   120 tokens
2025-11-08 06:58:33,305 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 788 | stop processing: n_tokens = 11044, truncated = 0
2025-11-08 06:58:33,305 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:58:33,305 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:58:33,305 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:58:33,305 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:58:33,306 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:58:33,422 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:58:33,422 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:58:33,423 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:58:33,424 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:58:33,424 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:58:33,425 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:58:33,425 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:58:33,426 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:58:33,427 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:58:33,427 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:58:33,428 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:58:33,428 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:58:33,429 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:58:33,429 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:58:33,429 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:58:33,691 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:58:33,693 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AFA40>
2025-11-08 06:58:33,693 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:58:33,694 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:58:33,694 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:58:33,694 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:58:33,694 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,698 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,699 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,699 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,699 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,699 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:33,803 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:58:33,803 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:58:33,803 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:58:33,803 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:58:33,804 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:58:33,804 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:58:33,806 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:58:33,806 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 836 | processing task
2025-11-08 06:58:33,806 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11080
2025-11-08 06:58:33,806 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | n_tokens = 11002, memory_seq_rm [11002, end)
2025-11-08 06:58:33,806 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | prompt processing progress, n_tokens = 11016, batch.n_tokens = 14, progress = 0.994224
2025-11-08 06:58:33,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | n_tokens = 11016, memory_seq_rm [11016, end)
2025-11-08 06:58:33,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | prompt processing progress, n_tokens = 11080, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:58:33,817 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | prompt done, n_tokens = 11080, batch.n_tokens = 64
2025-11-08 06:58:33,817 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | erasing old context checkpoint (pos_min = 9417, pos_max = 10367, size = 22.300 MiB)
2025-11-08 06:58:34,374 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 836 | created context checkpoint 8 of 8 (pos_min = 10020, pos_max = 11015, size = 23.355 MiB)
2025-11-08 06:58:55,999 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 836 |
2025-11-08 06:58:55,999 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1774.38 ms /    78 tokens (   22.75 ms per token,    43.96 tokens per second)
2025-11-08 06:58:55,999 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   20418.33 ms /    55 tokens (  371.24 ms per token,     2.69 tokens per second)
2025-11-08 06:58:55,999 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   22192.71 ms /   133 tokens
2025-11-08 06:58:55,999 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 836 | stop processing: n_tokens = 11134, truncated = 0
2025-11-08 06:58:55,999 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:58:56,000 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:58:56,000 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:58:56,000 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:58:56,000 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:58:56,117 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:58:56,118 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:58:56,118 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:58:56,119 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:58:56,119 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:58:56,119 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:58:56,120 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:58:56,120 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:58:56,121 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:58:56,121 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:58:56,122 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:58:56,123 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:58:56,123 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:58:56,123 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:58:56,124 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:58:56,385 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:58:56,387 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3ADF70>
2025-11-08 06:58:56,387 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:58:56,388 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:58:56,388 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:58:56,388 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:58:56,388 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,392 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:58:56,497 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:58:56,497 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.995
2025-11-08 06:58:56,498 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:58:56,498 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:58:56,498 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:58:56,498 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:58:56,500 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:58:56,500 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 893 | processing task
2025-11-08 06:58:56,500 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11161
2025-11-08 06:58:56,500 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | n_tokens = 11083, memory_seq_rm [11083, end)
2025-11-08 06:58:56,500 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | prompt processing progress, n_tokens = 11097, batch.n_tokens = 14, progress = 0.994266
2025-11-08 06:58:56,508 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | n_tokens = 11097, memory_seq_rm [11097, end)
2025-11-08 06:58:56,508 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | prompt processing progress, n_tokens = 11161, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:58:56,512 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | prompt done, n_tokens = 11161, batch.n_tokens = 64
2025-11-08 06:58:56,512 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | erasing old context checkpoint (pos_min = 9498, pos_max = 10448, size = 22.300 MiB)
2025-11-08 06:58:57,039 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 893 | created context checkpoint 8 of 8 (pos_min = 10110, pos_max = 11096, size = 23.144 MiB)
2025-11-08 06:59:32,564 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 893 |
2025-11-08 06:59:32,564 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1719.80 ms /    78 tokens (   22.05 ms per token,    45.35 tokens per second)
2025-11-08 06:59:32,564 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   34343.09 ms /    92 tokens (  373.29 ms per token,     2.68 tokens per second)
2025-11-08 06:59:32,565 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   36062.89 ms /   170 tokens
2025-11-08 06:59:32,565 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 893 | stop processing: n_tokens = 11252, truncated = 0
2025-11-08 06:59:32,565 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:59:32,565 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:59:32,565 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:59:32,565 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:59:32,566 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:59:32,687 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:59:32,688 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:59:32,689 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:59:32,689 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:59:32,689 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:59:32,690 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:59:32,690 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:59:32,691 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:59:32,691 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:59:32,692 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:59:32,692 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:59:32,693 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:59:32,694 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:59:32,694 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:59:32,694 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:59:32,956 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:59:32,958 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AFE00>
2025-11-08 06:59:32,959 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:59:32,959 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:59:32,959 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:59:32,959 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:59:32,959 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:59:32,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:32,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:33,073 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:59:33,073 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.993
2025-11-08 06:59:33,074 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:59:33,074 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:59:33,074 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:59:33,074 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:59:33,076 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:59:33,076 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 987 | processing task
2025-11-08 06:59:33,076 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11242
2025-11-08 06:59:33,076 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | n_tokens = 11168, memory_seq_rm [11168, end)
2025-11-08 06:59:33,076 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | prompt processing progress, n_tokens = 11178, batch.n_tokens = 10, progress = 0.994307
2025-11-08 06:59:33,083 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | n_tokens = 11178, memory_seq_rm [11178, end)
2025-11-08 06:59:33,084 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | prompt processing progress, n_tokens = 11242, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:59:33,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | prompt done, n_tokens = 11242, batch.n_tokens = 64
2025-11-08 06:59:33,087 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | erasing old context checkpoint (pos_min = 9539, pos_max = 10529, size = 23.238 MiB)
2025-11-08 06:59:33,538 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 987 | created context checkpoint 8 of 8 (pos_min = 10228, pos_max = 11177, size = 22.277 MiB)
2025-11-08 06:59:55,489 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 987 |
2025-11-08 06:59:55,489 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1642.77 ms /    74 tokens (   22.20 ms per token,    45.05 tokens per second)
2025-11-08 06:59:55,489 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   20770.15 ms /    56 tokens (  370.90 ms per token,     2.70 tokens per second)
2025-11-08 06:59:55,489 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   22412.92 ms /   130 tokens
2025-11-08 06:59:55,490 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 987 | stop processing: n_tokens = 11297, truncated = 0
2025-11-08 06:59:55,490 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 06:59:55,490 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 06:59:55,490 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 06:59:55,490 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 06:59:55,491 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 06:59:55,618 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 06:59:55,619 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 06:59:55,620 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 06:59:55,620 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 06:59:55,621 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 06:59:55,621 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 06:59:55,622 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 06:59:55,623 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 06:59:55,623 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 06:59:55,624 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 06:59:55,624 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 06:59:55,625 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 06:59:55,625 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 06:59:55,626 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 06:59:55,626 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 06:59:55,881 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 06:59:55,883 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AF350>
2025-11-08 06:59:55,883 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 06:59:55,883 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 06:59:55,883 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 06:59:55,883 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 06:59:55,883 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,888 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 06:59:55,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 06:59:55,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 06:59:55,994 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 06:59:55,994 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 06:59:55,994 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 06:59:55,995 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 06:59:55,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 06:59:55,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1045 | processing task
2025-11-08 06:59:55,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11323
2025-11-08 06:59:55,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | n_tokens = 11249, memory_seq_rm [11249, end)
2025-11-08 06:59:55,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | prompt processing progress, n_tokens = 11259, batch.n_tokens = 10, progress = 0.994348
2025-11-08 06:59:56,003 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | n_tokens = 11259, memory_seq_rm [11259, end)
2025-11-08 06:59:56,003 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | prompt processing progress, n_tokens = 11323, batch.n_tokens = 64, progress = 1.000000
2025-11-08 06:59:56,007 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | prompt done, n_tokens = 11323, batch.n_tokens = 64
2025-11-08 06:59:56,007 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | erasing old context checkpoint (pos_min = 9621, pos_max = 10610, size = 23.215 MiB)
2025-11-08 06:59:56,456 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1045 | created context checkpoint 8 of 8 (pos_min = 10273, pos_max = 11258, size = 23.121 MiB)
2025-11-08 07:00:20,068 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1045 |
2025-11-08 07:00:20,068 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1651.62 ms /    74 tokens (   22.32 ms per token,    44.80 tokens per second)
2025-11-08 07:00:20,068 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   22419.29 ms /    61 tokens (  367.53 ms per token,     2.72 tokens per second)
2025-11-08 07:00:20,068 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   24070.91 ms /   135 tokens
2025-11-08 07:00:20,069 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1045 | stop processing: n_tokens = 11383, truncated = 0
2025-11-08 07:00:20,069 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:00:20,069 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:00:20,070 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:00:20,070 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:00:20,070 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:00:20,182 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:00:20,183 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:00:20,184 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:00:20,184 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:00:20,185 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:00:20,185 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:00:20,186 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:00:20,186 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:00:20,187 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:00:20,188 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:00:20,188 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:00:20,189 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:00:20,189 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:00:20,190 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:00:20,190 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:00:20,451 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:00:20,458 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181310BB60>
2025-11-08 07:00:20,458 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:00:20,458 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:00:20,458 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:00:20,459 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:00:20,459 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:00:20,462 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,462 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,463 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:20,570 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:00:20,571 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:00:20,571 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:00:20,571 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:00:20,571 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:00:20,571 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:00:20,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:00:20,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1108 | processing task
2025-11-08 07:00:20,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11404
2025-11-08 07:00:20,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | n_tokens = 11326, memory_seq_rm [11326, end)
2025-11-08 07:00:20,573 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | prompt processing progress, n_tokens = 11340, batch.n_tokens = 14, progress = 0.994388
2025-11-08 07:00:20,580 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | n_tokens = 11340, memory_seq_rm [11340, end)
2025-11-08 07:00:20,580 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | prompt processing progress, n_tokens = 11404, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:00:20,584 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | prompt done, n_tokens = 11404, batch.n_tokens = 64
2025-11-08 07:00:20,584 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | erasing old context checkpoint (pos_min = 9701, pos_max = 10691, size = 23.238 MiB)
2025-11-08 07:00:21,088 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1108 | created context checkpoint 8 of 8 (pos_min = 10374, pos_max = 11339, size = 22.652 MiB)
2025-11-08 07:00:43,318 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1108 |
2025-11-08 07:00:43,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1711.75 ms /    78 tokens (   21.95 ms per token,    45.57 tokens per second)
2025-11-08 07:00:43,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   21033.81 ms /    57 tokens (  369.01 ms per token,     2.71 tokens per second)
2025-11-08 07:00:43,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   22745.56 ms /   135 tokens
2025-11-08 07:00:43,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1108 | stop processing: n_tokens = 11460, truncated = 0
2025-11-08 07:00:43,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:00:43,320 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:00:43,320 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:00:43,320 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:00:43,320 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:00:43,481 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:00:43,482 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:00:43,483 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:00:43,484 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:00:43,484 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:00:43,485 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:00:43,485 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:00:43,486 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:00:43,486 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:00:43,486 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:00:43,487 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:00:43,488 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:00:43,488 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:00:43,488 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:00:43,488 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:00:43,747 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:00:43,749 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181310BC20>
2025-11-08 07:00:43,749 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:00:43,749 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:00:43,749 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:00:43,750 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:00:43,750 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,754 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,755 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,755 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:00:43,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:00:43,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:00:43,862 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:00:43,862 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:00:43,862 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:00:43,862 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:00:43,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:00:43,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1167 | processing task
2025-11-08 07:00:43,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11485
2025-11-08 07:00:43,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | n_tokens = 11407, memory_seq_rm [11407, end)
2025-11-08 07:00:43,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | prompt processing progress, n_tokens = 11421, batch.n_tokens = 14, progress = 0.994428
2025-11-08 07:00:43,871 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | n_tokens = 11421, memory_seq_rm [11421, end)
2025-11-08 07:00:43,871 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | prompt processing progress, n_tokens = 11485, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:00:43,875 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | prompt done, n_tokens = 11485, batch.n_tokens = 64
2025-11-08 07:00:43,875 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | erasing old context checkpoint (pos_min = 9783, pos_max = 10772, size = 23.215 MiB)
2025-11-08 07:00:44,390 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1167 | created context checkpoint 8 of 8 (pos_min = 10451, pos_max = 11420, size = 22.746 MiB)
2025-11-08 07:01:06,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1167 |
2025-11-08 07:01:06,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1695.67 ms /    78 tokens (   21.74 ms per token,    46.00 tokens per second)
2025-11-08 07:01:06,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   21113.54 ms /    59 tokens (  357.86 ms per token,     2.79 tokens per second)
2025-11-08 07:01:06,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   22809.21 ms /   137 tokens
2025-11-08 07:01:06,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1167 | stop processing: n_tokens = 11543, truncated = 0
2025-11-08 07:01:06,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:01:06,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:01:06,674 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:01:06,675 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:01:06,675 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:01:06,798 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:01:06,799 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:01:06,800 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:01:06,800 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:01:06,801 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:01:06,802 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:01:06,803 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:01:06,803 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:01:06,804 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:01:06,804 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:01:06,805 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:01:06,805 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:01:06,806 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:01:06,806 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:01:06,806 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:01:07,063 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:01:07,065 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000218130D84A0>
2025-11-08 07:01:07,065 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:01:07,066 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:01:07,066 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:01:07,066 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:01:07,066 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:01:07,070 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,070 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,070 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,070 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,070 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,070 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,071 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:07,181 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:01:07,181 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:01:07,181 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:01:07,181 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:01:07,181 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:01:07,181 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:01:07,183 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:01:07,183 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1228 | processing task
2025-11-08 07:01:07,183 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11566
2025-11-08 07:01:07,183 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | n_tokens = 11492, memory_seq_rm [11492, end)
2025-11-08 07:01:07,184 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | prompt processing progress, n_tokens = 11502, batch.n_tokens = 10, progress = 0.994467
2025-11-08 07:01:07,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | n_tokens = 11502, memory_seq_rm [11502, end)
2025-11-08 07:01:07,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | prompt processing progress, n_tokens = 11566, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:01:07,194 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | prompt done, n_tokens = 11566, batch.n_tokens = 64
2025-11-08 07:01:07,194 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | erasing old context checkpoint (pos_min = 9863, pos_max = 10853, size = 23.238 MiB)
2025-11-08 07:01:07,623 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1228 | created context checkpoint 8 of 8 (pos_min = 10534, pos_max = 11501, size = 22.699 MiB)
2025-11-08 07:01:27,763 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1228 |
2025-11-08 07:01:27,763 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1607.13 ms /    74 tokens (   21.72 ms per token,    46.04 tokens per second)
2025-11-08 07:01:27,763 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   18972.39 ms /    53 tokens (  357.97 ms per token,     2.79 tokens per second)
2025-11-08 07:01:27,763 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   20579.52 ms /   127 tokens
2025-11-08 07:01:27,763 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1228 | stop processing: n_tokens = 11618, truncated = 0
2025-11-08 07:01:27,763 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:01:27,764 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:01:27,764 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:01:27,764 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:01:27,764 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:01:27,872 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:01:27,873 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:01:27,873 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:01:27,874 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:01:27,875 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:01:27,875 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:01:27,876 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:01:27,876 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:01:27,877 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:01:27,877 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:01:27,878 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:01:27,878 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:01:27,878 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:01:27,879 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:01:27,879 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:01:28,137 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:01:28,139 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000218130D84A0>
2025-11-08 07:01:28,139 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:01:28,140 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:01:28,140 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:01:28,140 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:01:28,140 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,145 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:28,264 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:01:28,264 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:01:28,264 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:01:28,264 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:01:28,265 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:01:28,265 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:01:28,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:01:28,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1283 | processing task
2025-11-08 07:01:28,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11647
2025-11-08 07:01:28,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | n_tokens = 11569, memory_seq_rm [11569, end)
2025-11-08 07:01:28,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | prompt processing progress, n_tokens = 11583, batch.n_tokens = 14, progress = 0.994505
2025-11-08 07:01:28,274 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | n_tokens = 11583, memory_seq_rm [11583, end)
2025-11-08 07:01:28,274 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | prompt processing progress, n_tokens = 11647, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:01:28,278 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | prompt done, n_tokens = 11647, batch.n_tokens = 64
2025-11-08 07:01:28,278 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | erasing old context checkpoint (pos_min = 9958, pos_max = 10934, size = 22.910 MiB)
2025-11-08 07:01:28,785 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1283 | created context checkpoint 8 of 8 (pos_min = 10609, pos_max = 11582, size = 22.840 MiB)
2025-11-08 07:01:46,018 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1283 |
2025-11-08 07:01:46,018 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1682.46 ms /    78 tokens (   21.57 ms per token,    46.36 tokens per second)
2025-11-08 07:01:46,018 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   16068.51 ms /    45 tokens (  357.08 ms per token,     2.80 tokens per second)
2025-11-08 07:01:46,018 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   17750.97 ms /   123 tokens
2025-11-08 07:01:46,018 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1283 | stop processing: n_tokens = 11691, truncated = 0
2025-11-08 07:01:46,018 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:01:46,019 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:01:46,019 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:01:46,019 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:01:46,019 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:01:46,126 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:01:46,127 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:01:46,127 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:01:46,128 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:01:46,129 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:01:46,129 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:01:46,130 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:01:46,131 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:01:46,132 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:01:46,133 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:01:46,134 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:01:46,135 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:01:46,135 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:01:46,136 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:01:46,136 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:01:46,396 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:01:46,398 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FA72ACC0>
2025-11-08 07:01:46,398 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:01:46,398 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:01:46,399 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:01:46,399 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:01:46,399 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:01:46,402 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,403 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:01:46,511 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:01:46,511 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:01:46,512 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:01:46,512 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:01:46,512 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:01:46,512 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:01:46,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:01:46,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1330 | processing task
2025-11-08 07:01:46,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11728
2025-11-08 07:01:46,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | n_tokens = 11650, memory_seq_rm [11650, end)
2025-11-08 07:01:46,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | prompt processing progress, n_tokens = 11664, batch.n_tokens = 14, progress = 0.994543
2025-11-08 07:01:46,521 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | n_tokens = 11664, memory_seq_rm [11664, end)
2025-11-08 07:01:46,522 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | prompt processing progress, n_tokens = 11728, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:01:46,525 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | prompt done, n_tokens = 11728, batch.n_tokens = 64
2025-11-08 07:01:46,525 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | erasing old context checkpoint (pos_min = 10020, pos_max = 11015, size = 23.355 MiB)
2025-11-08 07:01:47,030 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1330 | created context checkpoint 8 of 8 (pos_min = 10682, pos_max = 11663, size = 23.027 MiB)
2025-11-08 07:02:30,290 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1330 |
2025-11-08 07:02:30,291 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1691.86 ms /    78 tokens (   21.69 ms per token,    46.10 tokens per second)
2025-11-08 07:02:30,291 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   42083.43 ms /   116 tokens (  362.79 ms per token,     2.76 tokens per second)
2025-11-08 07:02:30,291 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   43775.30 ms /   194 tokens
2025-11-08 07:02:30,291 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1330 | stop processing: n_tokens = 11843, truncated = 0
2025-11-08 07:02:30,291 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:02:30,292 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:02:30,292 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:02:30,292 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:02:30,292 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:02:30,398 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:02:30,399 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:02:30,400 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:02:30,400 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:02:30,401 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:02:30,402 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:02:30,402 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:02:30,403 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:02:30,403 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:02:30,404 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:02:30,404 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:02:30,405 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:02:30,406 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:02:30,406 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:02:30,406 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:02:30,667 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:02:30,669 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181310BFB0>
2025-11-08 07:02:30,669 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:02:30,669 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:02:30,669 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:02:30,670 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:02:30,670 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,674 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,675 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:02:30,791 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:02:30,791 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.991
2025-11-08 07:02:30,791 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:02:30,791 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:02:30,792 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:02:30,792 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:02:30,794 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:02:30,794 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1448 | processing task
2025-11-08 07:02:30,794 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11809
2025-11-08 07:02:30,794 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | n_tokens = 11735, memory_seq_rm [11735, end)
2025-11-08 07:02:30,794 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | prompt processing progress, n_tokens = 11745, batch.n_tokens = 10, progress = 0.994580
2025-11-08 07:02:30,801 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | n_tokens = 11745, memory_seq_rm [11745, end)
2025-11-08 07:02:30,801 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | prompt processing progress, n_tokens = 11809, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:02:30,805 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | prompt done, n_tokens = 11809, batch.n_tokens = 64
2025-11-08 07:02:30,805 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | erasing old context checkpoint (pos_min = 10110, pos_max = 11096, size = 23.144 MiB)
2025-11-08 07:02:31,247 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1448 | created context checkpoint 8 of 8 (pos_min = 10834, pos_max = 11744, size = 21.362 MiB)
2025-11-08 07:03:11,537 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1448 |
2025-11-08 07:03:11,537 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1651.71 ms /    74 tokens (   22.32 ms per token,    44.80 tokens per second)
2025-11-08 07:03:11,537 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   39090.86 ms /   108 tokens (  361.95 ms per token,     2.76 tokens per second)
2025-11-08 07:03:11,537 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   40742.57 ms /   182 tokens
2025-11-08 07:03:11,538 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1448 | stop processing: n_tokens = 11916, truncated = 0
2025-11-08 07:03:11,538 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:03:11,538 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:03:11,538 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:03:11,538 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:03:11,539 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:03:11,656 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:03:11,657 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:03:11,657 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:03:11,658 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:03:11,659 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:03:11,659 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:03:11,660 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:03:11,660 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:03:11,661 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:03:11,661 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:03:11,662 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:03:11,663 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:03:11,664 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:03:11,664 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:03:11,664 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:03:11,923 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:03:11,925 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000218130D8DD0>
2025-11-08 07:03:11,925 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:03:11,926 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:03:11,926 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:03:11,926 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:03:11,926 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:03:11,930 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,930 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,930 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,930 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:11,931 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:12,044 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:03:12,044 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.992
2025-11-08 07:03:12,044 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:03:12,044 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:03:12,045 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:03:12,045 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:03:12,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:03:12,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1558 | processing task
2025-11-08 07:03:12,046 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11890
2025-11-08 07:03:12,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | n_tokens = 11816, memory_seq_rm [11816, end)
2025-11-08 07:03:12,047 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | prompt processing progress, n_tokens = 11826, batch.n_tokens = 10, progress = 0.994617
2025-11-08 07:03:12,053 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | n_tokens = 11826, memory_seq_rm [11826, end)
2025-11-08 07:03:12,054 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | prompt processing progress, n_tokens = 11890, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:03:12,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | prompt done, n_tokens = 11890, batch.n_tokens = 64
2025-11-08 07:03:12,057 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | erasing old context checkpoint (pos_min = 10228, pos_max = 11177, size = 22.277 MiB)
2025-11-08 07:03:12,514 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1558 | created context checkpoint 8 of 8 (pos_min = 10907, pos_max = 11825, size = 21.550 MiB)
2025-11-08 07:03:38,945 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1558 |
2025-11-08 07:03:38,945 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1639.06 ms /    74 tokens (   22.15 ms per token,    45.15 tokens per second)
2025-11-08 07:03:38,945 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   25258.84 ms /    70 tokens (  360.84 ms per token,     2.77 tokens per second)
2025-11-08 07:03:38,945 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   26897.90 ms /   144 tokens
2025-11-08 07:03:38,946 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1558 | stop processing: n_tokens = 11959, truncated = 0
2025-11-08 07:03:38,946 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:03:38,946 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:03:38,947 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:03:38,947 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:03:38,947 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:03:39,066 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:03:39,069 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:03:39,073 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:03:39,074 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:03:39,075 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:03:39,076 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:03:39,076 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:03:39,077 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:03:39,078 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:03:39,079 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:03:39,079 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:03:39,080 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:03:39,081 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:03:39,083 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:03:39,083 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:03:39,354 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:03:39,356 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FA72A300>
2025-11-08 07:03:39,356 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:03:39,356 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:03:39,356 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:03:39,356 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:03:39,356 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,361 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,362 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:39,476 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:03:39,476 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:03:39,476 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:03:39,477 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:03:39,477 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:03:39,477 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:03:39,479 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:03:39,479 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1630 | processing task
2025-11-08 07:03:39,479 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 11971
2025-11-08 07:03:39,479 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | n_tokens = 11897, memory_seq_rm [11897, end)
2025-11-08 07:03:39,479 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | prompt processing progress, n_tokens = 11907, batch.n_tokens = 10, progress = 0.994654
2025-11-08 07:03:39,486 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | n_tokens = 11907, memory_seq_rm [11907, end)
2025-11-08 07:03:39,486 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | prompt processing progress, n_tokens = 11971, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:03:39,490 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | prompt done, n_tokens = 11971, batch.n_tokens = 64
2025-11-08 07:03:39,490 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | erasing old context checkpoint (pos_min = 10273, pos_max = 11258, size = 23.121 MiB)
2025-11-08 07:03:39,925 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1630 | created context checkpoint 8 of 8 (pos_min = 10950, pos_max = 11906, size = 22.441 MiB)
2025-11-08 07:03:56,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1630 |
2025-11-08 07:03:56,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1612.06 ms /    74 tokens (   21.78 ms per token,    45.90 tokens per second)
2025-11-08 07:03:56,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   15704.91 ms /    44 tokens (  356.93 ms per token,     2.80 tokens per second)
2025-11-08 07:03:56,795 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   17316.97 ms /   118 tokens
2025-11-08 07:03:56,796 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1630 | stop processing: n_tokens = 12014, truncated = 0
2025-11-08 07:03:56,796 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:03:56,796 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:03:56,796 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:03:56,796 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:03:56,796 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:03:56,915 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:03:56,916 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:03:56,917 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:03:56,918 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:03:56,918 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:03:56,919 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:03:56,920 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:03:56,920 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:03:56,921 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:03:56,922 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:03:56,922 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:03:56,923 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:03:56,923 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:03:56,924 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:03:56,924 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:03:57,183 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:03:57,184 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AEAE0>
2025-11-08 07:03:57,184 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:03:57,184 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:03:57,184 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:03:57,184 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:03:57,184 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,189 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,190 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:03:57,305 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:03:57,305 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.997
2025-11-08 07:03:57,305 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:03:57,305 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:03:57,306 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:03:57,306 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:03:57,307 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:03:57,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1676 | processing task
2025-11-08 07:03:57,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12052
2025-11-08 07:03:57,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | n_tokens = 11974, memory_seq_rm [11974, end)
2025-11-08 07:03:57,308 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | prompt processing progress, n_tokens = 11988, batch.n_tokens = 14, progress = 0.994690
2025-11-08 07:03:57,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | n_tokens = 11988, memory_seq_rm [11988, end)
2025-11-08 07:03:57,315 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | prompt processing progress, n_tokens = 12052, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:03:57,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | prompt done, n_tokens = 12052, batch.n_tokens = 64
2025-11-08 07:03:57,319 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | erasing old context checkpoint (pos_min = 10374, pos_max = 11339, size = 22.652 MiB)
2025-11-08 07:03:57,830 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1676 | created context checkpoint 8 of 8 (pos_min = 11005, pos_max = 11987, size = 23.051 MiB)
2025-11-08 07:04:22,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1676 |
2025-11-08 07:04:22,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1698.38 ms /    78 tokens (   21.77 ms per token,    45.93 tokens per second)
2025-11-08 07:04:22,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   23442.33 ms /    65 tokens (  360.65 ms per token,     2.77 tokens per second)
2025-11-08 07:04:22,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   25140.71 ms /   143 tokens
2025-11-08 07:04:22,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1676 | stop processing: n_tokens = 12116, truncated = 0
2025-11-08 07:04:22,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:04:22,450 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:04:22,450 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:04:22,450 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:04:22,450 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:04:22,564 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:04:22,564 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:04:22,565 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:04:22,565 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:04:22,566 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:04:22,566 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:04:22,567 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:04:22,568 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:04:22,569 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:04:22,570 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:04:22,570 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:04:22,571 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:04:22,572 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:04:22,572 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:04:22,572 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:04:22,828 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:04:22,829 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AFA10>
2025-11-08 07:04:22,829 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:04:22,830 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:04:22,830 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:04:22,830 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:04:22,830 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,835 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:22,950 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:04:22,950 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:04:22,950 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:04:22,951 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:04:22,951 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:04:22,951 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:04:22,953 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:04:22,953 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1743 | processing task
2025-11-08 07:04:22,953 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12133
2025-11-08 07:04:22,953 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | n_tokens = 12055, memory_seq_rm [12055, end)
2025-11-08 07:04:22,953 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | prompt processing progress, n_tokens = 12069, batch.n_tokens = 14, progress = 0.994725
2025-11-08 07:04:22,960 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | n_tokens = 12069, memory_seq_rm [12069, end)
2025-11-08 07:04:22,960 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | prompt processing progress, n_tokens = 12133, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:04:22,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | prompt done, n_tokens = 12133, batch.n_tokens = 64
2025-11-08 07:04:22,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | erasing old context checkpoint (pos_min = 10451, pos_max = 11420, size = 22.746 MiB)
2025-11-08 07:04:23,476 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1743 | created context checkpoint 8 of 8 (pos_min = 11107, pos_max = 12068, size = 22.558 MiB)
2025-11-08 07:04:48,465 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1743 |
2025-11-08 07:04:48,466 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1704.75 ms /    78 tokens (   21.86 ms per token,    45.75 tokens per second)
2025-11-08 07:04:48,466 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   23807.27 ms /    66 tokens (  360.72 ms per token,     2.77 tokens per second)
2025-11-08 07:04:48,466 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   25512.02 ms /   144 tokens
2025-11-08 07:04:48,466 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1743 | stop processing: n_tokens = 12198, truncated = 0
2025-11-08 07:04:48,466 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:04:48,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:04:48,467 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:04:48,467 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:04:48,467 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:04:48,592 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:04:48,593 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:04:48,594 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:04:48,594 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:04:48,595 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:04:48,595 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:04:48,595 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:04:48,596 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:04:48,597 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:04:48,597 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:04:48,598 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:04:48,598 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:04:48,599 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:04:48,599 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:04:48,600 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:04:48,860 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:04:48,861 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AD6A0>
2025-11-08 07:04:48,861 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:04:48,861 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:04:48,861 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:04:48,862 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:04:48,862 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:04:48,866 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,866 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,866 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,866 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,866 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,866 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,867 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:04:48,990 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:04:48,991 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:04:48,991 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:04:48,991 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:04:48,991 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:04:48,992 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:04:48,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:04:48,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1811 | processing task
2025-11-08 07:04:48,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12214
2025-11-08 07:04:48,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | n_tokens = 12140, memory_seq_rm [12140, end)
2025-11-08 07:04:48,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | prompt processing progress, n_tokens = 12150, batch.n_tokens = 10, progress = 0.994760
2025-11-08 07:04:49,001 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | n_tokens = 12150, memory_seq_rm [12150, end)
2025-11-08 07:04:49,001 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | prompt processing progress, n_tokens = 12214, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:04:49,005 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | prompt done, n_tokens = 12214, batch.n_tokens = 64
2025-11-08 07:04:49,005 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | erasing old context checkpoint (pos_min = 10534, pos_max = 11501, size = 22.699 MiB)
2025-11-08 07:04:49,449 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1811 | created context checkpoint 8 of 8 (pos_min = 11189, pos_max = 12149, size = 22.535 MiB)
2025-11-08 07:05:39,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1811 |
2025-11-08 07:05:39,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1635.36 ms /    74 tokens (   22.10 ms per token,    45.25 tokens per second)
2025-11-08 07:05:39,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   48638.51 ms /   134 tokens (  362.97 ms per token,     2.76 tokens per second)
2025-11-08 07:05:39,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   50273.87 ms /   208 tokens
2025-11-08 07:05:39,268 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1811 | stop processing: n_tokens = 12347, truncated = 0
2025-11-08 07:05:39,268 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:05:39,268 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:05:39,269 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:05:39,269 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:05:39,269 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:05:39,435 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:05:39,436 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:05:39,436 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:05:39,437 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:05:39,438 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:05:39,438 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:05:39,439 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:05:39,440 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:05:39,440 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:05:39,441 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:05:39,441 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:05:39,442 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:05:39,442 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:05:39,442 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:05:39,442 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:05:39,698 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:05:39,698 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FC3AD6A0>
2025-11-08 07:05:39,698 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:05:39,699 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:05:39,699 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:05:39,699 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:05:39,699 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:05:39,703 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,704 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,705 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,705 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,705 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,705 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,705 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:39,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:05:39,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.990
2025-11-08 07:05:39,861 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:05:39,861 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:05:39,862 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:05:39,862 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:05:39,863 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:05:39,863 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1947 | processing task
2025-11-08 07:05:39,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12295
2025-11-08 07:05:39,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | n_tokens = 12221, memory_seq_rm [12221, end)
2025-11-08 07:05:39,864 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | prompt processing progress, n_tokens = 12231, batch.n_tokens = 10, progress = 0.994795
2025-11-08 07:05:39,871 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | n_tokens = 12231, memory_seq_rm [12231, end)
2025-11-08 07:05:39,871 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | prompt processing progress, n_tokens = 12295, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:05:39,875 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | prompt done, n_tokens = 12295, batch.n_tokens = 64
2025-11-08 07:05:39,875 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | erasing old context checkpoint (pos_min = 10609, pos_max = 11582, size = 22.840 MiB)
2025-11-08 07:05:40,328 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1947 | created context checkpoint 8 of 8 (pos_min = 11323, pos_max = 12230, size = 21.292 MiB)
2025-11-08 07:05:59,161 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1947 |
2025-11-08 07:05:59,162 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1643.27 ms /    74 tokens (   22.21 ms per token,    45.03 tokens per second)
2025-11-08 07:05:59,162 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   17654.32 ms /    49 tokens (  360.29 ms per token,     2.78 tokens per second)
2025-11-08 07:05:59,162 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   19297.59 ms /   123 tokens
2025-11-08 07:05:59,162 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1947 | stop processing: n_tokens = 12343, truncated = 0
2025-11-08 07:05:59,163 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:05:59,163 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:05:59,163 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:05:59,163 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:05:59,163 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:05:59,278 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:05:59,279 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:05:59,279 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:05:59,280 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:05:59,280 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:05:59,281 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:05:59,282 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:05:59,282 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:05:59,283 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:05:59,283 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:05:59,284 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:05:59,284 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:05:59,285 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:05:59,285 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:05:59,285 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:05:59,544 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:05:59,544 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311EA20>
2025-11-08 07:05:59,545 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:05:59,545 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:05:59,545 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:05:59,545 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:05:59,545 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,550 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,551 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:05:59,708 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:05:59,708 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:05:59,708 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:05:59,708 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:05:59,708 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:05:59,709 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:05:59,710 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:05:59,710 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 1998 | processing task
2025-11-08 07:05:59,710 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12376
2025-11-08 07:05:59,710 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | n_tokens = 12298, memory_seq_rm [12298, end)
2025-11-08 07:05:59,710 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | prompt processing progress, n_tokens = 12312, batch.n_tokens = 14, progress = 0.994829
2025-11-08 07:05:59,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | n_tokens = 12312, memory_seq_rm [12312, end)
2025-11-08 07:05:59,718 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | prompt processing progress, n_tokens = 12376, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:05:59,722 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | prompt done, n_tokens = 12376, batch.n_tokens = 64
2025-11-08 07:05:59,722 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | erasing old context checkpoint (pos_min = 10682, pos_max = 11663, size = 23.027 MiB)
2025-11-08 07:06:00,232 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 1998 | created context checkpoint 8 of 8 (pos_min = 11371, pos_max = 12311, size = 22.066 MiB)
2025-11-08 07:06:19,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 1998 |
2025-11-08 07:06:19,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1694.20 ms /    78 tokens (   21.72 ms per token,    46.04 tokens per second)
2025-11-08 07:06:19,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   17658.35 ms /    49 tokens (  360.37 ms per token,     2.77 tokens per second)
2025-11-08 07:06:19,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   19352.55 ms /   127 tokens
2025-11-08 07:06:19,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 1998 | stop processing: n_tokens = 12424, truncated = 0
2025-11-08 07:06:19,064 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:06:19,065 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:06:19,065 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:06:19,065 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:06:19,065 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:06:19,192 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:06:19,193 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:06:19,194 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:06:19,195 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:06:19,195 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:06:19,196 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:06:19,196 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:06:19,197 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:06:19,198 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:06:19,198 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:06:19,199 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:06:19,199 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:06:19,200 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:06:19,200 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:06:19,200 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:06:19,461 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:06:19,462 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311CEC0>
2025-11-08 07:06:19,462 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:06:19,462 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:06:19,462 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:06:19,462 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:06:19,462 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,467 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,468 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:19,625 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:06:19,625 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:06:19,626 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:06:19,626 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:06:19,626 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:06:19,626 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:06:19,628 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:06:19,628 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 2049 | processing task
2025-11-08 07:06:19,628 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12457
2025-11-08 07:06:19,628 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | n_tokens = 12379, memory_seq_rm [12379, end)
2025-11-08 07:06:19,628 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | prompt processing progress, n_tokens = 12393, batch.n_tokens = 14, progress = 0.994862
2025-11-08 07:06:19,636 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | n_tokens = 12393, memory_seq_rm [12393, end)
2025-11-08 07:06:19,636 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | prompt processing progress, n_tokens = 12457, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:06:19,640 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | prompt done, n_tokens = 12457, batch.n_tokens = 64
2025-11-08 07:06:19,640 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | erasing old context checkpoint (pos_min = 10834, pos_max = 11744, size = 21.362 MiB)
2025-11-08 07:06:20,144 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2049 | created context checkpoint 8 of 8 (pos_min = 11452, pos_max = 12392, size = 22.066 MiB)
2025-11-08 07:06:46,566 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 2049 |
2025-11-08 07:06:46,566 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1696.20 ms /    78 tokens (   21.75 ms per token,    45.99 tokens per second)
2025-11-08 07:06:46,566 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   25241.40 ms /    70 tokens (  360.59 ms per token,     2.77 tokens per second)
2025-11-08 07:06:46,566 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   26937.61 ms /   148 tokens
2025-11-08 07:06:46,567 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 2049 | stop processing: n_tokens = 12526, truncated = 0
2025-11-08 07:06:46,567 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:06:46,567 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:06:46,567 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:06:46,567 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:06:46,568 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:06:46,689 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:06:46,690 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:06:46,690 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:06:46,691 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:06:46,692 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:06:46,693 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:06:46,694 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:06:46,694 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:06:46,695 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:06:46,696 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:06:46,696 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:06:46,697 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:06:46,698 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:06:46,698 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:06:46,698 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:06:46,957 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:06:46,958 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FA729190>
2025-11-08 07:06:46,958 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:06:46,959 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:06:46,959 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:06:46,959 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:06:46,959 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:06:46,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,963 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,964 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:46,965 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:06:47,124 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:06:47,124 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:06:47,124 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:06:47,125 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:06:47,125 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:06:47,125 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:06:47,127 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:06:47,127 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 2121 | processing task
2025-11-08 07:06:47,127 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12538
2025-11-08 07:06:47,127 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | n_tokens = 12460, memory_seq_rm [12460, end)
2025-11-08 07:06:47,127 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | prompt processing progress, n_tokens = 12474, batch.n_tokens = 14, progress = 0.994896
2025-11-08 07:06:47,135 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | n_tokens = 12474, memory_seq_rm [12474, end)
2025-11-08 07:06:47,135 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | prompt processing progress, n_tokens = 12538, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:06:47,141 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | prompt done, n_tokens = 12538, batch.n_tokens = 64
2025-11-08 07:06:47,141 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | erasing old context checkpoint (pos_min = 10907, pos_max = 11825, size = 21.550 MiB)
2025-11-08 07:06:47,644 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2121 | created context checkpoint 8 of 8 (pos_min = 11554, pos_max = 12473, size = 21.573 MiB)
2025-11-08 07:07:11,768 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 2121 |
2025-11-08 07:07:11,768 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1697.24 ms /    78 tokens (   21.76 ms per token,    45.96 tokens per second)
2025-11-08 07:07:11,768 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   22943.39 ms /    63 tokens (  364.18 ms per token,     2.75 tokens per second)
2025-11-08 07:07:11,768 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   24640.63 ms /   141 tokens
2025-11-08 07:07:11,769 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 2121 | stop processing: n_tokens = 12600, truncated = 0
2025-11-08 07:07:11,769 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:07:11,769 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:07:11,769 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:07:11,770 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:07:11,770 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:07:11,907 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:07:11,916 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:07:11,918 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:07:11,919 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:07:11,920 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:07:11,921 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:07:11,922 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:07:11,923 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:07:11,924 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:07:11,925 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:07:11,926 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:07:11,927 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:07:11,927 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:07:11,928 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:07:11,928 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:07:12,201 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:07:12,202 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311F410>
2025-11-08 07:07:12,202 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:07:12,202 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:07:12,202 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:07:12,202 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:07:12,202 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,207 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,208 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:12,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:07:12,329 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:07:12,329 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:07:12,329 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:07:12,330 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:07:12,330 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:07:12,332 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:07:12,332 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 2186 | processing task
2025-11-08 07:07:12,332 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12619
2025-11-08 07:07:12,332 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | n_tokens = 12545, memory_seq_rm [12545, end)
2025-11-08 07:07:12,332 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | prompt processing progress, n_tokens = 12555, batch.n_tokens = 10, progress = 0.994928
2025-11-08 07:07:12,339 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | n_tokens = 12555, memory_seq_rm [12555, end)
2025-11-08 07:07:12,339 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | prompt processing progress, n_tokens = 12619, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:07:12,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | prompt done, n_tokens = 12619, batch.n_tokens = 64
2025-11-08 07:07:12,343 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | erasing old context checkpoint (pos_min = 10950, pos_max = 11906, size = 22.441 MiB)
2025-11-08 07:07:12,787 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2186 | created context checkpoint 8 of 8 (pos_min = 11628, pos_max = 12554, size = 21.737 MiB)
2025-11-08 07:07:39,411 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 2186 |
2025-11-08 07:07:39,412 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1648.60 ms /    74 tokens (   22.28 ms per token,    44.89 tokens per second)
2025-11-08 07:07:39,412 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   25430.93 ms /    70 tokens (  363.30 ms per token,     2.75 tokens per second)
2025-11-08 07:07:39,412 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   27079.53 ms /   144 tokens
2025-11-08 07:07:39,412 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 2186 | stop processing: n_tokens = 12688, truncated = 0
2025-11-08 07:07:39,413 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:07:39,413 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:07:39,413 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:07:39,413 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:07:39,414 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:07:39,539 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:07:39,540 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:07:39,540 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:07:39,541 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:07:39,542 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:07:39,542 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:07:39,543 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:07:39,543 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:07:39,544 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:07:39,544 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:07:39,545 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:07:39,545 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:07:39,545 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:07:39,546 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:07:39,546 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:07:39,807 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:07:39,808 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000002181311F230>
2025-11-08 07:07:39,808 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:07:39,808 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:07:39,809 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:07:39,809 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:07:39,809 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:07:39,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,813 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,814 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,815 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,815 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,815 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:07:39,982 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:07:39,982 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.995
2025-11-08 07:07:39,982 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:07:39,982 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:07:39,983 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:07:39,983 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:07:39,984 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:07:39,984 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 2258 | processing task
2025-11-08 07:07:39,985 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12700
2025-11-08 07:07:39,985 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | n_tokens = 12622, memory_seq_rm [12622, end)
2025-11-08 07:07:39,985 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | prompt processing progress, n_tokens = 12636, batch.n_tokens = 14, progress = 0.994961
2025-11-08 07:07:39,992 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | n_tokens = 12636, memory_seq_rm [12636, end)
2025-11-08 07:07:39,993 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | prompt processing progress, n_tokens = 12700, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:07:39,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | prompt done, n_tokens = 12700, batch.n_tokens = 64
2025-11-08 07:07:39,996 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | erasing old context checkpoint (pos_min = 11005, pos_max = 11987, size = 23.051 MiB)
2025-11-08 07:07:40,493 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2258 | created context checkpoint 8 of 8 (pos_min = 11716, pos_max = 12635, size = 21.573 MiB)
2025-11-08 07:08:04,265 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 2258 |
2025-11-08 07:08:04,265 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1707.16 ms /    78 tokens (   21.89 ms per token,    45.69 tokens per second)
2025-11-08 07:08:04,265 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   22573.64 ms /    62 tokens (  364.09 ms per token,     2.75 tokens per second)
2025-11-08 07:08:04,266 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   24280.81 ms /   140 tokens
2025-11-08 07:08:04,266 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 2258 | stop processing: n_tokens = 12761, truncated = 0
2025-11-08 07:08:04,266 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:08:04,267 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:08:04,267 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:08:04,267 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:08:04,267 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:08:04,392 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:08:04,393 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:08:04,394 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:08:04,394 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:08:04,395 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:08:04,396 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:08:04,396 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:08:04,397 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:08:04,397 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:08:04,398 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:08:04,399 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:08:04,399 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:08:04,400 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:08:04,400 - DEBUG - root - Runner for gpt-oss-20b-MXFP4 is already running on port 8585.
2025-11-08 07:08:04,400 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:08:04,665 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:08:04,666 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000216FA729FA0>
2025-11-08 07:08:04,666 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:08:04,666 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:08:04,667 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:08:04,667 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:08:04,667 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:08:04,671 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,671 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,671 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,672 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,673 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: Failed to parse arguments: [json.exception.parse_error.101] parse error at line 1, column 6: syntax error while parsing object separator - invalid literal; last read: '"{"fi'; expected ':'
2025-11-08 07:08:04,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  params_from_: Chat format: GPT-OSS
2025-11-08 07:08:04,836 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.996
2025-11-08 07:08:04,837 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:08:04,837 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:08:04,837 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:08:04,837 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:08:04,839 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:08:04,839 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot launch_slot_: id  3 | task 2322 | processing task
2025-11-08 07:08:04,839 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 12781
2025-11-08 07:08:04,839 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | n_tokens = 12707, memory_seq_rm [12707, end)
2025-11-08 07:08:04,839 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | prompt processing progress, n_tokens = 12717, batch.n_tokens = 10, progress = 0.994993
2025-11-08 07:08:04,846 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | n_tokens = 12717, memory_seq_rm [12717, end)
2025-11-08 07:08:04,846 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | prompt processing progress, n_tokens = 12781, batch.n_tokens = 64, progress = 1.000000
2025-11-08 07:08:04,850 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | prompt done, n_tokens = 12781, batch.n_tokens = 64
2025-11-08 07:08:04,850 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | erasing old context checkpoint (pos_min = 11107, pos_max = 12068, size = 22.558 MiB)
2025-11-08 07:08:05,295 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot update_slots: id  3 | task 2322 | created context checkpoint 8 of 8 (pos_min = 11789, pos_max = 12716, size = 21.761 MiB)
2025-11-08 07:08:27,860 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot print_timing: id  3 | task 2322 |
2025-11-08 07:08:27,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: prompt eval time =    1628.71 ms /    74 tokens (   22.01 ms per token,    45.43 tokens per second)
2025-11-08 07:08:27,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: eval time =   21391.83 ms /    60 tokens (  356.53 ms per token,     2.80 tokens per second)
2025-11-08 07:08:27,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: total time =   23020.54 ms /   134 tokens
2025-11-08 07:08:27,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: slot      release: id  3 | task 2322 | stop processing: n_tokens = 12840, truncated = 0
2025-11-08 07:08:27,861 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  update_slots: all slots are idle
2025-11-08 07:08:27,862 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:08:27,862 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:08:27,862 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:08:27,862 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:15:52,825 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: srv    operator(): operator(): cleaning up before exit...
2025-11-08 07:15:52,826 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_memory_breakdown_print: | memory breakdown [MiB] | total   free     self   model   context   compute       unaccounted |
2025-11-08 07:15:52,827 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_memory_breakdown_print: |   - CUDA0 (GTX 1070)   |  8191 =    0 + (12121 = 10949 +     774 +     398) + 17592186040486 |
2025-11-08 07:15:52,827 - DEBUG - root - llama.cpp[gpt-oss-20b-MXFP4]: llama_memory_breakdown_print: |   - Host               |                   656 =   586 +       0 +      70                   |
2025-11-08 07:15:53,155 - DEBUG - root - Log reader for gpt-oss-20b-MXFP4 cancelled.
2025-11-08 07:15:53,155 - INFO - llama_runner.metrics - Runner stopped
2025-11-08 07:15:53,155 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Stopped gpt-oss-20b-MXFP4
2025-11-08 07:15:53,156 - INFO - root - Application exited with code 0.
2025-11-08 07:15:53,190 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 07:34:15,737 - INFO - root - App file logging to: ./config\app.log
2025-11-08 07:34:15,843 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 07:34:15,845 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 07:34:16,265 - INFO - root - LM Studio Proxy listening on http://127.0.0.1:1234
2025-11-08 07:34:16,303 - INFO - root - Ollama Proxy listening on http://127.0.0.1:11434
2025-11-08 07:39:10,425 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:39:10,426 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:39:10,426 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:39:10,427 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:39:10,427 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:39:10,428 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:39:10,428 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:39:10,428 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:39:10,429 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:39:10,429 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:39:10,430 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:39:10,430 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:39:10,431 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:39:14,953 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:39:14,954 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:39:14,954 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:39:14,955 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:39:14,955 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:39:14,955 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:39:14,956 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:39:14,956 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:39:14,957 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:39:14,957 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:39:14,958 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:39:14,958 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:39:14,959 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:39:21,457 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:39:21,458 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:39:21,458 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:39:21,460 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:39:21,460 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:39:21,460 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:39:21,460 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:39:21,460 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:39:21,462 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:39:21,462 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:39:21,462 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:39:21,463 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:39:21,463 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:41:52,139 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:41:52,140 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:41:52,141 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:41:52,142 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:41:52,142 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:41:52,143 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:41:52,144 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:41:52,145 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:41:52,145 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:41:52,146 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:41:52,147 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:41:52,148 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:41:52,149 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:41:52,149 - INFO - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S not running. Requesting startup.
2025-11-08 07:41:52,152 - INFO - llama_runner.services.runner_service - Starting Llama runner for Qwen3-4B-Thinking-2507-Q4_K_S
2025-11-08 07:41:52,153 - INFO - llama_runner.services.runner_service - First runner will use port 8585 for web UI routing
2025-11-08 07:41:52,154 - INFO - llama_runner.metrics - Runner started
2025-11-08 07:41:52,155 - INFO - root - Starting llama.cpp server with command: F:\llm\llama\llama-server.exe --model F:\llm\llama\models\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\Qwen3-4B-Thinking-2507-Q4_K_S.gguf --alias Qwen3-4B-Thinking-2507-Q4_K_S --host 127.0.0.1 --port 8585 --ctx-size 32000 --temp 0.7 --batch-size 1024 --ubatch-size 512 --threads 10 --mlock --no-mmap --flash-attn on --n-gpu-layers 85 --jinja
2025-11-08 07:41:52,189 - INFO - root - Process started with PID: 264912
2025-11-08 07:41:53,294 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2025-11-08 07:41:53,295 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2025-11-08 07:41:53,295 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ggml_cuda_init: found 1 CUDA devices:
2025-11-08 07:41:53,295 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: Device 0: NVIDIA GeForce GTX 1070, compute capability 6.1, VMM: yes
2025-11-08 07:41:53,295 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_backend: loaded CUDA backend from F:\llm\llama\ggml-cuda.dll
2025-11-08 07:41:53,297 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_backend: loaded RPC backend from F:\llm\llama\ggml-rpc.dll
2025-11-08 07:41:53,609 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ggml_vulkan: Found 2 Vulkan devices:
2025-11-08 07:41:53,611 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none
2025-11-08 07:41:53,614 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ggml_vulkan: 1 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none
2025-11-08 07:41:53,614 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_backend: loaded Vulkan backend from F:\llm\llama\ggml-vulkan.dll
2025-11-08 07:41:53,655 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_backend: loaded CPU backend from F:\llm\llama\ggml-cpu-haswell.dll
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: build: 6975 (16bcc1259) with clang version 19.1.5 for x86_64-pc-windows-msvc
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: system info: n_threads = 10, n_threads_batch = 10, total_threads = 12
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: 
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: system_info: n_threads = 10 (n_threads_batch = 10) / 12 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: 
2025-11-08 07:41:53,657 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: binding port with default address family
2025-11-08 07:41:53,661 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: HTTP server is listening, hostname: 127.0.0.1, port: 8585, http threads: 11
2025-11-08 07:41:53,661 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: loading model
2025-11-08 07:41:53,661 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv    load_model: loading model 'F:\llm\llama\models\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\Qwen3-4B-Thinking-2507-Q4_K_S.gguf'
2025-11-08 07:41:53,789 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_load_from_file_impl: skipping device Vulkan0 (NVIDIA GeForce GTX 1070) with id 0000:01:00.0 - already using device CUDA0 (NVIDIA GeForce GTX 1070) with the same id
2025-11-08 07:41:53,789 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1070) (0000:01:00.0) - 7215 MiB free
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: loaded meta data with 42 key-value pairs and 398 tensors from F:\llm\llama\models\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\Qwen3-4B-Thinking-2507-Q4_K_S.gguf (version GGUF V3 (latest))
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   1:                               general.type str              = model
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   2:                               general.name str              = Qwen3-4B-Thinking-2507
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   3:                            general.version str              = 2507
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   4:                           general.finetune str              = Thinking
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   5:                           general.basename str              = Qwen3-4B-Thinking-2507
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   7:                         general.size_label str              = 4B
2025-11-08 07:41:53,828 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   8:                            general.license str              = apache-2.0
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv   9:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-4B-...
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  10:                           general.repo_url str              = https://huggingface.co/unsloth
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  11:                   general.base_model.count u32              = 1
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  12:                  general.base_model.0.name str              = Qwen3 4B Thinking 2507
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  13:               general.base_model.0.version str              = 2507
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  14:          general.base_model.0.organization str              = Qwen
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  15:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-4B-...
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  16:                               general.tags arr[str,2]       = ["unsloth", "text-generation"]
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  17:                          qwen3.block_count u32              = 36
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  18:                       qwen3.context_length u32              = 262144
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  19:                     qwen3.embedding_length u32              = 2560
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  20:                  qwen3.feed_forward_length u32              = 9728
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  21:                 qwen3.attention.head_count u32              = 32
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  22:              qwen3.attention.head_count_kv u32              = 8
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  23:                       qwen3.rope.freq_base f32              = 5000000.000000
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  24:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  25:                 qwen3.attention.key_length u32              = 128
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  26:               qwen3.attention.value_length u32              = 128
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  27:                       tokenizer.ggml.model str              = gpt2
2025-11-08 07:41:53,829 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  28:                         tokenizer.ggml.pre str              = qwen2
2025-11-08 07:41:53,868 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  29:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-11-08 07:41:53,883 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  30:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  32:                tokenizer.ggml.eos_token_id u32              = 151645
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 151654
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = false
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  35:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  36:               general.quantization_version u32              = 2
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  37:                          general.file_type u32              = 14
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  38:                      quantize.imatrix.file str              = Qwen3-4B-Thinking-2507-GGUF/imatrix_u...
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  39:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-4B-Thinking...
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  40:             quantize.imatrix.entries_count u32              = 252
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - kv  41:              quantize.imatrix.chunks_count u32              = 79
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - type  f32:  145 tensors
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - type q4_K:  244 tensors
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - type q5_K:    8 tensors
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_model_loader: - type q6_K:    1 tensors
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: file format = GGUF V3 (latest)
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: file type   = Q4_K - Small
2025-11-08 07:41:53,932 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: file size   = 2.21 GiB (4.73 BPW)
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load: printing all EOG tokens:
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load:   - 151643 ('<|endoftext|>')
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load:   - 151645 ('<|im_end|>')
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load:   - 151662 ('<|fim_pad|>')
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load:   - 151663 ('<|repo_name|>')
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load:   - 151664 ('<|file_sep|>')
2025-11-08 07:41:54,055 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load: special tokens cache size = 26
2025-11-08 07:41:54,088 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load: token to piece cache size = 0.9311 MB
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: arch             = qwen3
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: vocab_only       = 0
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_ctx_train      = 262144
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_embd           = 2560
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_layer          = 36
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_head           = 32
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_head_kv        = 8
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_rot            = 128
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_swa            = 0
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: is_swa_any       = 0
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_embd_head_k    = 128
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_embd_head_v    = 128
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_gqa            = 4
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_embd_k_gqa     = 1024
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_embd_v_gqa     = 1024
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: f_norm_eps       = 0.0e+00
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: f_norm_rms_eps   = 1.0e-06
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: f_clamp_kqv      = 0.0e+00
2025-11-08 07:41:54,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: f_max_alibi_bias = 0.0e+00
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: f_logit_scale    = 0.0e+00
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: f_attn_scale     = 0.0e+00
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_ff             = 9728
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_expert         = 0
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_expert_used    = 0
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_expert_groups  = 0
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_group_used     = 0
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: causal attn      = 1
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: pooling type     = -1
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: rope type        = 2
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: rope scaling     = linear
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: freq_base_train  = 5000000.0
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: freq_scale_train = 1
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_ctx_orig_yarn  = 262144
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: rope_finetuned   = unknown
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: model type       = 4B
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: model params     = 4.02 B
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: general.name     = Qwen3-4B-Thinking-2507
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: vocab type       = BPE
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_vocab          = 151936
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: n_merges         = 151387
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: BOS token        = 11 ','
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOS token        = 151645 '<|im_end|>'
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOT token        = 151645 '<|im_end|>'
2025-11-08 07:41:54,090 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: PAD token        = 151654 '<|vision_pad|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: FIM REP token    = 151663 '<|repo_name|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOG token        = 151643 '<|endoftext|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOG token        = 151645 '<|im_end|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOG token        = 151662 '<|fim_pad|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOG token        = 151663 '<|repo_name|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: EOG token        = 151664 '<|file_sep|>'
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: print_info: max token length = 256
2025-11-08 07:41:54,093 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_tensors: loading model tensors, this can take a while... (mmap = false)
2025-11-08 07:41:54,142 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: 
2025-11-08 07:41:54,147 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_tensors: offloading 36 repeating layers to GPU
2025-11-08 07:41:54,147 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_tensors: offloading output layer to GPU
2025-11-08 07:41:54,147 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_tensors: offloaded 37/37 layers to GPU
2025-11-08 07:41:54,147 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_tensors:          CPU model buffer size =   304.28 MiB
2025-11-08 07:41:54,147 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: load_tensors:        CUDA0 model buffer size =  2267.22 MiB
2025-11-08 07:41:59,975 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: ...............................................................................
2025-11-08 07:41:59,978 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: constructing llama_context
2025-11-08 07:41:59,978 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: n_seq_max     = 4
2025-11-08 07:41:59,978 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: n_ctx         = 32000
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: n_ctx_seq     = 32000
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: n_batch       = 1024
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: n_ubatch      = 512
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: causal_attn   = 1
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: flash_attn    = enabled
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: kv_unified    = true
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: freq_base     = 5000000.0
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: freq_scale    = 1
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: n_ctx_seq (32000) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
2025-11-08 07:41:59,979 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context:  CUDA_Host  output buffer size =     2.32 MiB
2025-11-08 07:41:59,993 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_kv_cache:      CUDA0 KV buffer size =  4500.00 MiB
2025-11-08 07:42:00,148 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_kv_cache: size = 4500.00 MiB ( 32000 cells,  36 layers,  4/1 seqs), K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
2025-11-08 07:42:00,160 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context:      CUDA0 compute buffer size =   301.75 MiB
2025-11-08 07:42:00,160 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context:  CUDA_Host compute buffer size =    67.51 MiB
2025-11-08 07:42:00,160 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: graph nodes  = 1267
2025-11-08 07:42:00,160 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_context: graph splits = 2
2025-11-08 07:42:00,161 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: added <|endoftext|> logit bias = -inf
2025-11-08 07:42:00,161 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: added <|im_end|> logit bias = -inf
2025-11-08 07:42:00,161 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: added <|fim_pad|> logit bias = -inf
2025-11-08 07:42:00,161 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: added <|repo_name|> logit bias = -inf
2025-11-08 07:42:00,161 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: added <|file_sep|> logit bias = -inf
2025-11-08 07:42:00,162 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: setting dry_penalty_last_n to ctx_size = 32000
2025-11-08 07:42:00,162 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv          init: initializing slots, n_slots = 4
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot         init: id  0 | task -1 | new slot, n_ctx = 32000
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot         init: id  1 | task -1 | new slot, n_ctx = 32000
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot         init: id  2 | task -1 | new slot, n_ctx = 32000
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot         init: id  3 | task -1 | new slot, n_ctx = 32000
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv          init: prompt cache is enabled, size limit: 8192 MiB
2025-11-08 07:42:07,914 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv          init: use `--cache-ram 0` to disable the prompt cache
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv          init: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv          init: thinking = 1
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: model loaded
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: chat template, chat_template: {%- if tools %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>system\n' }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if messages[0].role == 'system' %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- messages[0].content + '\n\n' }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- for tool in tools %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- "\n" }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- tool | tojson }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endfor %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- else %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if messages[0].role == 'system' %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- for message in messages[::-1] %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set index = (messages|length - 1) - loop.index0 %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set ns.multi_step_tool = false %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set ns.last_query_index = index %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,915 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endfor %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- for message in messages %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if message.content is string %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set content = message.content %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- else %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set content = '' %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- elif message.role == "assistant" %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set reasoning_content = '' %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if message.reasoning_content is string %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set reasoning_content = message.reasoning_content %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- else %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if '</think>' in content %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\n').split('<think>')|last).lstrip('\n') %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set content = (content.split('</think>')|last).lstrip('\n') %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if loop.index0 > ns.last_query_index %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if loop.last or (not loop.last and reasoning_content) %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- else %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>' + message.role + '\n' + content }}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- else %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>' + message.role + '\n' + content }}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if message.tool_calls %}
2025-11-08 07:42:07,916 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- for tool_call in message.tool_calls %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if (loop.first and content) or (not loop.first) %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '\n' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if tool_call.function %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- set tool_call = tool_call.function %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<tool_call>\n{"name": "' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- tool_call.name }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '", "arguments": ' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if tool_call.arguments is string %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- tool_call.arguments }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- else %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- tool_call.arguments | tojson }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '}\n</tool_call>' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endfor %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_end|>\n' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- elif message.role == "tool" %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>user' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '\n<tool_response>\n' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- content }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '\n</tool_response>' }}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
2025-11-08 07:42:07,917 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_end|>\n' }}
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endfor %}
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- if add_generation_prompt %}
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {{- '<|im_start|>assistant\n<think>\n' }}
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: {%- endif %}, example_format: '<|im_start|>system
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: You are a helpful assistant<|im_end|>
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: <|im_start|>user
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: Hello<|im_end|>
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: <|im_start|>assistant
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: Hi there<|im_end|>
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: <|im_start|>user
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: How are you?<|im_end|>
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: <|im_start|>assistant
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: <think>
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: '
2025-11-08 07:42:07,918 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: main: server is listening on http://127.0.0.1:8585 - starting the main loop
2025-11-08 07:42:07,918 - INFO - root - llama.cpp server for Qwen3-4B-Thinking-2507-Q4_K_S is configured on port 8585
2025-11-08 07:42:07,918 - INFO - llama_runner.metrics - Runner ready
2025-11-08 07:42:07,919 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 07:42:08,006 - INFO - llama_runner.services.runner_service - Llama runner for Qwen3-4B-Thinking-2507-Q4_K_S is ready on port 8585
2025-11-08 07:42:08,006 - INFO - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S is ready on port 8585 after startup.
2025-11-08 07:42:08,007 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:42:08,299 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:42:08,300 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001D2413C4350>
2025-11-08 07:42:08,300 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:42:08,301 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:42:08,301 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:42:08,301 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:42:08,301 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:42:08,395 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-08 07:42:08,398 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
2025-11-08 07:42:08,398 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:42:08,398 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:42:08,399 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:42:08,399 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:42:08,401 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:42:08,401 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  3 | task 0 | processing task
2025-11-08 07:42:08,401 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 18409
2025-11-08 07:42:08,401 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
2025-11-08 07:42:08,401 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 1024, batch.n_tokens = 1024, progress = 0.055625
2025-11-08 07:42:09,062 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 1024, memory_seq_rm [1024, end)
2025-11-08 07:42:09,062 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 1024, progress = 0.111250
2025-11-08 07:42:10,251 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 2048, memory_seq_rm [2048, end)
2025-11-08 07:42:10,251 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 3072, batch.n_tokens = 1024, progress = 0.166875
2025-11-08 07:42:11,651 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 3072, memory_seq_rm [3072, end)
2025-11-08 07:42:11,651 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 1024, progress = 0.222500
2025-11-08 07:42:13,250 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 4096, memory_seq_rm [4096, end)
2025-11-08 07:42:13,251 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 5120, batch.n_tokens = 1024, progress = 0.278125
2025-11-08 07:42:15,043 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 5120, memory_seq_rm [5120, end)
2025-11-08 07:42:15,043 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 1024, progress = 0.333750
2025-11-08 07:42:17,032 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 6144, memory_seq_rm [6144, end)
2025-11-08 07:42:17,033 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 7168, batch.n_tokens = 1024, progress = 0.389375
2025-11-08 07:42:19,219 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 7168, memory_seq_rm [7168, end)
2025-11-08 07:42:19,219 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 1024, progress = 0.445000
2025-11-08 07:42:21,597 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 8192, memory_seq_rm [8192, end)
2025-11-08 07:42:21,597 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 9216, batch.n_tokens = 1024, progress = 0.500625
2025-11-08 07:42:24,168 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 9216, memory_seq_rm [9216, end)
2025-11-08 07:42:24,168 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 1024, progress = 0.556250
2025-11-08 07:42:26,940 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 10240, memory_seq_rm [10240, end)
2025-11-08 07:42:26,940 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 11264, batch.n_tokens = 1024, progress = 0.611875
2025-11-08 07:42:29,908 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 11264, memory_seq_rm [11264, end)
2025-11-08 07:42:29,909 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 1024, progress = 0.667500
2025-11-08 07:42:33,079 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 12288, memory_seq_rm [12288, end)
2025-11-08 07:42:33,079 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 13312, batch.n_tokens = 1024, progress = 0.723125
2025-11-08 07:42:36,467 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 13312, memory_seq_rm [13312, end)
2025-11-08 07:42:36,467 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 1024, progress = 0.778750
2025-11-08 07:42:40,053 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 14336, memory_seq_rm [14336, end)
2025-11-08 07:42:40,053 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 15360, batch.n_tokens = 1024, progress = 0.834374
2025-11-08 07:42:43,838 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 15360, memory_seq_rm [15360, end)
2025-11-08 07:42:43,838 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 1024, progress = 0.889999
2025-11-08 07:42:47,823 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 16384, memory_seq_rm [16384, end)
2025-11-08 07:42:47,823 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 17408, batch.n_tokens = 1024, progress = 0.945624
2025-11-08 07:42:52,004 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | n_tokens = 17408, memory_seq_rm [17408, end)
2025-11-08 07:42:52,004 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 18409, batch.n_tokens = 1001, progress = 1.000000
2025-11-08 07:42:52,011 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 0 | prompt done, n_tokens = 18409, batch.n_tokens = 1001
2025-11-08 07:43:37,399 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot print_timing: id  3 | task 0 |
2025-11-08 07:43:37,399 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: prompt eval time =   50259.12 ms / 18409 tokens (    2.73 ms per token,   366.28 tokens per second)
2025-11-08 07:43:37,399 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: eval time =   38737.92 ms /   960 tokens (   40.35 ms per token,    24.78 tokens per second)
2025-11-08 07:43:37,399 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: total time =   88997.04 ms / 19369 tokens
2025-11-08 07:43:37,399 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot      release: id  3 | task 0 | stop processing: n_tokens = 19368, truncated = 0
2025-11-08 07:43:37,400 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 07:43:37,400 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:43:37,400 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:43:37,400 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:43:37,401 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:43:38,356 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:43:38,357 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:43:38,358 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:43:38,358 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:43:38,359 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:43:38,360 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:43:38,361 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:43:38,361 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:43:38,362 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:43:38,363 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:43:38,363 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:43:38,364 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:43:38,365 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:43:38,365 - DEBUG - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S is already running on port 8585.
2025-11-08 07:43:38,365 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:43:38,625 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:43:38,626 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001D2433CA5A0>
2025-11-08 07:43:38,626 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:43:38,627 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:43:38,627 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:43:38,627 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:43:38,627 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:43:38,730 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-08 07:43:38,733 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.899 (> 0.100 thold), f_keep = 0.950
2025-11-08 07:43:38,734 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:43:38,734 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:43:38,734 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:43:38,734 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:43:38,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:43:38,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  3 | task 978 | processing task
2025-11-08 07:43:38,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 20477
2025-11-08 07:43:38,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | n_tokens = 18406, memory_seq_rm [18406, end)
2025-11-08 07:43:38,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | prompt processing progress, n_tokens = 19430, batch.n_tokens = 1024, progress = 0.948869
2025-11-08 07:43:41,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | n_tokens = 19430, memory_seq_rm [19430, end)
2025-11-08 07:43:41,089 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | prompt processing progress, n_tokens = 20454, batch.n_tokens = 1024, progress = 0.998877
2025-11-08 07:43:45,855 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | n_tokens = 20454, memory_seq_rm [20454, end)
2025-11-08 07:43:45,855 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | prompt processing progress, n_tokens = 20477, batch.n_tokens = 23, progress = 1.000000
2025-11-08 07:43:45,861 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  3 | task 978 | prompt done, n_tokens = 20477, batch.n_tokens = 23
2025-11-08 07:45:30,101 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot print_timing: id  3 | task 978 |
2025-11-08 07:45:30,101 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: prompt eval time =    9751.31 ms /  2071 tokens (    4.71 ms per token,   212.38 tokens per second)
2025-11-08 07:45:30,101 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: eval time =  101607.45 ms /  2300 tokens (   44.18 ms per token,    22.64 tokens per second)
2025-11-08 07:45:30,101 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: total time =  111358.76 ms /  4371 tokens
2025-11-08 07:45:30,105 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot      release: id  3 | task 978 | stop processing: n_tokens = 22776, truncated = 0
2025-11-08 07:45:30,105 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 07:45:30,105 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:45:30,106 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:45:30,106 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:45:30,106 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:46:44,733 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:46:44,733 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:46:44,734 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:46:44,735 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:46:44,735 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:46:44,736 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:46:44,736 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:46:44,736 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:46:44,737 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:46:44,737 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:46:44,738 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:46:44,738 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:46:44,738 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:46:44,739 - DEBUG - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S is already running on port 8585.
2025-11-08 07:46:44,739 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/completions
2025-11-08 07:46:45,003 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:46:45,004 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001D2413BFFE0>
2025-11-08 07:46:45,004 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:46:45,005 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:46:45,005 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:46:45,005 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:46:45,005 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:46:45,012 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:46:45,012 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/completions "HTTP/1.1 200 OK"
2025-11-08 07:46:45,013 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/completions to client (SSE -> SSE)
2025-11-08 07:46:45,013 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:46:45,013 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
2025-11-08 07:46:45,013 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  2 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:46:45,013 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  2 | task 3281 | processing task
2025-11-08 07:46:45,013 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 2068
2025-11-08 07:46:45,013 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | n_tokens = 0, memory_seq_rm [0, end)
2025-11-08 07:46:45,013 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | prompt processing progress, n_tokens = 1024, batch.n_tokens = 1024, progress = 0.495164
2025-11-08 07:46:47,953 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | n_tokens = 1024, memory_seq_rm [1024, end)
2025-11-08 07:46:47,954 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 1024, progress = 0.990329
2025-11-08 07:46:53,523 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | n_tokens = 2048, memory_seq_rm [2048, end)
2025-11-08 07:46:53,523 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | prompt processing progress, n_tokens = 2068, batch.n_tokens = 20, progress = 1.000000
2025-11-08 07:46:53,523 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  2 | task 3281 | prompt done, n_tokens = 2068, batch.n_tokens = 20
2025-11-08 07:51:56,734 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: decode: failed to find a memory slot for batch of size 1
2025-11-08 07:51:56,734 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  try_purge_id: purging slot 3 with 22776 tokens
2025-11-08 07:51:56,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 1024, ret = 1
2025-11-08 07:53:58,168 - DEBUG - httpcore.http11 - receive_response_body.failed exception=CancelledError("Cancelled via cancel scope 1d24312c500 by <Task pending name='Task-41' coro=<RequestResponseCycle.run_asgi() running at F:\\llm\\llama-runner-async-proxy\\dev-venv\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py:403> cb=[set.discard()]>")
2025-11-08 07:53:58,168 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:53:58,168 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:53:58,225 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  cancel_tasks: cancel task, id_task = 3281
2025-11-08 07:53:58,225 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  log_server_r: request: POST /v1/completions 127.0.0.1 200
2025-11-08 07:53:58,271 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot      release: id  2 | task 3281 | stop processing: n_tokens = 11952, truncated = 0
2025-11-08 07:53:58,271 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 07:54:07,049 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:54:07,050 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:54:07,051 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:54:07,051 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:54:07,052 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:54:07,052 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:54:07,052 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:54:07,053 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:54:07,053 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:54:07,054 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:54:07,054 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:54:07,055 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:54:07,055 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:54:07,055 - DEBUG - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S is already running on port 8585.
2025-11-08 07:54:07,055 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:54:07,316 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:54:07,317 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001D24312E240>
2025-11-08 07:54:07,317 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:54:07,318 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:54:07,318 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:54:07,318 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:54:07,318 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:54:07,420 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-08 07:54:07,423 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
2025-11-08 07:54:07,423 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:54:07,424 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:54:07,424 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:54:07,424 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:54:07,427 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  1 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:54:07,427 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  1 | task 13170 | processing task
2025-11-08 07:54:07,427 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 21053
2025-11-08 07:54:07,427 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 0, memory_seq_rm [0, end)
2025-11-08 07:54:07,427 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 1024, batch.n_tokens = 1024, progress = 0.048639
2025-11-08 07:54:11,235 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 1024, memory_seq_rm [1024, end)
2025-11-08 07:54:11,235 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 1024, progress = 0.097278
2025-11-08 07:54:18,493 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 2048, memory_seq_rm [2048, end)
2025-11-08 07:54:18,493 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 3072, batch.n_tokens = 1024, progress = 0.145917
2025-11-08 07:54:25,782 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 3072, memory_seq_rm [3072, end)
2025-11-08 07:54:25,782 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 1024, progress = 0.194557
2025-11-08 07:54:33,023 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 4096, memory_seq_rm [4096, end)
2025-11-08 07:54:33,023 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 5120, batch.n_tokens = 1024, progress = 0.243196
2025-11-08 07:54:40,300 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 5120, memory_seq_rm [5120, end)
2025-11-08 07:54:40,300 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 1024, progress = 0.291835
2025-11-08 07:54:47,585 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 6144, memory_seq_rm [6144, end)
2025-11-08 07:54:47,585 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 7168, batch.n_tokens = 1024, progress = 0.340474
2025-11-08 07:54:54,871 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 7168, memory_seq_rm [7168, end)
2025-11-08 07:54:54,871 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 1024, progress = 0.389113
2025-11-08 07:55:02,160 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 8192, memory_seq_rm [8192, end)
2025-11-08 07:55:02,160 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 9216, batch.n_tokens = 1024, progress = 0.437752
2025-11-08 07:55:09,453 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 9216, memory_seq_rm [9216, end)
2025-11-08 07:55:09,453 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 1024, progress = 0.486391
2025-11-08 07:55:16,737 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 10240, memory_seq_rm [10240, end)
2025-11-08 07:55:16,738 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 11264, batch.n_tokens = 1024, progress = 0.535031
2025-11-08 07:55:24,023 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 11264, memory_seq_rm [11264, end)
2025-11-08 07:55:24,023 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 1024, progress = 0.583670
2025-11-08 07:55:31,312 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 12288, memory_seq_rm [12288, end)
2025-11-08 07:55:31,312 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 13312, batch.n_tokens = 1024, progress = 0.632309
2025-11-08 07:55:38,603 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 13312, memory_seq_rm [13312, end)
2025-11-08 07:55:38,603 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 1024, progress = 0.680948
2025-11-08 07:55:45,893 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 14336, memory_seq_rm [14336, end)
2025-11-08 07:55:45,893 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 15360, batch.n_tokens = 1024, progress = 0.729587
2025-11-08 07:55:53,185 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 15360, memory_seq_rm [15360, end)
2025-11-08 07:55:53,185 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 1024, progress = 0.778226
2025-11-08 07:56:00,481 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 16384, memory_seq_rm [16384, end)
2025-11-08 07:56:00,481 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 17408, batch.n_tokens = 1024, progress = 0.826866
2025-11-08 07:56:07,762 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 17408, memory_seq_rm [17408, end)
2025-11-08 07:56:07,762 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 1024, progress = 0.875505
2025-11-08 07:56:15,079 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 18432, memory_seq_rm [18432, end)
2025-11-08 07:56:15,079 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 19456, batch.n_tokens = 1024, progress = 0.924144
2025-11-08 07:56:22,389 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 19456, memory_seq_rm [19456, end)
2025-11-08 07:56:22,389 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 1024, progress = 0.972783
2025-11-08 07:56:22,390 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: decode: failed to find a memory slot for batch of size 1024
2025-11-08 07:56:22,390 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  try_purge_id: purging slot 2 with 11952 tokens
2025-11-08 07:56:22,392 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: failed to find free space in the KV cache, retrying with smaller batch size, i = 0, n_batch = 1024, ret = 1
2025-11-08 07:56:28,767 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | n_tokens = 20480, memory_seq_rm [20480, end)
2025-11-08 07:56:28,767 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt processing progress, n_tokens = 21053, batch.n_tokens = 573, progress = 1.000000
2025-11-08 07:56:28,773 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 13170 | prompt done, n_tokens = 21053, batch.n_tokens = 573
2025-11-08 07:57:11,550 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot print_timing: id  1 | task 13170 |
2025-11-08 07:57:11,550 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: prompt eval time =  147123.62 ms / 21053 tokens (    6.99 ms per token,   143.10 tokens per second)
2025-11-08 07:57:11,550 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: eval time =   36998.78 ms /   930 tokens (   39.78 ms per token,    25.14 tokens per second)
2025-11-08 07:57:11,550 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: total time =  184122.40 ms / 21983 tokens
2025-11-08 07:57:11,551 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot      release: id  1 | task 13170 | stop processing: n_tokens = 21982, truncated = 0
2025-11-08 07:57:11,551 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 07:57:11,551 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:57:11,551 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:57:11,551 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:57:11,552 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:57:12,402 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:57:12,404 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:57:12,406 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:57:12,406 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:57:12,407 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:57:12,408 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:57:12,409 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:57:12,409 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:57:12,410 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:57:12,410 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:57:12,412 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:57:12,413 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:57:12,414 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:57:12,414 - DEBUG - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S is already running on port 8585.
2025-11-08 07:57:12,415 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:57:12,696 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:57:12,696 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001D24312EA80>
2025-11-08 07:57:12,697 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:57:12,697 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:57:12,697 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:57:12,697 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:57:12,697 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:57:12,798 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-08 07:57:12,801 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.958
2025-11-08 07:57:12,802 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:57:12,802 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:57:12,802 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:57:12,803 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:57:12,806 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  1 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:57:12,806 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  1 | task 14121 | processing task
2025-11-08 07:57:12,806 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 14121 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 21134
2025-11-08 07:57:12,806 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 14121 | n_tokens = 21050, memory_seq_rm [21050, end)
2025-11-08 07:57:12,806 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 14121 | prompt processing progress, n_tokens = 21134, batch.n_tokens = 84, progress = 1.000000
2025-11-08 07:57:12,812 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 14121 | prompt done, n_tokens = 21134, batch.n_tokens = 84
2025-11-08 07:58:05,649 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot print_timing: id  1 | task 14121 |
2025-11-08 07:58:05,649 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: prompt eval time =     544.49 ms /    84 tokens (    6.48 ms per token,   154.27 tokens per second)
2025-11-08 07:58:05,649 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: eval time =   52298.44 ms /  1276 tokens (   40.99 ms per token,    24.40 tokens per second)
2025-11-08 07:58:05,649 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: total time =   52842.93 ms /  1360 tokens
2025-11-08 07:58:05,649 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot      release: id  1 | task 14121 | stop processing: n_tokens = 22409, truncated = 0
2025-11-08 07:58:05,649 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 07:58:05,650 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 07:58:05,650 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 07:58:05,650 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 07:58:05,650 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 07:59:34,729 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 07:59:34,730 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 07:59:34,730 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 07:59:34,731 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 07:59:34,731 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 07:59:34,732 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 07:59:34,732 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 07:59:34,733 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 07:59:34,734 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 07:59:34,734 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 07:59:34,735 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 07:59:34,736 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 07:59:34,736 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 07:59:34,737 - DEBUG - root - Runner for Qwen3-4B-Thinking-2507-Q4_K_S is already running on port 8585.
2025-11-08 07:59:34,737 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-08 07:59:35,002 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-08 07:59:35,003 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001D24312DB80>
2025-11-08 07:59:35,003 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-08 07:59:35,003 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-08 07:59:35,003 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-08 07:59:35,004 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-08 07:59:35,004 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-08 07:59:35,112 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-08 07:59:35,115 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.943
2025-11-08 07:59:35,116 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-08 07:59:35,116 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-08 07:59:35,116 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-08 07:59:35,116 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-08 07:59:35,120 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  1 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-08 07:59:35,120 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot launch_slot_: id  1 | task 15398 | processing task
2025-11-08 07:59:35,120 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23258
2025-11-08 07:59:35,120 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | n_tokens = 21131, memory_seq_rm [21131, end)
2025-11-08 07:59:35,120 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | prompt processing progress, n_tokens = 22155, batch.n_tokens = 1024, progress = 0.952575
2025-11-08 07:59:37,997 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | n_tokens = 22155, memory_seq_rm [22155, end)
2025-11-08 07:59:37,997 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | prompt processing progress, n_tokens = 23179, batch.n_tokens = 1024, progress = 0.996603
2025-11-08 07:59:43,314 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | n_tokens = 23179, memory_seq_rm [23179, end)
2025-11-08 07:59:43,314 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | prompt processing progress, n_tokens = 23258, batch.n_tokens = 79, progress = 1.000000
2025-11-08 07:59:43,321 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot update_slots: id  1 | task 15398 | prompt done, n_tokens = 23258, batch.n_tokens = 79
2025-11-08 08:01:27,418 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot print_timing: id  1 | task 15398 |
2025-11-08 08:01:27,418 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: prompt eval time =   11458.12 ms /  2127 tokens (    5.39 ms per token,   185.63 tokens per second)
2025-11-08 08:01:27,418 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: eval time =  100834.75 ms /  2180 tokens (   46.25 ms per token,    21.62 tokens per second)
2025-11-08 08:01:27,418 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: total time =  112292.87 ms /  4307 tokens
2025-11-08 08:01:27,423 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: slot      release: id  1 | task 15398 | stop processing: n_tokens = 25437, truncated = 0
2025-11-08 08:01:27,423 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  update_slots: all slots are idle
2025-11-08 08:01:27,423 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-08 08:01:27,423 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-08 08:01:27,424 - DEBUG - httpcore.http11 - response_closed.started
2025-11-08 08:01:27,424 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-08 08:02:34,338 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: srv    operator(): operator(): cleaning up before exit...
2025-11-08 08:02:34,339 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |
2025-11-08 08:02:34,339 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_memory_breakdown_print: |   - CUDA0 (GTX 1070)   |  8191 =   15 + (7068 =  2267 +    4500 +     301) +        1107 |
2025-11-08 08:02:34,339 - DEBUG - root - llama.cpp[Qwen3-4B-Thinking-2507-Q4_K_S]: llama_memory_breakdown_print: |   - Host               |                  371 =   304 +       0 +      67                |
2025-11-08 08:02:34,469 - INFO - root - Ollama Proxy server shut down.
2025-11-08 08:02:34,531 - INFO - root - Process for Qwen3-4B-Thinking-2507-Q4_K_S exited with code 0.
2025-11-08 08:02:34,532 - INFO - llama_runner.metrics - Runner stopped
2025-11-08 08:02:34,685 - INFO - root - LM Studio Proxy server shut down.
2025-11-08 08:02:34,686 - INFO - root - Application exited with code 0.
2025-11-08 08:02:34,746 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 08:08:28,172 - INFO - root - App file logging to: ./config\app.log
2025-11-08 08:08:28,199 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 08:08:28,201 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 08:08:28,465 - INFO - root - LM Studio Proxy listening on http://127.0.0.1:1234
2025-11-08 08:08:28,491 - INFO - root - Ollama Proxy listening on http://127.0.0.1:11434
2025-11-08 08:10:06,719 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-08 08:10:06,720 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-08 08:10:06,721 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-08 08:10:06,721 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-08 08:10:06,722 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-08 08:10:06,722 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-08 08:10:06,723 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-08 08:10:06,724 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-08 08:10:06,724 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-08 08:10:06,725 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-08 08:10:06,725 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-08 08:10:06,726 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-08 08:10:06,727 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-08 10:44:35,427 - INFO - root - App file logging to: ./config\app.log
2025-11-08 10:44:35,458 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 10:44:35,459 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 10:44:35,721 - INFO - root - WebUI available at http://localhost:8081/
2025-11-08 10:44:35,721 - INFO - root - Metrics Dashboard available at http://localhost:8080/
2025-11-08 10:44:35,722 - INFO - root - LM Studio Proxy listening on http://127.0.0.1:1234
2025-11-08 10:44:35,751 - INFO - root - Ollama Proxy listening on http://127.0.0.1:11434
2025-11-08 11:16:16,599 - INFO - root - Ollama Proxy server shut down.
2025-11-08 11:16:16,816 - INFO - root - LM Studio Proxy server shut down.
2025-11-08 11:16:16,817 - INFO - root - Application exited with code 0.
2025-11-08 11:16:16,862 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 11:16:24,413 - INFO - root - App file logging to: ./config\app.log
2025-11-08 11:16:24,415 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 11:16:24,417 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 11:16:24,417 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 11:16:24,418 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 11:16:24,418 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 11:16:24,419 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 11:16:24,419 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 11:16:24,419 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 11:16:24,419 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 11:16:24,420 - INFO - llama_runner.headless_service_manager - All services started successfully. Waiting for shutdown signal...
2025-11-08 11:17:16,991 - INFO - root - Application exited with code 0.
2025-11-08 11:17:17,016 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 11:17:22,622 - INFO - root - App file logging to: ./config\app.log
2025-11-08 11:17:22,624 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 11:17:22,625 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 11:17:22,626 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 11:17:22,627 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 11:17:22,627 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 11:17:22,627 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 11:17:22,627 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 11:17:22,627 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 11:17:22,628 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 11:17:22,628 - INFO - llama_runner.headless_service_manager - All services started successfully. Waiting for shutdown signal...
2025-11-08 12:40:10,547 - INFO - root - Application exited with code 0.
2025-11-08 12:40:10,572 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 12:40:20,441 - INFO - root - App file logging to: ./config\app.log
2025-11-08 12:40:20,443 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 12:40:20,445 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 12:40:20,446 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 12:40:20,447 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 12:40:20,447 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 12:40:20,447 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 12:40:20,447 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 12:40:20,447 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 12:40:20,454 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 12:41:18,317 - INFO - root - Application exited with code 0.
2025-11-08 12:41:18,350 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 12:41:32,180 - INFO - root - App file logging to: ./config\app.log
2025-11-08 12:41:32,182 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 12:41:32,183 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 12:41:32,184 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 12:41:32,185 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 12:41:32,185 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 12:41:32,185 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 12:41:32,185 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 12:41:32,185 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 12:41:32,191 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 12:53:21,256 - INFO - root - Application exited with code 0.
2025-11-08 12:53:21,284 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 12:53:34,090 - INFO - root - App file logging to: ./config\app.log
2025-11-08 12:53:34,092 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 12:53:34,094 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 12:53:34,095 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 12:53:34,095 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 12:53:34,095 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 12:53:34,096 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 12:53:34,096 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 12:53:34,096 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 12:53:34,096 - INFO - llama_runner.headless_service_manager - Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-08 12:53:34,097 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 12:53:34,097 - INFO - llama_runner.headless_service_manager - LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-08 12:53:34,097 - INFO - llama_runner.headless_service_manager - All services started successfully. Waiting for shutdown signal...
2025-11-08 13:38:26,900 - INFO - root - Application exited with code 0.
2025-11-08 13:38:26,925 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-08 14:13:57,319 - INFO - root - App file logging to: ./config\app.log
2025-11-08 14:13:57,321 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-08 14:13:57,323 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-08 14:13:57,324 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-08 14:13:57,324 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-08 14:13:57,325 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-08 14:13:57,325 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-08 14:13:57,325 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-08 14:13:57,325 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-08 14:13:57,326 - INFO - llama_runner.headless_service_manager - Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-08 14:13:57,326 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-08 14:13:57,326 - INFO - llama_runner.headless_service_manager - LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-08 14:13:57,326 - INFO - llama_runner.headless_service_manager - All services started successfully. Waiting for shutdown signal...
