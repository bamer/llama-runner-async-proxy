{
  "default_model": "test-model",
  "config_version": 3,
  "models": {
    "test-model": {
      "model_path": "models/test-model.gguf",
      "llama_cpp_runtime": "test-runtime",
      "parameters": {
        "ctx_size": 2048,
        "temp": 0.7,
        "n_gpu_layers": 0,
        "port": 8585,
        "host": "127.0.0.1"
      },
      "display_name": "Test Model",
      "auto_discovered": false,
      "auto_update_model": false,
      "has_tools": false
    }
  },
  "runtimes": {
    "test-runtime": {
      "runtime": "python -c \"print('Mock runtime started')\"",
      "supports_tools": false
    },
    "default": {
      "runtime": "python -c \"print('Default runtime started')\"",
      "supports_tools": true
    }
  },
  "default_runtime": "default",
  "concurrentRunners": 1,
  "proxies": {
    "ollama": {
      "enabled": true,
      "port": 11434
    },
    "lmstudio": {
      "enabled": true,
      "port": 1234
    }
  },
  "logging": {
    "prompt_logging_enabled": false
  },
  "llama-runtimes": {
    "test-runtime": {
      "runtime": "python -c \"print('Mock runtime started')\""
    }
  }
}