2025-11-10 01:18:22,452 - INFO - __main__ - ğŸ SCRIPT DE DÃ‰BOGAGE COMPLET DU DÃ‰MARRAGE
2025-11-10 01:18:22,454 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-10 01:18:22,456 - INFO - __main__ - ğŸš€ DÃ‰MARRAGE COMPLET AVEC DÃ‰BOGAGE DÃ‰TAILLÃ‰
2025-11-10 01:18:22,456 - INFO - __main__ - ğŸ Python version: 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]
2025-11-10 01:18:22,457 - INFO - __main__ - ğŸ“ RÃ©pertoire courant: F:\llm\llama-runner-async-proxy
2025-11-10 01:18:22,457 - INFO - __main__ - ğŸ”§ Ã‰TAPE 1: Chargement de la configuration...
2025-11-10 01:18:23,970 - DEBUG - root - Successfully imported GGUFReader and LlamaFileType.
2025-11-10 01:18:23,970 - DEBUG - root - GGUF_AVAILABLE status after import attempt: True
2025-11-10 01:18:23,994 - INFO - root - Ensured metadata cache directory exists: F:\llm\llama-runner-async-proxy\config\metadata_cache
2025-11-10 01:18:25,438 - INFO - root - Updated dynamic routing handlers for /v1/chat/completions, /v1/completions, /v1/embeddings to support conditional streaming.
2025-11-10 01:18:25,523 - INFO - __main__ - âœ… Configuration chargÃ©e avec 9 clÃ©s
2025-11-10 01:18:25,524 - DEBUG - __main__ - ğŸ”§ Configuration complÃ¨te: {'default_model': 'test-model', 'config_version': 3, 'models': {'test-model': {'model_path': 'models/test-model.gguf', 'llama_cpp_runtime': 'test-runtime', 'parameters': {'ctx_size': 2048, 'temp': 0.7, 'n_gpu_layers': 0, 'port': 8585, 'host': '127.0.0.1'}, 'display_name': 'Test Model', 'auto_discovered': False, 'auto_update_model': False, 'has_tools': False}}, 'runtimes': {'test-runtime': {'runtime': 'python -c "print(\'Mock runtime started\')"', 'supports_tools': False}, 'default': {'runtime': 'python -c "print(\'Default runtime started\')"', 'supports_tools': True}}, 'default_runtime': 'default', 'concurrentRunners': 1, 'proxies': {'ollama': {'enabled': True, 'port': 11434}, 'lmstudio': {'enabled': True, 'port': 1234}}, 'logging': {'prompt_logging_enabled': False}, 'llama-runtimes': {'test-runtime': {'runtime': 'python -c "print(\'Mock runtime started\')"'}}}
2025-11-10 01:18:25,524 - INFO - __main__ - ğŸ”§ Ã‰TAPE 2: Initialisation du HeadlessServiceManager...
2025-11-10 01:18:25,524 - INFO - __main__ - ğŸ“‹ Nombre de modÃ¨les configurÃ©s: 1
2025-11-10 01:18:25,525 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-10 01:18:25,525 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-10 01:18:25,534 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-10 01:18:25,535 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-10 01:18:25,535 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-10 01:18:25,535 - INFO - __main__ - âœ… HeadlessServiceManager initialisÃ© avec succÃ¨s
2025-11-10 01:18:25,535 - INFO - __main__ - ğŸ”§ Ã‰TAPE 3: DÃ©marrage des services...
2025-11-10 01:18:25,536 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-10 01:18:25,536 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-10 01:18:25,537 - INFO - llama_runner.headless_service_manager - âœ… Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-10 01:18:25,537 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-10 01:18:25,538 - INFO - llama_runner.headless_service_manager - âœ… LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-10 01:18:25,538 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-10 01:18:25,538 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-10 01:18:25,538 - INFO - llama_runner.headless_service_manager - âœ… All services started successfully. Waiting for shutdown signal...
2025-11-10 01:18:25,539 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-10 01:18:25,539 - INFO - llama_runner.headless_service_manager - ğŸŒ SERVICES ACCESSIBLES :
2025-11-10 01:18:25,539 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-10 01:18:25,539 - INFO - llama_runner.headless_service_manager - ğŸ  llama.cpp WebUI: http://localhost:8035/
2025-11-10 01:18:25,539 - INFO - llama_runner.headless_service_manager -    âœ… Direct access - no proxy needed
2025-11-10 01:18:25,539 - INFO - llama_runner.headless_service_manager - ğŸ  Llama Runner WebUI: http://localhost:8081/
2025-11-10 01:18:25,540 - INFO - llama_runner.headless_service_manager - ğŸ”— Ollama Proxy: http://localhost:11434/
2025-11-10 01:18:25,540 - INFO - llama_runner.headless_service_manager - ğŸ”— LM Studio Proxy: http://localhost:1234/
2025-11-10 01:18:25,540 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-10 01:18:25,540 - INFO - __main__ - â³ Service en cours d'exÃ©cution...
2025-11-10 01:18:25,540 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:18:55,565 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:19:25,581 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:19:55,588 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:20:25,616 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:20:55,611 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:21:25,630 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:21:55,646 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:22:25,620 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:22:55,645 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:23:25,627 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:23:55,633 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:24:25,636 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:24:55,632 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:25:25,645 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:25:55,662 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:26:25,657 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:26:55,657 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:27:25,658 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:27:55,675 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:28:25,675 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:28:55,689 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:29:08,511 - ERROR - root - File not found for size check: models/test-model.gguf
2025-11-10 01:29:08,511 - WARNING - root - Non-blocking error: Could not get size for models/test-model.gguf. Cannot use cache.
2025-11-10 01:29:25,702 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:29:55,708 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:30:25,704 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:30:55,717 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:31:25,736 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:31:55,753 - INFO - __main__ - ğŸ’“ Service toujours actif...
2025-11-10 01:31:59,038 - ERROR - asyncio - Task was destroyed but it is pending!
task: <Task pending name='Task-1' coro=<debug_full_startup() running at F:\llm\llama-runner-async-proxy\debug_full_startup.py:45> wait_for=<_GatheringFuture pending cb=[Task.task_wakeup()]> cb=[_wait.<locals>._on_completion() at C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\tasks.py:534]>
2025-11-10 01:31:59,039 - ERROR - asyncio - Task was destroyed but it is pending!
task: <Task pending name='Task-2' coro=<keep_alive() running at F:\llm\llama-runner-async-proxy\debug_full_startup.py:73> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[_wait.<locals>._on_completion() at C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\tasks.py:534]>
2025-11-10 01:31:59,040 - ERROR - asyncio - Task exception was never retrieved
future: <Task finished name='Task-4' coro=<Server.serve() done, defined at F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py:69> exception=KeyboardInterrupt()>
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\debug_full_startup.py", line 93, in <module>
    done, pending = loop.run_until_complete(asyncio.wait(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 674, in run_until_complete
    self.run_forever()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\windows_events.py", line 322, in run_forever
    super().run_forever()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 641, in run_forever
    self._run_once()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 1987, in _run_once
    handle._run()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\events.py", line 88, in _run
    self._context.run(self._callback, *self._args)
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py", line 70, in serve
    with self.capture_signals():
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\contextlib.py", line 144, in __exit__
    next(self.gen)
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py", line 331, in capture_signals
    signal.raise_signal(captured_signal)
KeyboardInterrupt
