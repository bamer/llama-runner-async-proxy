2025-11-10 23:28:59,926 - INFO - root - App file logging to: logs\app.log
2025-11-10 23:28:59,928 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-10 23:28:59,930 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-10 23:28:59,931 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-10 23:28:59,932 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-10 23:28:59,932 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-10 23:28:59,932 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-10 23:28:59,933 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-10 23:28:59,933 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-10 23:28:59,933 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-10 23:28:59,933 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-10 23:28:59,934 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-10 23:28:59,934 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-10 23:28:59,935 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-10 23:28:59,935 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-10 23:28:59,935 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-10 23:28:59,936 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-10 23:28:59,936 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-10 23:28:59,937 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-10 23:28:59,937 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-10 23:28:59,938 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-10 23:28:59,939 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-10 23:28:59,939 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-10 23:28:59,939 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-10 23:28:59,939 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-10 23:28:59,940 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-10 23:28:59,940 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-10 23:28:59,940 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-10 23:28:59,940 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-10 23:28:59,940 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-10 23:28:59,940 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-10 23:28:59,941 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-10 23:28:59,941 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-10 23:31:28,395 - INFO - root - Application exited with code 0.
2025-11-10 23:31:28,422 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-10 23:45:39,764 - INFO - root - App file logging to: logs\app.log
2025-11-10 23:45:39,766 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-10 23:45:39,768 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-10 23:45:39,769 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-10 23:45:39,769 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-10 23:45:39,770 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-10 23:45:39,770 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-10 23:45:39,770 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-10 23:45:39,771 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-10 23:45:39,771 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-10 23:45:39,771 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-10 23:45:39,771 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-10 23:45:39,772 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-10 23:45:39,772 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-10 23:45:39,772 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-10 23:45:39,773 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-10 23:45:39,773 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-10 23:45:39,773 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-10 23:45:39,774 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-10 23:45:39,774 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-10 23:45:39,774 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-10 23:45:39,775 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-10 23:45:39,775 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-10 23:45:39,776 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-10 23:45:39,777 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-10 23:51:56,965 - INFO - root - Application exited with code 0.
2025-11-10 23:51:56,991 - DEBUG - qasync._windows._IocpProactor - Closing
