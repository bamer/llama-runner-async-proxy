2025-11-09 03:40:37,051 - INFO - root - App file logging to: logs\app.log
2025-11-09 03:40:37,052 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 03:40:37,053 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 03:40:37,054 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 03:40:37,055 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 03:40:37,056 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 03:40:37,056 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 03:40:37,056 - CRITICAL - root - An unhandled error occurred in main: 'HeadlessServiceManager' object has no attribute 'models_speScific_config'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\main.py", line 244, in main
    asyncio.run(run_app())
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\main.py", line 169, in run_app
    hsm = HeadlessServiceManager(loaded_config, models_config)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\headless_service_manager.py", line 31, in __init__
    self._initialize_services()
  File "F:\llm\llama-runner-async-proxy\llama_runner\headless_service_manager.py", line 85, in _initialize_services
    all_models_config=self.models_speScific_config,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'HeadlessServiceManager' object has no attribute 'models_speScific_config'. Did you mean: 'models_specific_config'?
2025-11-09 03:40:37,059 - INFO - root - Application exited with code 1.
2025-11-09 03:40:37,086 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 03:45:48,573 - INFO - root - App file logging to: logs\app.log
2025-11-09 03:45:48,574 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 03:45:48,575 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 03:45:48,576 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 03:45:48,577 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 03:45:48,577 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 03:45:48,578 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 03:45:48,578 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 03:45:48,578 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 03:45:48,579 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 03:45:48,579 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 03:45:48,579 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 03:45:48,580 - INFO - llama_runner.headless_service_manager - Starting llama.cpp WebUI reverse proxy on port 8080...
2025-11-09 03:45:48,583 - INFO - llama_runner.headless_service_manager - ‚úÖ llama.cpp WebUI reverse proxy started on http://0.0.0.0:8080/
2025-11-09 03:45:48,583 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8080/
2025-11-09 03:45:48,583 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Ensure llama-server.exe is running on port 8000
2025-11-09 03:45:48,583 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 03:45:48,584 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-09 03:45:48,585 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-09 03:45:48,585 - INFO - llama_runner.headless_service_manager - Starting Vue.js Dashboard service on port 8082...
2025-11-09 03:45:48,587 - INFO - llama_runner.headless_service_manager - ‚úÖ Vue.js Dashboard service started on http://0.0.0.0:8082/
2025-11-09 03:45:48,587 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8082/
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8080/
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Requires llama-server.exe running on port 8000
2025-11-09 03:45:48,588 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 03:45:48,589 - INFO - llama_runner.headless_service_manager - üìä Vue.js Dashboard: http://localhost:8082/
2025-11-09 03:45:48,589 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 03:45:48,589 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 03:45:48,589 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 03:46:34,605 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:46:36,795 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:47:40,301 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:47:47,402 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:47:49,610 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:48:18,099 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:48:20,163 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:48:20,318 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:48:21,840 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:49:37,461 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 03:49:37,461 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 03:49:37,462 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 03:49:37,462 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 03:49:37,463 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 03:49:37,463 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 03:49:37,464 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 03:49:37,465 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 03:49:37,465 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 03:49:37,466 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 03:49:37,466 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 03:49:37,467 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 03:49:37,467 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 03:49:37,468 - INFO - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 not running. Requesting startup.
2025-11-09 03:49:37,468 - INFO - llama_runner.services.runner_service - Starting Llama runner for Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 03:49:37,468 - INFO - llama_runner.services.runner_service - First runner will use port 8585 for web UI routing
2025-11-09 03:49:37,468 - INFO - llama_runner.metrics - Runner started
2025-11-09 03:49:37,469 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Started Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 03:49:37,469 - INFO - root - Starting llama.cpp server on port 8585: F:\llm\llama\llama-server.exe --model F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf --alias Dorado-WebSurf_Tool-ext.Q8_0 --host 127.0.0.1 --port 8585 --ctx-size 32000 --temp 0.7 --batch-size 1024 --ubatch-size 512 --threads 10 --mlock --no-mmap --flash-attn on --n-gpu-layers 85 --repeat-penalty 1.05 --top-p 0.8 --top-k 20 --jinja
2025-11-09 03:49:38,085 - INFO - root - Process started with PID: 25092
2025-11-09 03:49:41,159 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2025-11-09 03:49:41,159 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2025-11-09 03:49:41,159 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: found 1 CUDA devices:
2025-11-09 03:49:41,160 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: Device 0: NVIDIA GeForce GTX 1070, compute capability 6.1, VMM: yes
2025-11-09 03:49:41,160 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded CUDA backend from F:\llm\llama\ggml-cuda.dll
2025-11-09 03:49:41,185 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded RPC backend from F:\llm\llama\ggml-rpc.dll
2025-11-09 03:49:41,516 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: Found 2 Vulkan devices:
2025-11-09 03:49:41,519 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none
2025-11-09 03:49:41,521 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: 1 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none
2025-11-09 03:49:41,521 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded Vulkan backend from F:\llm\llama\ggml-vulkan.dll
2025-11-09 03:49:41,787 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded CPU backend from F:\llm\llama\ggml-cpu-haswell.dll
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: build: 6975 (16bcc1259) with clang version 19.1.5 for x86_64-pc-windows-msvc
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: system info: n_threads = 10, n_threads_batch = 10, total_threads = 12
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: 
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: system_info: n_threads = 10 (n_threads_batch = 10) / 12 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: 
2025-11-09 03:49:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: binding port with default address family
2025-11-09 03:49:41,791 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: couldn't bind HTTP server socket, hostname: 127.0.0.1, port: 8585
2025-11-09 03:49:41,791 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv    operator(): operator(): cleaning up before exit...
2025-11-09 03:49:41,828 - INFO - root - Process for Dorado-WebSurf_Tool-ext.Q8_0 exited with code 1.
2025-11-09 03:49:41,829 - ERROR - root - Llama.cpp server for Dorado-WebSurf_Tool-ext.Q8_0 exited unexpectedly with code 1.
2025-11-09 03:49:41,829 - ERROR - llama_runner.metrics - Runner error
2025-11-09 03:49:41,829 - ERROR - llama_runner.headless_service_manager - Runner error for Dorado-WebSurf_Tool-ext.Q8_0: Llama.cpp server for Dorado-WebSurf_Tool-ext.Q8_0 exited unexpectedly with code 1.
2025-11-09 03:49:41,829 - INFO - llama_runner.metrics - Runner stopped
2025-11-09 03:49:41,829 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Stopped Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 03:50:51,533 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:50:53,598 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:50:53,722 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:50:56,103 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:50:58,152 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:00,205 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:00,328 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:02,401 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:14,082 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:16,220 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:19,151 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:51:21,233 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 03:55:36,482 - ERROR - root - Timeout waiting for runner Dorado-WebSurf_Tool-ext.Q8_0 to start.
2025-11-09 04:22:39,497 - INFO - root - Application exited with code 0.
2025-11-09 04:22:39,526 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 04:22:57,870 - INFO - root - App file logging to: logs\app.log
2025-11-09 04:22:57,871 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 04:22:57,873 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 04:22:57,874 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 04:22:57,875 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 04:22:57,875 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 04:22:57,875 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 04:22:57,875 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 04:22:57,875 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 04:22:57,876 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 04:22:57,876 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 04:22:57,877 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 04:22:57,877 - INFO - llama_runner.headless_service_manager - Starting llama.cpp WebUI reverse proxy on port 8080...
2025-11-09 04:22:57,881 - INFO - llama_runner.headless_service_manager - ‚úÖ llama.cpp WebUI reverse proxy started on http://0.0.0.0:8080/
2025-11-09 04:22:57,881 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8080/
2025-11-09 04:22:57,881 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Ensure llama-server.exe is running on port 8000
2025-11-09 04:22:57,881 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 04:22:57,882 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-09 04:22:57,882 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-09 04:22:57,882 - INFO - llama_runner.headless_service_manager - Starting Vue.js Dashboard service on port 8082...
2025-11-09 04:22:57,885 - INFO - llama_runner.headless_service_manager - ‚úÖ Vue.js Dashboard service started on http://0.0.0.0:8082/
2025-11-09 04:22:57,885 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8082/
2025-11-09 04:22:57,886 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 04:22:57,886 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 04:22:57,886 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 04:22:57,886 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 04:22:57,886 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8080/
2025-11-09 04:22:57,886 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Requires llama-server.exe running on port 8000
2025-11-09 04:22:57,887 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 04:22:57,887 - INFO - llama_runner.headless_service_manager - üìä Vue.js Dashboard: http://localhost:8082/
2025-11-09 04:22:57,887 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 04:22:57,887 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 04:22:57,887 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 04:23:27,634 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:23:29,703 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:23:29,859 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:23:45,436 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:23:47,128 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:23:49,262 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:24:07,394 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:24:09,464 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:24:09,619 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:08,947 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:11,079 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:19,272 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:21,343 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:21,514 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:40,814 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:25:42,947 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:28:11,545 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:28:13,615 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:28:26,343 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:28:28,473 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:28:37,790 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:28:39,920 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:33:20,630 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 04:33:20,633 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 04:33:20,635 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 04:33:20,636 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 04:33:20,640 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 04:33:20,641 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 04:33:20,642 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 04:33:20,643 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 04:33:20,644 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 04:33:20,645 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 04:33:20,646 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 04:33:20,647 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 04:33:20,649 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 04:33:28,648 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 04:33:28,649 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 04:33:28,649 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 04:33:28,650 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 04:33:28,650 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 04:33:28,651 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 04:33:28,652 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 04:33:28,652 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 04:33:28,653 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 04:33:28,653 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 04:33:28,654 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 04:33:28,655 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 04:33:28,655 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 04:33:30,308 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 04:33:30,308 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 04:33:30,309 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 04:33:30,309 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 04:33:30,310 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 04:33:30,311 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 04:33:30,311 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 04:33:30,312 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 04:33:30,313 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 04:33:30,313 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 04:33:30,314 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 04:33:30,315 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 04:33:30,315 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 04:39:39,849 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 04:39:39,850 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 04:39:39,851 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 04:39:39,852 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 04:39:39,852 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 04:39:39,853 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 04:39:39,854 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 04:39:39,854 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 04:39:39,855 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 04:39:39,856 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 04:39:39,857 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 04:39:39,858 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 04:39:39,858 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 04:39:39,859 - INFO - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 not running. Requesting startup.
2025-11-09 04:39:39,859 - INFO - llama_runner.services.runner_service - Starting Llama runner for Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 04:39:39,859 - INFO - llama_runner.services.runner_service - First runner will use port 8585 for web UI routing
2025-11-09 04:39:39,860 - INFO - llama_runner.metrics - Runner started
2025-11-09 04:39:39,860 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Started Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 04:39:39,860 - INFO - root - Starting llama.cpp server on port 8585: F:\llm\llama\llama-server.exe --model F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf --alias Dorado-WebSurf_Tool-ext.Q8_0 --host 192.168.0.3 --port 8585 --ctx-size 32000 --temp 0.7 --batch-size 1024 --ubatch-size 512 --threads 10 --mlock --no-mmap --flash-attn on --n-gpu-layers 85 --repeat-penalty 1.05 --top-p 0.8 --top-k 20 --jinja
2025-11-09 04:39:39,890 - INFO - root - Process started with PID: 28920
2025-11-09 04:39:39,992 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2025-11-09 04:39:39,992 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2025-11-09 04:39:39,992 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: found 1 CUDA devices:
2025-11-09 04:39:39,992 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: Device 0: NVIDIA GeForce GTX 1070, compute capability 6.1, VMM: yes
2025-11-09 04:39:39,992 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded CUDA backend from F:\llm\llama\ggml-cuda.dll
2025-11-09 04:39:39,993 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded RPC backend from F:\llm\llama\ggml-rpc.dll
2025-11-09 04:39:40,106 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: Found 2 Vulkan devices:
2025-11-09 04:39:40,109 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none
2025-11-09 04:39:40,111 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: 1 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none
2025-11-09 04:39:40,111 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded Vulkan backend from F:\llm\llama\ggml-vulkan.dll
2025-11-09 04:39:40,136 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded CPU backend from F:\llm\llama\ggml-cpu-haswell.dll
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: build: 6975 (16bcc1259) with clang version 19.1.5 for x86_64-pc-windows-msvc
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: system info: n_threads = 10, n_threads_batch = 10, total_threads = 12
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: 
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: system_info: n_threads = 10 (n_threads_batch = 10) / 12 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: 
2025-11-09 04:39:40,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: binding port with default address family
2025-11-09 04:39:40,139 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: couldn't bind HTTP server socket, hostname: 192.168.0.3, port: 8585
2025-11-09 04:39:40,139 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv    operator(): operator(): cleaning up before exit...
2025-11-09 04:39:40,166 - INFO - root - Process for Dorado-WebSurf_Tool-ext.Q8_0 exited with code 1.
2025-11-09 04:39:40,166 - ERROR - root - Llama.cpp server for Dorado-WebSurf_Tool-ext.Q8_0 exited unexpectedly with code 1.
2025-11-09 04:39:40,166 - ERROR - llama_runner.metrics - Runner error
2025-11-09 04:39:40,166 - ERROR - llama_runner.headless_service_manager - Runner error for Dorado-WebSurf_Tool-ext.Q8_0: Llama.cpp server for Dorado-WebSurf_Tool-ext.Q8_0 exited unexpectedly with code 1.
2025-11-09 04:39:40,166 - INFO - llama_runner.metrics - Runner stopped
2025-11-09 04:39:40,166 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Stopped Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 04:41:29,351 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:41:31,410 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:41:31,612 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:01,757 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:03,823 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:03,979 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:31,520 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:34,135 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:36,264 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:43,878 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:46,023 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:53,362 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:42:55,495 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 192.168.1.3:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:43:38,352 - INFO - root - App file logging to: logs\app.log
2025-11-09 04:43:38,353 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 04:43:38,355 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 04:43:38,356 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 04:43:38,356 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 04:43:38,357 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 04:43:38,357 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 04:43:38,357 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 04:43:38,357 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 04:43:38,358 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 04:43:38,358 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 04:43:38,358 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 04:43:38,358 - INFO - llama_runner.headless_service_manager - Starting llama.cpp WebUI reverse proxy on port 8080...
2025-11-09 04:43:38,361 - INFO - llama_runner.headless_service_manager - ‚úÖ llama.cpp WebUI reverse proxy started on http://0.0.0.0:8080/
2025-11-09 04:43:38,362 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8080/
2025-11-09 04:43:38,362 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Ensure llama-server.exe is running on port 8000
2025-11-09 04:43:38,362 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 04:43:38,363 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-09 04:43:38,363 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-09 04:43:38,363 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 04:43:38,363 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 04:43:38,363 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 04:43:38,363 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8080/
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Requires llama-server.exe running on port 8000
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager - üìä Vue.js Dashboard: http://localhost:8082/
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 04:43:38,364 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 04:43:38,402 - INFO - root - Application exited with code 0.
2025-11-09 04:43:38,462 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 04:43:38,462 - ERROR - asyncio - Task exception was never retrieved
future: <Task finished name='Task-1' coro=<main.<locals>.run_app() done, defined at F:\llm\llama-runner-async-proxy\main.py:164> exception=SystemExit(1)>
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py", line 164, in startup
    server = await loop.create_server(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 1572, in create_server
    raise OSError(err.errno, msg) from None
OSError: [Errno 10048] error while attempting to bind on address ('0.0.0.0', 11434): une seule utilisation de chaque adresse de socket (protocole/adresse r√©seau/port) est habituellement autoris√©e

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\main.py", line 244, in main
    asyncio.run(run_app())
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 193, in run
    with Runner(debug=debug, loop_factory=loop_factory) as runner:
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 62, in __exit__
    self.close()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 70, in close
    _cancel_all_tasks(loop)
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 205, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 674, in run_until_complete
    self.run_forever()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\windows_events.py", line 322, in run_forever
    super().run_forever()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 641, in run_forever
    self._run_once()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 1987, in _run_once
    handle._run()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\events.py", line 88, in _run
    self._context.run(self._callback, *self._args)
  File "F:\llm\llama-runner-async-proxy\main.py", line 190, in run_app
    await hsm.start_services()
  File "F:\llm\llama-runner-async-proxy\llama_runner\headless_service_manager.py", line 310, in start_services
    await asyncio.gather(*self.running_tasks)
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 674, in run_until_complete
    self.run_forever()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\windows_events.py", line 322, in run_forever
    super().run_forever()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 641, in run_forever
    self._run_once()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\base_events.py", line 1987, in _run_once
    handle._run()
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\events.py", line 88, in _run
    self._context.run(self._callback, *self._args)
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py", line 71, in serve
    await self._serve(sockets)
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py", line 86, in _serve
    await self.startup(sockets=sockets)
  File "F:\llm\llama-runner-async-proxy\dev-venv\Lib\site-packages\uvicorn\server.py", line 174, in startup
    sys.exit(1)
SystemExit: 1
2025-11-09 04:45:38,858 - ERROR - root - Timeout waiting for runner Dorado-WebSurf_Tool-ext.Q8_0 to start.
2025-11-09 04:45:39,132 - INFO - root - Application exited with code 0.
2025-11-09 04:45:39,161 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 04:46:59,248 - INFO - root - App file logging to: logs\app.log
2025-11-09 04:46:59,249 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 04:46:59,251 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 04:46:59,252 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 04:46:59,253 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 04:46:59,253 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 04:46:59,253 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 04:46:59,253 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 04:46:59,253 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 04:46:59,254 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 04:46:59,254 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 04:46:59,255 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 04:46:59,255 - INFO - llama_runner.headless_service_manager - Starting llama.cpp WebUI reverse proxy on port 8080...
2025-11-09 04:46:59,259 - INFO - llama_runner.headless_service_manager - ‚úÖ llama.cpp WebUI reverse proxy started on http://0.0.0.0:8080/
2025-11-09 04:46:59,259 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8080/
2025-11-09 04:46:59,259 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Ensure llama-server.exe is running on port 8000
2025-11-09 04:46:59,259 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 04:46:59,260 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-09 04:46:59,260 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-09 04:46:59,260 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8080/
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Requires llama-server.exe running on port 8000
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 04:46:59,261 - INFO - llama_runner.headless_service_manager - üìä Vue.js Dashboard: http://localhost:8082/
2025-11-09 04:46:59,262 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 04:46:59,262 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 04:46:59,262 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 04:47:28,825 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:47:30,925 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:48:00,567 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:48:02,635 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:48:02,821 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:48:13,841 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:48:15,897 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 04:48:16,067 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8000 ssl:default [Le syst√®me distant a refus√© la connexion r√©seau]
2025-11-09 05:50:18,264 - INFO - root - Application exited with code 0.
2025-11-09 05:50:18,291 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 05:50:36,311 - INFO - root - App file logging to: logs\app.log
2025-11-09 05:50:36,312 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 05:50:36,313 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 05:50:36,314 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 05:50:36,315 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 05:50:36,315 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 05:50:36,316 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 05:50:36,316 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 05:50:36,316 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 05:50:36,317 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 05:50:36,317 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 05:50:36,317 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 05:50:36,318 - INFO - llama_runner.headless_service_manager - Starting llama.cpp WebUI reverse proxy on port 8080...
2025-11-09 05:50:36,321 - INFO - llama_runner.headless_service_manager - ‚úÖ llama.cpp WebUI reverse proxy started on http://0.0.0.0:8080/
2025-11-09 05:50:36,321 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8080/
2025-11-09 05:50:36,321 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Ensure llama-server.exe is running on port 8000
2025-11-09 05:50:36,322 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 05:50:36,323 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-09 05:50:36,323 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-09 05:50:36,323 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 05:50:36,323 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 05:50:36,323 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 05:50:36,323 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8080/
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Requires llama-server.exe running on port 8000
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager - üìä Vue.js Dashboard: http://localhost:8082/
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 05:50:36,324 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 06:40:41,504 - INFO - root - Application exited with code 0.
2025-11-09 06:40:41,532 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 06:41:03,290 - INFO - root - App file logging to: logs\app.log
2025-11-09 06:41:03,291 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 06:41:03,293 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 06:41:03,294 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 06:41:03,295 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 06:41:03,295 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 06:41:03,295 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 06:41:03,295 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 06:41:03,295 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 06:41:03,296 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 06:41:03,296 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 06:41:03,296 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 06:41:03,297 - INFO - llama_runner.headless_service_manager - Starting llama.cpp WebUI reverse proxy on port 8033...
2025-11-09 06:41:03,297 - INFO - llama_runner.headless_service_manager - ‚úÖ llama.cpp WebUI reverse proxy started on http://127.0.0.1:8033/
2025-11-09 06:41:03,297 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://127.0.0.1:8033/
2025-11-09 06:41:03,297 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Ensure llama-server.exe is running on port 8033
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8080/
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager -    ‚ö†Ô∏è  Requires llama-server.exe running on port 8000
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - üìä Vue.js Dashboard: http://localhost:8082/
2025-11-09 06:41:03,298 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 06:41:03,299 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 06:41:03,299 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 06:42:44,537 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8033 ssl:default [Une op√©ration sur un socket n‚Äôa pas pu √™tre effectu√©e car le syst√®me ne disposait pas de suffisamment d‚Äôespace dans la m√©moire tampon ou parce que la file d‚Äôattente √©tait satur√©e]
2025-11-09 06:43:08,876 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8033 ssl:default [Vous n‚Äô√©tiez pas connect√© car il y avait un nom en double sur le r√©seau. Si vous joignez un domaine,¬†ouvrez le Panneau de configuration Syst√®me et modifiez le nom de l‚Äôordinateur, puis r√©essayez. Si vous joignez un groupe de travail, choisissez un autre nom pour ce groupe]
2025-11-09 06:44:10,762 - ERROR - llama_runner.headless_service_manager - Error proxying to llama.cpp: Cannot connect to host 127.0.0.1:8033 ssl:default [Vous n‚Äô√©tiez pas connect√© car il y avait un nom en double sur le r√©seau. Si vous joignez un domaine,¬†ouvrez le Panneau de configuration Syst√®me et modifiez le nom de l‚Äôordinateur, puis r√©essayez. Si vous joignez un groupe de travail, choisissez un autre nom pour ce groupe]
2025-11-09 07:11:49,207 - INFO - root - Application exited with code 0.
2025-11-09 07:11:49,384 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 07:12:34,186 - INFO - root - App file logging to: logs\app.log
2025-11-09 07:12:34,188 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 07:12:34,189 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 07:12:34,190 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 07:12:34,191 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 07:12:34,191 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 07:12:34,191 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 07:12:34,191 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 07:12:34,191 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 07:12:34,192 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 07:12:34,192 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 07:12:34,193 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 07:12:34,193 - INFO - llama_runner.headless_service_manager - üöÄ Starting llama-server for model: Chroma1-HD-Flash-Q4_K_S
2025-11-09 07:12:34,193 - INFO - llama_runner.services.runner_service - Starting Llama runner for Chroma1-HD-Flash-Q4_K_S
2025-11-09 07:12:34,193 - INFO - llama_runner.services.runner_service - First runner will use port 8585 for web UI routing
2025-11-09 07:12:34,220 - INFO - llama_runner.metrics - Runner started
2025-11-09 07:12:34,220 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Started Chroma1-HD-Flash-Q4_K_S
2025-11-09 07:12:34,220 - INFO - root - Starting llama.cpp server on port 8585: F:\llm\llama\llama-server.exe --model F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf --alias Chroma1-HD-Flash-Q4_K_S --host 127.0.0.1 --jinja -c 0 --port 8585 --ctx-size 32000 --temp 0.7 --batch-size 1024 --ubatch-size 512 --threads 10 --mlock --no-mmap --flash-attn on --n-gpu-layers 85 --jinja
2025-11-09 07:12:34,964 - INFO - root - Process started with PID: 12432
2025-11-09 07:12:42,313 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2025-11-09 07:12:42,313 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2025-11-09 07:12:42,314 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: ggml_cuda_init: found 1 CUDA devices:
2025-11-09 07:12:42,314 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: Device 0: NVIDIA GeForce GTX 1070, compute capability 6.1, VMM: yes
2025-11-09 07:12:42,314 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: load_backend: loaded CUDA backend from F:\llm\llama\ggml-cuda.dll
2025-11-09 07:12:42,337 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: load_backend: loaded RPC backend from F:\llm\llama\ggml-rpc.dll
2025-11-09 07:12:42,448 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: ggml_vulkan: Found 2 Vulkan devices:
2025-11-09 07:12:42,451 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none
2025-11-09 07:12:42,453 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: ggml_vulkan: 1 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none
2025-11-09 07:12:42,453 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: load_backend: loaded Vulkan backend from F:\llm\llama\ggml-vulkan.dll
2025-11-09 07:12:42,681 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: load_backend: loaded CPU backend from F:\llm\llama\ggml-cpu-haswell.dll
2025-11-09 07:12:42,682 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)
2025-11-09 07:12:42,682 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: build: 6992 (aa3b7a90b) with clang version 19.1.5 for x86_64-pc-windows-msvc
2025-11-09 07:12:42,683 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: system info: n_threads = 10, n_threads_batch = 10, total_threads = 12
2025-11-09 07:12:42,683 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: 
2025-11-09 07:12:42,683 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: system_info: n_threads = 10 (n_threads_batch = 10) / 12 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |
2025-11-09 07:12:42,683 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: 
2025-11-09 07:12:42,683 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: main: binding port with default address family
2025-11-09 07:12:42,694 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: main: HTTP server is listening, hostname: 127.0.0.1, port: 8585, http threads: 11
2025-11-09 07:12:42,694 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: main: loading model
2025-11-09 07:12:42,694 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: srv    load_model: loading model 'F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf'
2025-11-09 07:12:42,776 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_load_from_file_impl: skipping device Vulkan0 (NVIDIA GeForce GTX 1070) with id 0000:01:00.0 - already using device CUDA0 (NVIDIA GeForce GTX 1070) with the same id
2025-11-09 07:12:42,777 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1070) (0000:01:00.0) - 7215 MiB free
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: loaded meta data with 3 key-value pairs and 643 tensors from F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf (version GGUF V3 (latest))
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - kv   0:                       general.architecture str              = flux
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - kv   1:               general.quantization_version u32              = 2
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - kv   2:                          general.file_type u32              = 14
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - type  f32:  400 tensors
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - type  f16:    1 tensors
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - type q4_K:  239 tensors
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_loader: - type bf16:    3 tensors
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: print_info: file format = GGUF V3 (latest)
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: print_info: file type   = Q4_K - Small
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: print_info: file size   = 4.69 GiB (4.52 BPW)
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'flux'
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: llama_model_load_from_file_impl: failed to load model
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: common_init_from_params: failed to load model 'F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf', try reducing --n-gpu-layers if you're running out of VRAM
2025-11-09 07:12:42,799 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: srv    load_model: failed to load model, 'F:\llm\llama\models\Chroma1-HD-Flash-Q4_K_S-GGUF\Chroma1-HD-Flash-Q4_K_S.gguf'
2025-11-09 07:12:42,800 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: srv    operator(): operator(): cleaning up before exit...
2025-11-09 07:12:42,800 - DEBUG - root - llama.cpp[Chroma1-HD-Flash-Q4_K_S]: main: exiting due to model loading error
2025-11-09 07:12:42,844 - INFO - root - Process for Chroma1-HD-Flash-Q4_K_S exited with code 1.
2025-11-09 07:12:42,844 - ERROR - root - Llama.cpp server for Chroma1-HD-Flash-Q4_K_S exited unexpectedly with code 1.
2025-11-09 07:12:42,844 - ERROR - llama_runner.metrics - Runner error
2025-11-09 07:12:42,844 - ERROR - llama_runner.headless_service_manager - Runner error for Chroma1-HD-Flash-Q4_K_S: Llama.cpp server for Chroma1-HD-Flash-Q4_K_S exited unexpectedly with code 1.
2025-11-09 07:12:42,844 - INFO - llama_runner.metrics - Runner stopped
2025-11-09 07:12:42,844 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Stopped Chroma1-HD-Flash-Q4_K_S
2025-11-09 07:26:37,312 - INFO - root - Application exited with code 0.
2025-11-09 07:26:37,337 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 07:26:51,870 - INFO - root - App file logging to: logs\app.log
2025-11-09 07:26:51,871 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 07:26:51,873 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 07:26:51,873 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 07:26:51,874 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 07:26:51,874 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 07:26:51,874 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 07:26:51,875 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 07:26:51,875 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 07:26:51,875 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 07:26:51,876 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 07:26:51,876 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 07:26:51,876 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 07:26:51,876 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8000/
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-09 07:26:51,877 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 07:26:51,878 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 07:26:51,878 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 07:26:51,878 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 07:36:56,672 - INFO - root - Application exited with code 0.
2025-11-09 07:36:56,698 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 07:40:48,153 - INFO - root - App file logging to: logs\app.log
2025-11-09 07:40:48,154 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 07:40:48,156 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 07:40:48,157 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 07:40:48,157 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 07:40:48,158 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 07:40:48,158 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 07:40:48,158 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 07:40:48,158 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 07:40:48,159 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 07:40:48,159 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 07:40:48,160 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 07:40:48,160 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 07:40:48,160 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-09 07:40:48,160 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 07:40:48,160 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 07:40:48,160 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 07:40:48,161 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 07:40:48,161 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8000/
2025-11-09 07:40:48,161 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-09 07:40:48,161 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 07:40:48,161 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 07:40:48,161 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 07:40:48,162 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 12:31:25,955 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:31:25,956 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:31:25,956 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:31:25,957 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:31:25,957 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:31:25,958 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:31:25,958 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:31:25,959 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:31:25,959 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:31:25,960 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:31:25,960 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:31:25,961 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:31:25,961 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:31:25,961 - INFO - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 not running. Requesting startup.
2025-11-09 12:31:25,962 - INFO - llama_runner.services.runner_service - Starting Llama runner for Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 12:31:25,962 - INFO - llama_runner.services.runner_service - First runner will use port 8585 for web UI routing
2025-11-09 12:31:25,962 - INFO - llama_runner.metrics - Runner started
2025-11-09 12:31:25,962 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Started Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 12:31:25,963 - INFO - root - Starting llama.cpp server on port 8585: F:\llm\llama\llama-server.exe --model F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf --alias Dorado-WebSurf_Tool-ext.Q8_0 --host 127.0.0.1 --jinja -c 0 --port 8585 --ctx-size 32000 --temp 0.7 --batch-size 1024 --ubatch-size 512 --threads 10 --mlock --no-mmap --flash-attn on --n-gpu-layers 85 --repeat-penalty 1.05 --top-p 0.8 --top-k 20 --jinja
2025-11-09 12:31:26,540 - INFO - root - Process started with PID: 13320
2025-11-09 12:31:26,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2025-11-09 12:31:26,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2025-11-09 12:31:26,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_cuda_init: found 1 CUDA devices:
2025-11-09 12:31:26,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: Device 0: NVIDIA GeForce GTX 1070, compute capability 6.1, VMM: yes
2025-11-09 12:31:26,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded CUDA backend from F:\llm\llama\ggml-cuda.dll
2025-11-09 12:31:26,639 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded RPC backend from F:\llm\llama\ggml-rpc.dll
2025-11-09 12:31:26,757 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: Found 2 Vulkan devices:
2025-11-09 12:31:26,760 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none
2025-11-09 12:31:26,762 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: ggml_vulkan: 1 = AMD Radeon(TM) Graphics (AMD proprietary driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: none
2025-11-09 12:31:26,762 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded Vulkan backend from F:\llm\llama\ggml-vulkan.dll
2025-11-09 12:31:26,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_backend: loaded CPU backend from F:\llm\llama\ggml-cpu-haswell.dll
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: build: 6992 (aa3b7a90b) with clang version 19.1.5 for x86_64-pc-windows-msvc
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: system info: n_threads = 10, n_threads_batch = 10, total_threads = 12
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: 
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: system_info: n_threads = 10 (n_threads_batch = 10) / 12 | CUDA : ARCHS = 500,610,700,750,800,860,890 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: 
2025-11-09 12:31:26,795 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: binding port with default address family
2025-11-09 12:31:26,808 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: HTTP server is listening, hostname: 127.0.0.1, port: 8585, http threads: 11
2025-11-09 12:31:26,808 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: loading model
2025-11-09 12:31:26,808 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv    load_model: loading model 'F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf'
2025-11-09 12:31:26,891 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_load_from_file_impl: skipping device Vulkan0 (NVIDIA GeForce GTX 1070) with id 0000:01:00.0 - already using device CUDA0 (NVIDIA GeForce GTX 1070) with the same id
2025-11-09 12:31:26,891 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce GTX 1070) (0000:01:00.0) - 7215 MiB free
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: loaded meta data with 29 key-value pairs and 398 tensors from F:\llm\llama\models\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\Dorado-WebSurf_Tool-ext.Q8_0.gguf (version GGUF V3 (latest))
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   1:                               general.type str              = model
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   2:                               general.name str              = Model
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   3:                       general.quantized_by str              = Unsloth
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   4:                         general.size_label str              = 4.0B
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   5:                           general.repo_url str              = https://huggingface.co/unsloth
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   6:                               general.tags arr[str,2]       = ["unsloth", "llama.cpp"]
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   7:                          qwen3.block_count u32              = 36
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   8:                       qwen3.context_length u32              = 262144
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2560
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 9728
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 32
2025-11-09 12:31:26,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
2025-11-09 12:31:26,946 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 5000000.000000
2025-11-09 12:31:26,946 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
2025-11-09 12:31:26,946 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
2025-11-09 12:31:26,946 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
2025-11-09 12:31:26,946 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
2025-11-09 12:31:26,946 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
2025-11-09 12:31:26,984 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2025-11-09 12:31:26,999 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2025-11-09 12:31:27,042 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["ƒ† ƒ†", "ƒ†ƒ† ƒ†ƒ†", "i n", "ƒ† t",...
2025-11-09 12:31:27,042 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
2025-11-09 12:31:27,042 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643
2025-11-09 12:31:27,042 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = false
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  27:               general.quantization_version u32              = 2
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - kv  28:                          general.file_type u32              = 7
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - type  f32:  145 tensors
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_model_loader: - type q8_0:  253 tensors
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: file format = GGUF V3 (latest)
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: file type   = Q8_0
2025-11-09 12:31:27,043 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: file size   = 3.98 GiB (8.50 BPW)
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load: printing all EOG tokens:
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load:   - 151643 ('<|endoftext|>')
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load:   - 151645 ('<|im_end|>')
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load:   - 151662 ('<|fim_pad|>')
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load:   - 151663 ('<|repo_name|>')
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load:   - 151664 ('<|file_sep|>')
2025-11-09 12:31:27,172 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load: special tokens cache size = 26
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load: token to piece cache size = 0.9311 MB
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: arch             = qwen3
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: vocab_only       = 0
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_ctx_train      = 262144
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_embd           = 2560
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_embd_inp       = 2560
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_layer          = 36
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_head           = 32
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_head_kv        = 8
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_rot            = 128
2025-11-09 12:31:27,210 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_swa            = 0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: is_swa_any       = 0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_embd_head_k    = 128
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_embd_head_v    = 128
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_gqa            = 4
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_embd_k_gqa     = 1024
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_embd_v_gqa     = 1024
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: f_norm_eps       = 0.0e+00
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: f_norm_rms_eps   = 1.0e-06
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: f_clamp_kqv      = 0.0e+00
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: f_max_alibi_bias = 0.0e+00
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: f_logit_scale    = 0.0e+00
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: f_attn_scale     = 0.0e+00
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_ff             = 9728
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_expert         = 0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_expert_used    = 0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_expert_groups  = 0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_group_used     = 0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: causal attn      = 1
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: pooling type     = -1
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: rope type        = 2
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: rope scaling     = linear
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: freq_base_train  = 5000000.0
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: freq_scale_train = 1
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_ctx_orig_yarn  = 262144
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: rope_finetuned   = unknown
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: model type       = 4B
2025-11-09 12:31:27,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: model params     = 4.02 B
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: general.name     = Model
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: vocab type       = BPE
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_vocab          = 151936
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: n_merges         = 151387
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: BOS token        = 151643 '<|endoftext|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOS token        = 151645 '<|im_end|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOT token        = 151645 '<|im_end|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: PAD token        = 151643 '<|endoftext|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: LF token         = 198 'ƒä'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: FIM REP token    = 151663 '<|repo_name|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOG token        = 151643 '<|endoftext|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOG token        = 151645 '<|im_end|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOG token        = 151662 '<|fim_pad|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOG token        = 151663 '<|repo_name|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: EOG token        = 151664 '<|file_sep|>'
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: print_info: max token length = 256
2025-11-09 12:31:27,212 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_tensors: loading model tensors, this can take a while... (mmap = false)
2025-11-09 12:31:27,429 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_tensors: offloading 36 repeating layers to GPU
2025-11-09 12:31:27,429 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_tensors: offloading output layer to GPU
2025-11-09 12:31:27,429 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_tensors: offloaded 37/37 layers to GPU
2025-11-09 12:31:27,429 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_tensors:        CUDA0 model buffer size =  4076.43 MiB
2025-11-09 12:31:27,429 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: load_tensors:    CUDA_Host model buffer size =   394.12 MiB
2025-11-09 12:31:38,053 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: .....................................................................................
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: constructing llama_context
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: n_seq_max     = 4
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: n_ctx         = 32000
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: n_ctx_seq     = 32000
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: n_batch       = 1024
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: n_ubatch      = 512
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: causal_attn   = 1
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: flash_attn    = enabled
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: kv_unified    = true
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: freq_base     = 5000000.0
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: freq_scale    = 1
2025-11-09 12:31:38,056 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: n_ctx_seq (32000) < n_ctx_train (262144) -- the full capacity of the model will not be utilized
2025-11-09 12:31:38,057 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context:  CUDA_Host  output buffer size =     2.32 MiB
2025-11-09 12:31:38,346 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_kv_cache:      CUDA0 KV buffer size =  4500.00 MiB
2025-11-09 12:31:38,813 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_kv_cache: size = 4500.00 MiB ( 32000 cells,  36 layers,  4/1 seqs), K (f16): 2250.00 MiB, V (f16): 2250.00 MiB
2025-11-09 12:31:38,834 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context:      CUDA0 compute buffer size =   301.75 MiB
2025-11-09 12:31:38,835 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context:  CUDA_Host compute buffer size =    67.51 MiB
2025-11-09 12:31:38,835 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: graph nodes  = 1267
2025-11-09 12:31:38,835 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_context: graph splits = 2
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: added <|endoftext|> logit bias = -inf
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: added <|im_end|> logit bias = -inf
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: added <|fim_pad|> logit bias = -inf
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: added <|repo_name|> logit bias = -inf
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: added <|file_sep|> logit bias = -inf
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: setting dry_penalty_last_n to ctx_size = 32000
2025-11-09 12:31:38,836 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv          init: initializing slots, n_slots = 4
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot         init: id  0 | task -1 | new slot, n_ctx = 32000
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot         init: id  1 | task -1 | new slot, n_ctx = 32000
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot         init: id  2 | task -1 | new slot, n_ctx = 32000
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot         init: id  3 | task -1 | new slot, n_ctx = 32000
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv          init: prompt cache is enabled, size limit: 8192 MiB
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv          init: use `--cache-ram 0` to disable the prompt cache
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv          init: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv          init: thinking = 0
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: model loaded
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: chat template, chat_template: {%- if tools %}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>system\n' }}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if messages[0].role == 'system' %}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- messages[0].content + '\n\n' }}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- "In this environment you have access to a set of tools you can use to answer the user's question. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.\n\nTool Use Rules\nHere are the rules you should always follow to solve your task:\n1. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.\n2. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\n3. If no tool call is needed, just answer the question directly.\n4. Never re-do a tool call that you previously did with the exact same parameters.\n5. For tool use, MARK SURE use XML tag format as shown in the examples above. Do not use any other format.\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n\n" }}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- "# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- for tool in tools %}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- "\n" }}
2025-11-09 12:31:46,622 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- tool | tojson }}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endfor %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- else %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if messages[0].role == 'system' %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>system\n' + messages[0].content + '<|im_end|>\n' }}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- for message in messages[::-1] %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set index = (messages|length - 1) - loop.index0 %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if ns.multi_step_tool and message.role == "user" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set ns.multi_step_tool = false %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set ns.last_query_index = index %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endfor %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- for message in messages %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if message.content is string %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set content = message.content %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- else %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set content = '' %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if (message.role == "user") or (message.role == "system" and not loop.first) %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>' + message.role + '\n' + content + '<|im_end|>' + '\n' }}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- elif message.role == "assistant" %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set reasoning_content = '' %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if message.reasoning_content is string %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set reasoning_content = message.reasoning_content %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- else %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if '</think>' in content %}
2025-11-09 12:31:46,623 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set reasoning_content = content.split('</think>')[0].rstrip('\n').split('<think>')[-1].lstrip('\n') %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set content = content.split('</think>')[-1].lstrip('\n') %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if loop.index0 > ns.last_query_index %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if loop.last or (not loop.last and reasoning_content) %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>' + message.role + '\n<think>\n' + reasoning_content.strip('\n') + '\n</think>\n\n' + content.lstrip('\n') }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- else %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>' + message.role + '\n' + content }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- else %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>' + message.role + '\n' + content }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if message.tool_calls %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- for tool_call in message.tool_calls %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if (loop.first and content) or (not loop.first) %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '\n' }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if tool_call.function %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- set tool_call = tool_call.function %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<tool_call>\n{"name": "' }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- tool_call.name }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '", "arguments": ' }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if tool_call.arguments is string %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- tool_call.arguments }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- else %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- tool_call.arguments | tojson }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '}\n</tool_call>' }}
2025-11-09 12:31:46,624 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endfor %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_end|>\n' }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- elif message.role == "tool" %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if loop.first or (messages[loop.index0 - 1].role != "tool") %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>user' }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '\n<tool_response>\n' }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- content }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '\n</tool_response>' }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_end|>\n' }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endfor %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- if add_generation_prompt %}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {{- '<|im_start|>assistant\n' }}
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: {%- endif %}, example_format: '<|im_start|>system
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: You are a helpful assistant<|im_end|>
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: <|im_start|>user
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: Hello<|im_end|>
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: <|im_start|>assistant
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: Hi there<|im_end|>
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: <|im_start|>user
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: How are you?<|im_end|>
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: <|im_start|>assistant
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: '
2025-11-09 12:31:46,625 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: main: server is listening on http://127.0.0.1:8585 - starting the main loop
2025-11-09 12:31:46,625 - INFO - root - llama.cpp server for Dorado-WebSurf_Tool-ext.Q8_0 is ready on port 8585
2025-11-09 12:31:46,626 - INFO - llama_runner.metrics - Runner ready
2025-11-09 12:31:46,626 - INFO - llama_runner.headless_service_manager - Port 8585 ready for Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 12:31:46,626 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:31:46,736 - INFO - llama_runner.services.runner_service - Llama runner for Dorado-WebSurf_Tool-ext.Q8_0 is ready on port 8585
2025-11-09 12:31:46,736 - INFO - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is ready on port 8585 after startup.
2025-11-09 12:31:46,737 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:31:47,087 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:31:47,098 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8E840>
2025-11-09 12:31:47,099 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:31:47,099 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:31:47,099 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:31:47,100 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:31:47,100 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:31:47,214 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:31:47,217 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
2025-11-09 12:31:47,218 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:31:47,218 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:31:47,232 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:31:47,232 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:31:47,232 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:31:47,232 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 0 | processing task
2025-11-09 12:31:47,232 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23331
2025-11-09 12:31:47,233 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
2025-11-09 12:31:47,233 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 1024, batch.n_tokens = 1024, progress = 0.043890
2025-11-09 12:31:52,179 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 1024, memory_seq_rm [1024, end)
2025-11-09 12:31:52,179 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 1024, progress = 0.087780
2025-11-09 12:31:56,681 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 2048, memory_seq_rm [2048, end)
2025-11-09 12:31:56,682 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 3072, batch.n_tokens = 1024, progress = 0.131670
2025-11-09 12:32:01,941 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 3072, memory_seq_rm [3072, end)
2025-11-09 12:32:01,942 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 1024, progress = 0.175560
2025-11-09 12:32:07,769 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 4096, memory_seq_rm [4096, end)
2025-11-09 12:32:07,770 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 5120, batch.n_tokens = 1024, progress = 0.219451
2025-11-09 12:32:14,111 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 5120, memory_seq_rm [5120, end)
2025-11-09 12:32:14,112 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 1024, progress = 0.263341
2025-11-09 12:32:21,055 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 6144, memory_seq_rm [6144, end)
2025-11-09 12:32:21,055 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 7168, batch.n_tokens = 1024, progress = 0.307231
2025-11-09 12:32:28,679 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 7168, memory_seq_rm [7168, end)
2025-11-09 12:32:28,679 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 1024, progress = 0.351121
2025-11-09 12:32:36,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 8192, memory_seq_rm [8192, end)
2025-11-09 12:32:36,945 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 9216, batch.n_tokens = 1024, progress = 0.395011
2025-11-09 12:32:45,847 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 9216, memory_seq_rm [9216, end)
2025-11-09 12:32:45,847 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 1024, progress = 0.438901
2025-11-09 12:32:55,531 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 10240, memory_seq_rm [10240, end)
2025-11-09 12:32:55,531 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 11264, batch.n_tokens = 1024, progress = 0.482791
2025-11-09 12:33:05,852 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 11264, memory_seq_rm [11264, end)
2025-11-09 12:33:05,852 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 1024, progress = 0.526681
2025-11-09 12:33:16,849 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 12288, memory_seq_rm [12288, end)
2025-11-09 12:33:16,849 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 13312, batch.n_tokens = 1024, progress = 0.570571
2025-11-09 12:33:28,535 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 13312, memory_seq_rm [13312, end)
2025-11-09 12:33:28,535 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 1024, progress = 0.614461
2025-11-09 12:33:41,002 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 14336, memory_seq_rm [14336, end)
2025-11-09 12:33:41,003 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 15360, batch.n_tokens = 1024, progress = 0.658352
2025-11-09 12:33:53,825 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 15360, memory_seq_rm [15360, end)
2025-11-09 12:33:53,825 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 1024, progress = 0.702242
2025-11-09 12:34:07,372 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 16384, memory_seq_rm [16384, end)
2025-11-09 12:34:07,372 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 17408, batch.n_tokens = 1024, progress = 0.746132
2025-11-09 12:34:21,647 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 17408, memory_seq_rm [17408, end)
2025-11-09 12:34:21,647 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 1024, progress = 0.790022
2025-11-09 12:34:36,463 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 18432, memory_seq_rm [18432, end)
2025-11-09 12:34:36,463 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 19456, batch.n_tokens = 1024, progress = 0.833912
2025-11-09 12:34:51,994 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 19456, memory_seq_rm [19456, end)
2025-11-09 12:34:51,994 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 1024, progress = 0.877802
2025-11-09 12:35:08,030 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 20480, memory_seq_rm [20480, end)
2025-11-09 12:35:08,030 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 21504, batch.n_tokens = 1024, progress = 0.921692
2025-11-09 12:35:24,928 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 21504, memory_seq_rm [21504, end)
2025-11-09 12:35:24,928 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 1024, progress = 0.965582
2025-11-09 12:35:42,491 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | n_tokens = 22528, memory_seq_rm [22528, end)
2025-11-09 12:35:42,491 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 23331, batch.n_tokens = 803, progress = 1.000000
2025-11-09 12:35:42,497 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 0 | prompt done, n_tokens = 23331, batch.n_tokens = 803
2025-11-09 12:37:04,974 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 0 |
2025-11-09 12:37:04,975 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =  259047.17 ms / 23331 tokens (   11.10 ms per token,    90.06 tokens per second)
2025-11-09 12:37:04,975 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   58704.89 ms /   342 tokens (  171.65 ms per token,     5.83 tokens per second)
2025-11-09 12:37:04,975 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =  317752.07 ms / 23673 tokens
2025-11-09 12:37:04,976 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 0 | stop processing: n_tokens = 23672, truncated = 0
2025-11-09 12:37:04,976 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:37:04,977 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:37:04,977 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:37:04,977 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:37:04,978 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:37:06,367 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:37:06,368 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:37:06,368 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:37:06,368 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:37:06,369 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:37:06,369 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:37:06,370 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:37:06,370 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:37:06,370 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:37:06,371 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:37:06,371 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:37:06,372 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:37:06,372 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:37:06,372 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:37:06,372 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:37:06,617 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:37:06,618 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8E8D0>
2025-11-09 12:37:06,618 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:37:06,618 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:37:06,618 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:37:06,618 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:37:06,618 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:37:06,723 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:37:06,725 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.986
2025-11-09 12:37:06,726 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:37:06,726 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:37:06,727 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:37:06,727 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:37:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:37:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 365 | processing task
2025-11-09 12:37:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 365 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23384
2025-11-09 12:37:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 365 | n_tokens = 23330, memory_seq_rm [23330, end)
2025-11-09 12:37:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 365 | prompt processing progress, n_tokens = 23384, batch.n_tokens = 54, progress = 1.000000
2025-11-09 12:37:06,735 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 365 | prompt done, n_tokens = 23384, batch.n_tokens = 54
2025-11-09 12:37:42,220 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 365 |
2025-11-09 12:37:42,220 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1264.12 ms /    54 tokens (   23.41 ms per token,    42.72 tokens per second)
2025-11-09 12:37:42,220 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   34227.54 ms /   201 tokens (  170.29 ms per token,     5.87 tokens per second)
2025-11-09 12:37:42,220 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   35491.66 ms /   255 tokens
2025-11-09 12:37:42,221 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 365 | stop processing: n_tokens = 23584, truncated = 0
2025-11-09 12:37:42,221 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:37:42,222 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:37:42,222 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:37:42,222 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:37:42,222 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:37:43,348 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:37:43,348 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:37:43,349 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:37:43,350 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:37:43,350 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:37:43,351 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:37:43,352 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:37:43,352 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:37:43,353 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:37:43,353 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:37:43,354 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:37:43,354 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:37:43,355 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:37:43,355 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:37:43,355 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:37:43,602 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:37:43,603 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8E5D0>
2025-11-09 12:37:43,603 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:37:43,604 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:37:43,604 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:37:43,604 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:37:43,604 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:37:43,707 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:37:43,710 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.991
2025-11-09 12:37:43,710 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:37:43,711 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:37:43,711 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:37:43,711 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:37:43,714 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:37:43,714 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 567 | processing task
2025-11-09 12:37:43,714 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 567 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23447
2025-11-09 12:37:43,714 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 567 | n_tokens = 23383, memory_seq_rm [23383, end)
2025-11-09 12:37:43,714 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 567 | prompt processing progress, n_tokens = 23447, batch.n_tokens = 64, progress = 1.000000
2025-11-09 12:37:43,720 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 567 | prompt done, n_tokens = 23447, batch.n_tokens = 64
2025-11-09 12:38:34,799 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 567 |
2025-11-09 12:38:34,800 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1320.32 ms /    64 tokens (   20.63 ms per token,    48.47 tokens per second)
2025-11-09 12:38:34,800 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   49765.15 ms /   291 tokens (  171.01 ms per token,     5.85 tokens per second)
2025-11-09 12:38:34,800 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   51085.48 ms /   355 tokens
2025-11-09 12:38:34,800 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 567 | stop processing: n_tokens = 23737, truncated = 0
2025-11-09 12:38:34,801 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:38:34,801 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:38:34,801 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:38:34,801 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:38:34,801 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:38:35,942 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:38:35,943 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:38:35,944 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:38:35,945 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:38:35,945 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:38:35,946 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:38:35,946 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:38:35,947 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:38:35,948 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:38:35,948 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:38:35,949 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:38:35,950 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:38:35,950 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:38:35,950 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:38:35,950 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:38:36,200 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:38:36,201 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8E570>
2025-11-09 12:38:36,201 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:38:36,202 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:38:36,202 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:38:36,202 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:38:36,202 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:38:36,318 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:38:36,321 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.988
2025-11-09 12:38:36,321 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:38:36,322 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:38:36,322 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:38:36,322 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:38:36,325 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:38:36,325 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 859 | processing task
2025-11-09 12:38:36,325 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 859 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23506
2025-11-09 12:38:36,325 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 859 | n_tokens = 23446, memory_seq_rm [23446, end)
2025-11-09 12:38:36,325 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 859 | prompt processing progress, n_tokens = 23506, batch.n_tokens = 60, progress = 1.000000
2025-11-09 12:38:36,331 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 859 | prompt done, n_tokens = 23506, batch.n_tokens = 60
2025-11-09 12:39:11,955 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 859 |
2025-11-09 12:39:11,955 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1280.53 ms /    60 tokens (   21.34 ms per token,    46.86 tokens per second)
2025-11-09 12:39:11,955 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   34348.82 ms /   202 tokens (  170.04 ms per token,     5.88 tokens per second)
2025-11-09 12:39:11,955 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   35629.35 ms /   262 tokens
2025-11-09 12:39:11,955 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 859 | stop processing: n_tokens = 23707, truncated = 0
2025-11-09 12:39:11,955 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:39:11,956 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:39:11,956 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:39:11,956 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:39:11,957 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:39:12,982 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:39:12,982 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:39:12,983 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:39:12,983 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:39:12,983 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:39:12,984 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:39:12,984 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:39:12,985 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:39:12,985 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:39:12,985 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:39:12,986 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:39:12,986 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:39:12,987 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:39:12,987 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:39:12,987 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:39:13,228 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:39:13,229 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8FD70>
2025-11-09 12:39:13,229 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:39:13,229 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:39:13,229 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:39:13,229 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:39:13,229 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:39:13,334 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:39:13,337 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.991
2025-11-09 12:39:13,337 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:39:13,338 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:39:13,338 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:39:13,338 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:39:13,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:39:13,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 1062 | processing task
2025-11-09 12:39:13,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1062 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23542
2025-11-09 12:39:13,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1062 | n_tokens = 23505, memory_seq_rm [23505, end)
2025-11-09 12:39:13,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1062 | prompt processing progress, n_tokens = 23542, batch.n_tokens = 37, progress = 1.000000
2025-11-09 12:39:13,347 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1062 | prompt done, n_tokens = 23542, batch.n_tokens = 37
2025-11-09 12:40:12,771 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 1062 |
2025-11-09 12:40:12,771 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1202.75 ms /    37 tokens (   32.51 ms per token,    30.76 tokens per second)
2025-11-09 12:40:12,771 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   58227.23 ms /   340 tokens (  171.26 ms per token,     5.84 tokens per second)
2025-11-09 12:40:12,771 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   59429.98 ms /   377 tokens
2025-11-09 12:40:12,771 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 1062 | stop processing: n_tokens = 23881, truncated = 0
2025-11-09 12:40:12,772 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:40:12,772 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:40:12,772 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:40:12,772 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:40:12,772 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:40:14,036 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:40:14,036 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:40:14,037 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:40:14,038 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:40:14,039 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:40:14,039 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:40:14,040 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:40:14,040 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:40:14,040 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:40:14,041 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:40:14,041 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:40:14,042 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:40:14,042 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:40:14,042 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:40:14,042 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:40:14,284 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:40:14,285 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53C93D40>
2025-11-09 12:40:14,285 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:40:14,286 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:40:14,286 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:40:14,286 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:40:14,286 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:40:14,392 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:40:14,394 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.986
2025-11-09 12:40:14,395 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:40:14,395 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:40:14,395 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:40:14,395 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:40:14,398 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:40:14,398 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 1403 | processing task
2025-11-09 12:40:14,398 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1403 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23597
2025-11-09 12:40:14,398 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1403 | n_tokens = 23541, memory_seq_rm [23541, end)
2025-11-09 12:40:14,399 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1403 | prompt processing progress, n_tokens = 23597, batch.n_tokens = 56, progress = 1.000000
2025-11-09 12:40:14,404 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1403 | prompt done, n_tokens = 23597, batch.n_tokens = 56
2025-11-09 12:41:10,862 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 1403 |
2025-11-09 12:41:10,862 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1274.66 ms /    56 tokens (   22.76 ms per token,    43.93 tokens per second)
2025-11-09 12:41:10,862 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   55189.53 ms /   323 tokens (  170.87 ms per token,     5.85 tokens per second)
2025-11-09 12:41:10,862 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   56464.19 ms /   379 tokens
2025-11-09 12:41:10,863 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 1403 | stop processing: n_tokens = 23919, truncated = 0
2025-11-09 12:41:10,863 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:41:10,863 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:41:10,863 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:41:10,863 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:41:10,863 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:41:12,023 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:41:12,024 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:41:12,025 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:41:12,025 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:41:12,025 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:41:12,026 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:41:12,026 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:41:12,026 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:41:12,027 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:41:12,027 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:41:12,027 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:41:12,028 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:41:12,028 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:41:12,028 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:41:12,028 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:41:12,272 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:41:12,273 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8E5A0>
2025-11-09 12:41:12,273 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:41:12,273 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:41:12,273 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:41:12,273 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:41:12,274 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:41:12,382 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:41:12,386 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.986
2025-11-09 12:41:12,386 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:41:12,387 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:41:12,387 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:41:12,387 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:41:12,390 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:41:12,390 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 1727 | processing task
2025-11-09 12:41:12,390 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1727 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23660
2025-11-09 12:41:12,390 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1727 | n_tokens = 23596, memory_seq_rm [23596, end)
2025-11-09 12:41:12,390 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1727 | prompt processing progress, n_tokens = 23660, batch.n_tokens = 64, progress = 1.000000
2025-11-09 12:41:12,397 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 1727 | prompt done, n_tokens = 23660, batch.n_tokens = 64
2025-11-09 12:42:23,045 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 1727 |
2025-11-09 12:42:23,045 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1312.03 ms /    64 tokens (   20.50 ms per token,    48.78 tokens per second)
2025-11-09 12:42:23,045 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   69342.85 ms /   404 tokens (  171.64 ms per token,     5.83 tokens per second)
2025-11-09 12:42:23,045 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   70654.88 ms /   468 tokens
2025-11-09 12:42:23,046 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 1727 | stop processing: n_tokens = 24063, truncated = 0
2025-11-09 12:42:23,046 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:42:23,046 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:42:23,046 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:42:23,046 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:42:23,047 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:42:24,294 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:42:24,294 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:42:24,295 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:42:24,295 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:42:24,296 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:42:24,296 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:42:24,296 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:42:24,296 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:42:24,297 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:42:24,297 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:42:24,297 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:42:24,298 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:42:24,298 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:42:24,298 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:42:24,298 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:42:24,539 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:42:24,540 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE1910>
2025-11-09 12:42:24,540 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:42:24,540 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:42:24,540 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:42:24,540 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:42:24,540 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:42:24,646 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:42:24,649 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.983
2025-11-09 12:42:24,649 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:42:24,650 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:42:24,650 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:42:24,650 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:42:24,653 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:42:24,653 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 2132 | processing task
2025-11-09 12:42:24,653 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2132 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23715
2025-11-09 12:42:24,653 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2132 | n_tokens = 23659, memory_seq_rm [23659, end)
2025-11-09 12:42:24,653 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2132 | prompt processing progress, n_tokens = 23715, batch.n_tokens = 56, progress = 1.000000
2025-11-09 12:42:24,659 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2132 | prompt done, n_tokens = 23715, batch.n_tokens = 56
2025-11-09 12:44:20,832 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 2132 |
2025-11-09 12:44:20,832 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1268.28 ms /    56 tokens (   22.65 ms per token,    44.15 tokens per second)
2025-11-09 12:44:20,832 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =  114910.53 ms /   663 tokens (  173.32 ms per token,     5.77 tokens per second)
2025-11-09 12:44:20,832 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =  116178.81 ms /   719 tokens
2025-11-09 12:44:20,833 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 2132 | stop processing: n_tokens = 24377, truncated = 0
2025-11-09 12:44:20,833 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:44:20,833 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:44:20,834 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:44:20,834 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:44:20,834 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:44:22,007 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:44:22,008 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:44:22,008 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:44:22,009 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:44:22,009 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:44:22,010 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:44:22,010 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:44:22,011 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:44:22,011 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:44:22,012 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:44:22,012 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:44:22,013 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:44:22,013 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:44:22,013 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:44:22,013 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:44:22,277 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:44:22,278 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE2C90>
2025-11-09 12:44:22,278 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:44:22,279 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:44:22,279 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:44:22,279 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:44:22,279 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:44:22,392 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:44:22,395 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.973
2025-11-09 12:44:22,396 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:44:22,396 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:44:22,396 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:44:22,396 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:44:22,399 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:44:22,400 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 2796 | processing task
2025-11-09 12:44:22,400 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2796 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23780
2025-11-09 12:44:22,400 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2796 | n_tokens = 23714, memory_seq_rm [23714, end)
2025-11-09 12:44:22,400 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2796 | prompt processing progress, n_tokens = 23780, batch.n_tokens = 66, progress = 1.000000
2025-11-09 12:44:22,407 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 2796 | prompt done, n_tokens = 23780, batch.n_tokens = 66
2025-11-09 12:45:55,807 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 2796 |
2025-11-09 12:45:55,807 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1828.92 ms /    66 tokens (   27.71 ms per token,    36.09 tokens per second)
2025-11-09 12:45:55,807 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   91579.66 ms /   529 tokens (  173.12 ms per token,     5.78 tokens per second)
2025-11-09 12:45:55,807 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   93408.59 ms /   595 tokens
2025-11-09 12:45:55,808 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 2796 | stop processing: n_tokens = 24308, truncated = 0
2025-11-09 12:45:55,808 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:45:55,808 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:45:55,808 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:45:55,809 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:45:55,809 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:45:57,001 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:45:57,002 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:45:57,003 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:45:57,003 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:45:57,004 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:45:57,005 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:45:57,006 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:45:57,007 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:45:57,007 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:45:57,008 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:45:57,009 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:45:57,009 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:45:57,010 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:45:57,010 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:45:57,010 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:45:57,271 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:45:57,271 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE2D20>
2025-11-09 12:45:57,272 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:45:57,272 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:45:57,272 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:45:57,272 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:45:57,272 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:45:57,378 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:45:57,381 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.978
2025-11-09 12:45:57,383 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:45:57,383 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:45:57,383 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:45:57,383 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:45:57,386 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:45:57,386 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 3326 | processing task
2025-11-09 12:45:57,386 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3326 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23840
2025-11-09 12:45:57,386 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3326 | n_tokens = 23779, memory_seq_rm [23779, end)
2025-11-09 12:45:57,386 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3326 | prompt processing progress, n_tokens = 23840, batch.n_tokens = 61, progress = 1.000000
2025-11-09 12:45:57,392 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3326 | prompt done, n_tokens = 23840, batch.n_tokens = 61
2025-11-09 12:46:49,630 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 3326 |
2025-11-09 12:46:49,630 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1313.35 ms /    61 tokens (   21.53 ms per token,    46.45 tokens per second)
2025-11-09 12:46:49,630 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   50930.53 ms /   296 tokens (  172.06 ms per token,     5.81 tokens per second)
2025-11-09 12:46:49,630 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   52243.87 ms /   357 tokens
2025-11-09 12:46:49,631 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 3326 | stop processing: n_tokens = 24135, truncated = 0
2025-11-09 12:46:49,631 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:46:49,631 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:46:49,631 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:46:49,631 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:46:49,632 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:46:50,917 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:46:50,917 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:46:50,918 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:46:50,918 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:46:50,919 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:46:50,919 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:46:50,919 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:46:50,920 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:46:50,920 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:46:50,921 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:46:50,921 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:46:50,921 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:46:50,922 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:46:50,922 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:46:50,922 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:46:51,165 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:46:51,166 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE3CB0>
2025-11-09 12:46:51,166 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:46:51,167 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:46:51,167 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:46:51,167 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:46:51,167 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:46:51,272 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:46:51,274 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.988
2025-11-09 12:46:51,275 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:46:51,275 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:46:51,275 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:46:51,275 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:46:51,278 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:46:51,278 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 3623 | processing task
2025-11-09 12:46:51,278 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3623 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23873
2025-11-09 12:46:51,278 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3623 | n_tokens = 23839, memory_seq_rm [23839, end)
2025-11-09 12:46:51,279 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3623 | prompt processing progress, n_tokens = 23873, batch.n_tokens = 34, progress = 1.000000
2025-11-09 12:46:51,284 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 3623 | prompt done, n_tokens = 23873, batch.n_tokens = 34
2025-11-09 12:48:49,920 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 3623 |
2025-11-09 12:48:49,920 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1217.40 ms /    34 tokens (   35.81 ms per token,    27.93 tokens per second)
2025-11-09 12:48:49,920 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =  117423.68 ms /   678 tokens (  173.19 ms per token,     5.77 tokens per second)
2025-11-09 12:48:49,920 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =  118641.08 ms /   712 tokens
2025-11-09 12:48:49,920 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 3623 | stop processing: n_tokens = 24550, truncated = 0
2025-11-09 12:48:49,920 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:48:49,921 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:48:49,921 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:48:49,921 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:48:49,921 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 12:48:51,894 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 12:48:51,895 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 12:48:51,896 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 12:48:51,896 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 12:48:51,897 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 12:48:51,898 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 12:48:51,898 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 12:48:51,899 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 12:48:51,900 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 12:48:51,900 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 12:48:51,901 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 12:48:51,901 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 12:48:51,902 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 12:48:51,902 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 12:48:51,902 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 12:48:52,149 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 12:48:52,149 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE2B70>
2025-11-09 12:48:52,149 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 12:48:52,150 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 12:48:52,150 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 12:48:52,150 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 12:48:52,150 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 12:48:52,256 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 12:48:52,258 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.972
2025-11-09 12:48:52,259 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 12:48:52,259 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 12:48:52,259 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 12:48:52,259 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 12:48:52,262 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 12:48:52,262 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 4302 | processing task
2025-11-09 12:48:52,262 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4302 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 23924
2025-11-09 12:48:52,262 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4302 | n_tokens = 23872, memory_seq_rm [23872, end)
2025-11-09 12:48:52,263 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4302 | prompt processing progress, n_tokens = 23924, batch.n_tokens = 52, progress = 1.000000
2025-11-09 12:48:52,269 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4302 | prompt done, n_tokens = 23924, batch.n_tokens = 52
2025-11-09 12:49:47,634 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 4302 |
2025-11-09 12:49:47,634 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1226.25 ms /    52 tokens (   23.58 ms per token,    42.41 tokens per second)
2025-11-09 12:49:47,634 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   54145.10 ms /   316 tokens (  171.35 ms per token,     5.84 tokens per second)
2025-11-09 12:49:47,634 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   55371.35 ms /   368 tokens
2025-11-09 12:49:47,635 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 4302 | stop processing: n_tokens = 24239, truncated = 0
2025-11-09 12:49:47,635 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 12:49:47,635 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 12:49:47,635 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 12:49:47,635 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 12:49:47,635 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 13:12:40,851 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 13:12:40,852 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 13:12:40,853 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 13:12:40,854 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 13:12:40,855 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 13:12:40,857 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 13:12:40,858 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 13:12:40,859 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 13:12:40,859 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 13:12:40,861 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 13:12:40,862 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 13:12:40,863 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 13:12:40,864 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 13:12:40,865 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 13:12:40,865 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 13:12:41,132 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 13:12:41,133 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EF18B0>
2025-11-09 13:12:41,133 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 13:12:41,133 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 13:12:41,133 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 13:12:41,133 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 13:12:41,133 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 13:12:41,244 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 13:12:41,245 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.987
2025-11-09 13:12:41,246 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 13:12:41,246 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:12:41,246 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 13:12:41,246 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 13:12:41,249 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 13:12:41,249 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 4619 | processing task
2025-11-09 13:12:41,250 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4619 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24021
2025-11-09 13:12:41,250 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4619 | n_tokens = 23923, memory_seq_rm [23923, end)
2025-11-09 13:12:41,250 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4619 | prompt processing progress, n_tokens = 24021, batch.n_tokens = 98, progress = 1.000000
2025-11-09 13:12:41,256 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 4619 | prompt done, n_tokens = 24021, batch.n_tokens = 98
2025-11-09 13:14:44,071 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 4619 |
2025-11-09 13:14:44,071 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    7597.36 ms /    98 tokens (   77.52 ms per token,    12.90 tokens per second)
2025-11-09 13:14:44,071 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =  115223.70 ms /   415 tokens (  277.65 ms per token,     3.60 tokens per second)
2025-11-09 13:14:44,071 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =  122821.06 ms /   513 tokens
2025-11-09 13:14:44,072 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 4619 | stop processing: n_tokens = 24435, truncated = 0
2025-11-09 13:14:44,072 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 13:14:44,073 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 13:14:44,073 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 13:14:44,073 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 13:14:44,073 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 13:14:45,255 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 13:14:45,256 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 13:14:45,257 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 13:14:45,258 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 13:14:45,259 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 13:14:45,260 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 13:14:45,261 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 13:14:45,262 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 13:14:45,263 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 13:14:45,263 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 13:14:45,264 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 13:14:45,265 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 13:14:45,265 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 13:14:45,265 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 13:14:45,265 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 13:14:45,521 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 13:14:45,522 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EF15E0>
2025-11-09 13:14:45,522 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 13:14:45,522 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 13:14:45,522 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 13:14:45,523 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 13:14:45,523 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 13:14:45,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 13:14:45,641 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.983
2025-11-09 13:14:45,642 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 13:14:45,642 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:14:45,642 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 13:14:45,642 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 13:14:45,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 13:14:45,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 5035 | processing task
2025-11-09 13:14:45,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5035 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24113
2025-11-09 13:14:45,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5035 | n_tokens = 24020, memory_seq_rm [24020, end)
2025-11-09 13:14:45,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5035 | prompt processing progress, n_tokens = 24113, batch.n_tokens = 93, progress = 1.000000
2025-11-09 13:14:45,651 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5035 | prompt done, n_tokens = 24113, batch.n_tokens = 93
2025-11-09 13:16:31,292 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 5035 |
2025-11-09 13:16:31,292 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    3231.58 ms /    93 tokens (   34.75 ms per token,    28.78 tokens per second)
2025-11-09 13:16:31,292 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =  102415.23 ms /   383 tokens (  267.40 ms per token,     3.74 tokens per second)
2025-11-09 13:16:31,292 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =  105646.81 ms /   476 tokens
2025-11-09 13:16:31,293 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 5035 | stop processing: n_tokens = 24495, truncated = 0
2025-11-09 13:16:31,293 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 13:16:31,293 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 13:16:31,293 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 13:16:31,293 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 13:16:31,293 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 13:26:27,154 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 13:26:27,156 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 13:26:27,157 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 13:26:27,157 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 13:26:27,157 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 13:26:27,158 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 13:26:27,158 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 13:26:27,159 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 13:26:27,159 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 13:26:27,159 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 13:26:27,160 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 13:26:27,160 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 13:26:27,160 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 13:26:27,160 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 13:26:27,160 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 13:26:27,403 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 13:26:27,404 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53C235F0>
2025-11-09 13:26:27,404 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 13:26:27,404 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 13:26:27,404 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 13:26:27,405 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 13:26:27,405 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 13:26:27,516 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 13:26:27,519 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.984
2025-11-09 13:26:27,520 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 13:26:27,520 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 13:26:27,520 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 13:26:27,520 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 13:26:27,523 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 13:26:27,523 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 5419 | processing task
2025-11-09 13:26:27,523 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5419 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24210
2025-11-09 13:26:27,523 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5419 | n_tokens = 24112, memory_seq_rm [24112, end)
2025-11-09 13:26:27,523 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5419 | prompt processing progress, n_tokens = 24210, batch.n_tokens = 98, progress = 1.000000
2025-11-09 13:26:27,529 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5419 | prompt done, n_tokens = 24210, batch.n_tokens = 98
2025-11-09 13:28:22,482 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 5419 |
2025-11-09 13:28:22,482 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    6055.16 ms /    98 tokens (   61.79 ms per token,    16.18 tokens per second)
2025-11-09 13:28:22,482 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =  108902.96 ms /   390 tokens (  279.24 ms per token,     3.58 tokens per second)
2025-11-09 13:28:22,482 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =  114958.12 ms /   488 tokens
2025-11-09 13:28:22,483 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 5419 | stop processing: n_tokens = 24599, truncated = 0
2025-11-09 13:28:22,483 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 13:28:22,483 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 13:28:22,483 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 13:28:22,484 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 13:28:22,484 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:11:42,799 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:11:42,816 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:11:42,829 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:11:42,840 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:11:42,855 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:11:42,868 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:11:42,882 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:11:42,892 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:11:42,904 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:11:42,917 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:11:42,931 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:11:42,944 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:11:42,954 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:11:42,955 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:11:42,955 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:11:43,222 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:11:43,223 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE32F0>
2025-11-09 15:11:43,223 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:11:43,223 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:11:43,223 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:11:43,223 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:11:43,223 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:11:43,348 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:11:43,350 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.984
2025-11-09 15:11:43,351 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:11:43,351 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:11:43,351 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:11:43,352 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:11:43,355 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:11:43,355 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 5810 | processing task
2025-11-09 15:11:43,355 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5810 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24307
2025-11-09 15:11:43,355 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5810 | n_tokens = 24209, memory_seq_rm [24209, end)
2025-11-09 15:11:43,355 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5810 | prompt processing progress, n_tokens = 24307, batch.n_tokens = 98, progress = 1.000000
2025-11-09 15:11:43,361 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 5810 | prompt done, n_tokens = 24307, batch.n_tokens = 98
2025-11-09 15:12:52,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 5810 |
2025-11-09 15:12:52,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    3551.51 ms /    98 tokens (   36.24 ms per token,    27.59 tokens per second)
2025-11-09 15:12:52,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   65435.00 ms /   433 tokens (  151.12 ms per token,     6.62 tokens per second)
2025-11-09 15:12:52,341 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   68986.51 ms /   531 tokens
2025-11-09 15:12:52,346 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 5810 | stop processing: n_tokens = 24739, truncated = 0
2025-11-09 15:12:52,346 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:12:52,347 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:12:52,348 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:12:52,348 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:12:52,348 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:12:53,570 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:12:53,570 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:12:53,571 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:12:53,571 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:12:53,571 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:12:53,572 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:12:53,572 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:12:53,573 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:12:53,573 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:12:53,573 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:12:53,575 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:12:53,575 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:12:53,575 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:12:53,576 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:12:53,576 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:12:53,823 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:12:53,824 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EE3C50>
2025-11-09 15:12:53,824 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:12:53,824 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:12:53,824 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:12:53,824 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:12:53,824 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:12:53,941 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:12:53,943 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.982
2025-11-09 15:12:53,944 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:12:53,944 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:12:53,944 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:12:53,944 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:12:53,947 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:12:53,948 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 6244 | processing task
2025-11-09 15:12:53,948 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6244 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24387
2025-11-09 15:12:53,948 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6244 | n_tokens = 24306, memory_seq_rm [24306, end)
2025-11-09 15:12:53,948 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6244 | prompt processing progress, n_tokens = 24387, batch.n_tokens = 81, progress = 1.000000
2025-11-09 15:12:53,954 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6244 | prompt done, n_tokens = 24387, batch.n_tokens = 81
2025-11-09 15:13:47,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 6244 |
2025-11-09 15:13:47,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1723.46 ms /    81 tokens (   21.28 ms per token,    47.00 tokens per second)
2025-11-09 15:13:47,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   52121.20 ms /   349 tokens (  149.34 ms per token,     6.70 tokens per second)
2025-11-09 15:13:47,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   53844.66 ms /   430 tokens
2025-11-09 15:13:47,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 6244 | stop processing: n_tokens = 24735, truncated = 0
2025-11-09 15:13:47,793 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:13:47,794 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:13:47,794 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:13:47,794 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:13:47,794 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:13:48,970 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:13:48,971 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:13:48,972 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:13:48,972 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:13:48,973 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:13:48,974 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:13:48,975 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:13:48,975 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:13:48,976 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:13:48,977 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:13:48,977 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:13:48,978 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:13:48,978 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:13:48,978 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:13:48,978 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:13:49,225 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:13:49,226 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53EF3C80>
2025-11-09 15:13:49,226 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:13:49,226 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:13:49,226 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:13:49,227 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:13:49,227 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:13:49,340 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:13:49,342 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.986
2025-11-09 15:13:49,343 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:13:49,343 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:13:49,343 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:13:49,344 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:13:49,346 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:13:49,347 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 6594 | processing task
2025-11-09 15:13:49,347 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6594 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24464
2025-11-09 15:13:49,347 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6594 | n_tokens = 24386, memory_seq_rm [24386, end)
2025-11-09 15:13:49,347 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6594 | prompt processing progress, n_tokens = 24464, batch.n_tokens = 78, progress = 1.000000
2025-11-09 15:13:49,353 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 6594 | prompt done, n_tokens = 24464, batch.n_tokens = 78
2025-11-09 15:14:54,809 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 6594 |
2025-11-09 15:14:54,809 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1687.29 ms /    78 tokens (   21.63 ms per token,    46.23 tokens per second)
2025-11-09 15:14:54,809 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   63774.82 ms /   416 tokens (  153.30 ms per token,     6.52 tokens per second)
2025-11-09 15:14:54,809 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   65462.12 ms /   494 tokens
2025-11-09 15:14:54,810 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 6594 | stop processing: n_tokens = 24879, truncated = 0
2025-11-09 15:14:54,810 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:14:54,810 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:14:54,810 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:14:54,810 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:14:54,811 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:14:56,163 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:14:56,164 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:14:56,164 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:14:56,164 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:14:56,165 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:14:56,165 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:14:56,165 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:14:56,166 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:14:56,166 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:14:56,167 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:14:56,167 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:14:56,168 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:14:56,169 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:14:56,169 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:14:56,169 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:14:56,416 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:14:56,416 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53C22CC0>
2025-11-09 15:14:56,417 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:14:56,417 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:14:56,417 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:14:56,417 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:14:56,417 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:14:56,539 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:14:56,542 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.983
2025-11-09 15:14:56,543 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:14:56,543 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:14:56,543 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:14:56,543 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:14:56,546 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:14:56,546 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 7011 | processing task
2025-11-09 15:14:56,546 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7011 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24543
2025-11-09 15:14:56,546 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7011 | n_tokens = 24463, memory_seq_rm [24463, end)
2025-11-09 15:14:56,547 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7011 | prompt processing progress, n_tokens = 24543, batch.n_tokens = 80, progress = 1.000000
2025-11-09 15:14:56,553 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7011 | prompt done, n_tokens = 24543, batch.n_tokens = 80
2025-11-09 15:16:05,136 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 7011 |
2025-11-09 15:16:05,137 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1750.86 ms /    80 tokens (   21.89 ms per token,    45.69 tokens per second)
2025-11-09 15:16:05,137 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   66839.52 ms /   434 tokens (  154.01 ms per token,     6.49 tokens per second)
2025-11-09 15:16:05,137 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   68590.38 ms /   514 tokens
2025-11-09 15:16:05,137 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 7011 | stop processing: n_tokens = 24976, truncated = 0
2025-11-09 15:16:05,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:16:05,138 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:16:05,138 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:16:05,138 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:16:05,139 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:16:06,303 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:16:06,306 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:16:06,308 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:16:06,309 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:16:06,311 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:16:06,313 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:16:06,314 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:16:06,315 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:16:06,315 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:16:06,316 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:16:06,317 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:16:06,319 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:16:06,322 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:16:06,322 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:16:06,323 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:16:06,606 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:16:06,607 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53C200B0>
2025-11-09 15:16:06,607 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:16:06,608 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:16:06,608 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:16:06,608 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:16:06,608 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:16:06,723 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:16:06,726 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.983
2025-11-09 15:16:06,726 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:16:06,726 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:16:06,726 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:16:06,727 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:16:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:16:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 7446 | processing task
2025-11-09 15:16:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7446 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24624
2025-11-09 15:16:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7446 | n_tokens = 24542, memory_seq_rm [24542, end)
2025-11-09 15:16:06,729 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7446 | prompt processing progress, n_tokens = 24624, batch.n_tokens = 82, progress = 1.000000
2025-11-09 15:16:06,736 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7446 | prompt done, n_tokens = 24624, batch.n_tokens = 82
2025-11-09 15:17:20,420 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 7446 |
2025-11-09 15:17:20,420 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1761.78 ms /    82 tokens (   21.49 ms per token,    46.54 tokens per second)
2025-11-09 15:17:20,420 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   71928.98 ms /   467 tokens (  154.02 ms per token,     6.49 tokens per second)
2025-11-09 15:17:20,420 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   73690.76 ms /   549 tokens
2025-11-09 15:17:20,421 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 7446 | stop processing: n_tokens = 25090, truncated = 0
2025-11-09 15:17:20,421 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:17:20,421 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:17:20,422 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:17:20,422 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:17:20,422 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:17:21,880 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:17:21,882 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:17:21,883 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:17:21,884 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:17:21,885 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:17:21,885 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:17:21,886 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:17:21,887 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:17:21,887 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:17:21,888 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:17:21,888 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:17:21,889 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:17:21,890 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:17:21,890 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:17:21,890 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:17:22,149 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:17:22,150 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53C23740>
2025-11-09 15:17:22,150 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:17:22,151 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:17:22,151 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:17:22,151 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:17:22,151 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:17:22,264 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:17:22,266 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.981
2025-11-09 15:17:22,267 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:17:22,267 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:17:22,268 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:17:22,268 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:17:22,270 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:17:22,271 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 7914 | processing task
2025-11-09 15:17:22,271 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7914 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24684
2025-11-09 15:17:22,271 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7914 | n_tokens = 24623, memory_seq_rm [24623, end)
2025-11-09 15:17:22,271 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7914 | prompt processing progress, n_tokens = 24684, batch.n_tokens = 61, progress = 1.000000
2025-11-09 15:17:22,277 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 7914 | prompt done, n_tokens = 24684, batch.n_tokens = 61
2025-11-09 15:18:33,287 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 7914 |
2025-11-09 15:18:33,287 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1193.21 ms /    61 tokens (   19.56 ms per token,    51.12 tokens per second)
2025-11-09 15:18:33,287 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   69823.04 ms /   452 tokens (  154.48 ms per token,     6.47 tokens per second)
2025-11-09 15:18:33,287 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   71016.25 ms /   513 tokens
2025-11-09 15:18:33,288 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 7914 | stop processing: n_tokens = 25135, truncated = 0
2025-11-09 15:18:33,288 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:18:33,288 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:18:33,288 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:18:33,288 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:18:33,288 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:18:34,501 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:18:34,501 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:18:34,502 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:18:34,503 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:18:34,504 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:18:34,505 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:18:34,505 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:18:34,506 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:18:34,507 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:18:34,508 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:18:34,509 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:18:34,510 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:18:34,511 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:18:34,511 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:18:34,511 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:18:34,777 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:18:34,778 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53C23B60>
2025-11-09 15:18:34,779 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:18:34,779 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:18:34,779 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:18:34,779 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:18:34,779 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:18:34,896 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:18:34,899 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.982
2025-11-09 15:18:34,899 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:18:34,899 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:18:34,900 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:18:34,900 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:18:34,902 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:18:34,902 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 8367 | processing task
2025-11-09 15:18:34,902 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8367 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24717
2025-11-09 15:18:34,902 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8367 | n_tokens = 24683, memory_seq_rm [24683, end)
2025-11-09 15:18:34,903 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8367 | prompt processing progress, n_tokens = 24717, batch.n_tokens = 34, progress = 1.000000
2025-11-09 15:18:34,909 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8367 | prompt done, n_tokens = 24717, batch.n_tokens = 34
2025-11-09 15:19:37,413 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 8367 |
2025-11-09 15:19:37,414 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1154.71 ms /    34 tokens (   33.96 ms per token,    29.44 tokens per second)
2025-11-09 15:19:37,414 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   61355.95 ms /   394 tokens (  155.73 ms per token,     6.42 tokens per second)
2025-11-09 15:19:37,414 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   62510.66 ms /   428 tokens
2025-11-09 15:19:37,415 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 8367 | stop processing: n_tokens = 25110, truncated = 0
2025-11-09 15:19:37,415 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:19:37,415 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:19:37,415 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:19:37,415 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:19:37,416 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:19:38,696 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:19:38,697 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:19:38,698 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:19:38,698 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:19:38,699 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:19:38,700 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:19:38,700 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:19:38,701 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:19:38,701 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:19:38,702 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:19:38,702 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:19:38,703 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:19:38,703 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:19:38,703 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:19:38,703 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:19:38,957 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:19:38,957 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53B8AF00>
2025-11-09 15:19:38,958 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:19:38,958 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:19:38,958 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:19:38,958 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:19:38,958 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:19:39,072 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:19:39,075 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.984
2025-11-09 15:19:39,075 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:19:39,076 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:19:39,076 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:19:39,076 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:19:39,080 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:19:39,080 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 8762 | processing task
2025-11-09 15:19:39,080 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8762 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24768
2025-11-09 15:19:39,080 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8762 | n_tokens = 24716, memory_seq_rm [24716, end)
2025-11-09 15:19:39,080 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8762 | prompt processing progress, n_tokens = 24768, batch.n_tokens = 52, progress = 1.000000
2025-11-09 15:19:39,086 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 8762 | prompt done, n_tokens = 24768, batch.n_tokens = 52
2025-11-09 15:20:50,976 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 8762 |
2025-11-09 15:20:50,976 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1177.71 ms /    52 tokens (   22.65 ms per token,    44.15 tokens per second)
2025-11-09 15:20:50,976 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   70719.13 ms /   457 tokens (  154.75 ms per token,     6.46 tokens per second)
2025-11-09 15:20:50,976 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   71896.84 ms /   509 tokens
2025-11-09 15:20:50,977 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 8762 | stop processing: n_tokens = 25224, truncated = 0
2025-11-09 15:20:50,977 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:20:50,977 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:20:50,977 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:20:50,978 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:20:50,978 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:20:52,197 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:20:52,198 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:20:52,199 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:20:52,199 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:20:52,200 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:20:52,200 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:20:52,201 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:20:52,201 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:20:52,202 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:20:52,202 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:20:52,203 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:20:52,204 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:20:52,204 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:20:52,204 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:20:52,205 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:20:52,454 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:20:52,455 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53B8B770>
2025-11-09 15:20:52,455 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:20:52,455 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:20:52,455 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:20:52,456 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:20:52,456 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:20:52,569 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:20:52,571 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.982
2025-11-09 15:20:52,572 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:20:52,572 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:20:52,572 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:20:52,572 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:20:52,575 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:20:52,575 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 9220 | processing task
2025-11-09 15:20:52,575 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9220 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24819
2025-11-09 15:20:52,575 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9220 | n_tokens = 24767, memory_seq_rm [24767, end)
2025-11-09 15:20:52,575 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9220 | prompt processing progress, n_tokens = 24819, batch.n_tokens = 52, progress = 1.000000
2025-11-09 15:20:52,582 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9220 | prompt done, n_tokens = 24819, batch.n_tokens = 52
2025-11-09 15:21:41,789 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 9220 |
2025-11-09 15:21:41,789 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1141.73 ms /    52 tokens (   21.96 ms per token,    45.54 tokens per second)
2025-11-09 15:21:41,789 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   48071.44 ms /   317 tokens (  151.64 ms per token,     6.59 tokens per second)
2025-11-09 15:21:41,789 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   49213.18 ms /   369 tokens
2025-11-09 15:21:41,789 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 9220 | stop processing: n_tokens = 25135, truncated = 0
2025-11-09 15:21:41,789 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:21:41,790 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:21:41,790 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:21:41,790 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:21:41,790 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:21:43,205 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:21:43,206 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:21:43,207 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:21:43,208 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:21:43,208 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:21:43,209 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:21:43,210 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:21:43,211 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:21:43,212 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:21:43,212 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:21:43,213 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:21:43,214 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:21:43,215 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:21:43,215 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:21:43,216 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:21:43,510 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:21:43,511 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53E8E4E0>
2025-11-09 15:21:43,511 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:21:43,511 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:21:43,511 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:21:43,512 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:21:43,512 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:21:43,638 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:21:43,641 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.987
2025-11-09 15:21:43,641 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:21:43,641 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:21:43,642 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:21:43,642 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:21:43,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:21:43,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 9538 | processing task
2025-11-09 15:21:43,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9538 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24863
2025-11-09 15:21:43,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9538 | n_tokens = 24818, memory_seq_rm [24818, end)
2025-11-09 15:21:43,645 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9538 | prompt processing progress, n_tokens = 24863, batch.n_tokens = 45, progress = 1.000000
2025-11-09 15:21:43,652 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9538 | prompt done, n_tokens = 24863, batch.n_tokens = 45
2025-11-09 15:22:34,247 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 9538 |
2025-11-09 15:22:34,247 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1187.98 ms /    45 tokens (   26.40 ms per token,    37.88 tokens per second)
2025-11-09 15:22:34,247 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   49414.10 ms /   325 tokens (  152.04 ms per token,     6.58 tokens per second)
2025-11-09 15:22:34,248 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   50602.08 ms /   370 tokens
2025-11-09 15:22:34,249 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 9538 | stop processing: n_tokens = 25187, truncated = 0
2025-11-09 15:22:34,249 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:22:34,250 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:22:34,250 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:22:34,250 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:22:34,250 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:22:35,430 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:22:35,430 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:22:35,431 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:22:35,431 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:22:35,431 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:22:35,432 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:22:35,432 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:22:35,433 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:22:35,433 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:22:35,433 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:22:35,434 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:22:35,434 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:22:35,434 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:22:35,435 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:22:35,435 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:22:35,683 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:22:35,684 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53F081A0>
2025-11-09 15:22:35,684 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:22:35,685 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:22:35,685 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:22:35,685 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:22:35,685 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:22:35,798 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:22:35,800 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.987
2025-11-09 15:22:35,801 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:22:35,801 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:22:35,802 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:22:35,802 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:22:35,805 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:22:35,805 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 9864 | processing task
2025-11-09 15:22:35,805 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9864 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24898
2025-11-09 15:22:35,805 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9864 | n_tokens = 24862, memory_seq_rm [24862, end)
2025-11-09 15:22:35,805 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9864 | prompt processing progress, n_tokens = 24898, batch.n_tokens = 36, progress = 1.000000
2025-11-09 15:22:35,811 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 9864 | prompt done, n_tokens = 24898, batch.n_tokens = 36
2025-11-09 15:23:37,600 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 9864 |
2025-11-09 15:23:37,600 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1154.30 ms /    36 tokens (   32.06 ms per token,    31.19 tokens per second)
2025-11-09 15:23:37,600 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   60640.93 ms /   399 tokens (  151.98 ms per token,     6.58 tokens per second)
2025-11-09 15:23:37,600 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   61795.23 ms /   435 tokens
2025-11-09 15:23:37,602 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 9864 | stop processing: n_tokens = 25296, truncated = 0
2025-11-09 15:23:37,602 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:23:37,602 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:23:37,602 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:23:37,602 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:23:37,603 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 15:23:38,845 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 15:23:38,846 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 15:23:38,847 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 15:23:38,847 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 15:23:38,848 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 15:23:38,848 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 15:23:38,848 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 15:23:38,849 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 15:23:38,849 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 15:23:38,850 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 15:23:38,850 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 15:23:38,850 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 15:23:38,851 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 15:23:38,851 - DEBUG - root - Runner for Dorado-WebSurf_Tool-ext.Q8_0 is already running on port 8585.
2025-11-09 15:23:38,851 - DEBUG - root - Streaming: Target URL: http://127.0.0.1:8585/v1/chat/completions
2025-11-09 15:23:39,095 - DEBUG - httpcore.connection - connect_tcp.started host='127.0.0.1' port=8585 local_address=None timeout=600.0 socket_options=None
2025-11-09 15:23:39,096 - DEBUG - httpcore.connection - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001EA53F086E0>
2025-11-09 15:23:39,096 - DEBUG - httpcore.http11 - send_request_headers.started request=<Request [b'POST']>
2025-11-09 15:23:39,096 - DEBUG - httpcore.http11 - send_request_headers.complete
2025-11-09 15:23:39,096 - DEBUG - httpcore.http11 - send_request_body.started request=<Request [b'POST']>
2025-11-09 15:23:39,096 - DEBUG - httpcore.http11 - send_request_body.complete
2025-11-09 15:23:39,096 - DEBUG - httpcore.http11 - receive_response_headers.started request=<Request [b'POST']>
2025-11-09 15:23:39,208 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  params_from_: Chat format: Hermes 2 Pro
2025-11-09 15:23:39,211 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.999 (> 0.100 thold), f_keep = 0.984
2025-11-09 15:23:39,211 - DEBUG - httpcore.http11 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Transfer-Encoding', b'chunked'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Type', b'text/event-stream'), (b'Connection', b'close')])
2025-11-09 15:23:39,212 - INFO - httpx - HTTP Request: POST http://127.0.0.1:8585/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-09 15:23:39,212 - DEBUG - root - Streaming response from http://127.0.0.1:8585/v1/chat/completions to client (SSE -> SSE)
2025-11-09 15:23:39,212 - DEBUG - httpcore.http11 - receive_response_body.started request=<Request [b'POST']>
2025-11-09 15:23:39,215 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task -1 | sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist
2025-11-09 15:23:39,215 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot launch_slot_: id  3 | task 10264 | processing task
2025-11-09 15:23:39,215 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 10264 | new prompt, n_ctx_slot = 32000, n_keep = 0, task.n_tokens = 24933
2025-11-09 15:23:39,215 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 10264 | n_tokens = 24897, memory_seq_rm [24897, end)
2025-11-09 15:23:39,215 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 10264 | prompt processing progress, n_tokens = 24933, batch.n_tokens = 36, progress = 1.000000
2025-11-09 15:23:39,221 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot update_slots: id  3 | task 10264 | prompt done, n_tokens = 24933, batch.n_tokens = 36
2025-11-09 15:24:30,440 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot print_timing: id  3 | task 10264 |
2025-11-09 15:24:30,440 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: prompt eval time =    1133.69 ms /    36 tokens (   31.49 ms per token,    31.75 tokens per second)
2025-11-09 15:24:30,440 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: eval time =   50090.45 ms /   330 tokens (  151.79 ms per token,     6.59 tokens per second)
2025-11-09 15:24:30,440 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: total time =   51224.15 ms /   366 tokens
2025-11-09 15:24:30,442 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: slot      release: id  3 | task 10264 | stop processing: n_tokens = 25262, truncated = 0
2025-11-09 15:24:30,442 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  update_slots: all slots are idle
2025-11-09 15:24:30,442 - DEBUG - httpcore.http11 - receive_response_body.complete
2025-11-09 15:24:30,442 - DEBUG - httpcore.http11 - response_closed.started
2025-11-09 15:24:30,442 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
2025-11-09 15:24:30,442 - DEBUG - httpcore.http11 - response_closed.complete
2025-11-09 16:46:06,965 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:46:06,978 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:46:06,990 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:46:06,998 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:46:07,010 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:46:07,021 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:46:07,032 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:46:07,042 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:46:07,052 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:46:07,062 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:46:07,073 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:46:07,085 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:46:07,094 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:46:07,653 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:46:07,654 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:46:07,654 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:46:07,655 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:46:07,655 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:46:07,656 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:46:07,656 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:46:07,656 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:46:07,657 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:46:07,657 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:46:07,658 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:46:07,658 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:46:07,658 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:47:54,723 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:47:54,723 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:47:54,724 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:47:54,724 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:47:54,725 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:47:54,725 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:47:54,726 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:47:54,727 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:47:54,727 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:47:54,728 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:47:54,728 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:47:54,729 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:47:54,729 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:47:54,731 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:47:54,731 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:47:54,732 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:47:54,732 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:47:54,732 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:47:54,733 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:47:54,733 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:47:54,734 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:47:54,734 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:47:54,735 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:47:54,735 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:47:54,736 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:47:54,736 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:49:20,611 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:49:20,612 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:49:20,613 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:49:20,613 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:49:20,615 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:49:20,615 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:49:20,616 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:49:20,616 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:49:20,616 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:49:20,617 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:49:20,617 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:49:20,618 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:49:20,618 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:51:37,491 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:51:37,491 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:51:37,492 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:51:37,492 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:51:37,493 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:51:37,493 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:51:37,493 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:51:37,494 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:51:37,494 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:51:37,495 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:51:37,495 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:51:37,496 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:51:37,496 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:51:37,746 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:51:37,746 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:51:37,747 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:51:37,747 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:51:37,747 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:51:37,748 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:51:37,748 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:51:37,748 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:51:37,749 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:51:37,749 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:51:37,750 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:51:37,750 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:51:37,750 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:52:06,736 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:52:06,736 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:52:06,737 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:52:06,737 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:52:06,738 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:52:06,738 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:52:06,739 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:52:06,739 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:52:06,739 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:52:06,740 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:52:06,740 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:52:06,740 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:52:06,741 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:52:06,947 - INFO - root - Loaded metadata from cache for Chroma1-HD-Flash-Q4_K_S (size: 5032612000)
2025-11-09 16:52:06,947 - INFO - root - Loaded metadata from cache for DeepSeek-R1-0528-Qwen3-8B-Q4_K_M (size: 5027785216)
2025-11-09 16:52:06,948 - INFO - root - Loaded metadata from cache for Dorado-WebSurf_Tool-ext.Q8_0 (size: 4280405760)
2025-11-09 16:52:06,948 - INFO - root - Loaded metadata from cache for gpt-oss-20b-MXFP4 (size: 12109565632)
2025-11-09 16:52:06,948 - INFO - root - Loaded metadata from cache for Qwen2.5-7B-Instruct-Q4_K_M (size: 4683073952)
2025-11-09 16:52:06,949 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-1.5B-Q8_0 (size: 1646573056)
2025-11-09 16:52:06,949 - INFO - root - Loaded metadata from cache for Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M (size: 5444832064)
2025-11-09 16:52:06,950 - INFO - root - Loaded metadata from cache for Qwen2.5-Omni-3B (size: 3616087360)
2025-11-09 16:52:06,950 - INFO - root - Loaded metadata from cache for Qwen2.5-VL-7B-Instruct (size: 4457767808)
2025-11-09 16:52:06,950 - INFO - root - Loaded metadata from cache for Qwen3-1.7B-Q8_0 (size: 1834426016)
2025-11-09 16:52:06,951 - INFO - root - Loaded metadata from cache for Qwen3-4B-Thinking-2507-Q4_K_S (size: 2383309952)
2025-11-09 16:52:06,951 - INFO - root - Loaded metadata from cache for Qwen3-Coder-30B-A3B-Instruct-Q4_K_S (size: 17456012448)
2025-11-09 16:52:06,951 - INFO - root - Loaded metadata from cache for Qwen3-VL-8B-Thinking.Q4_K (size: 4608376544)
2025-11-09 16:59:13,550 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: srv    operator(): operator(): cleaning up before exit...
2025-11-09 16:59:13,562 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute       unaccounted |
2025-11-09 16:59:13,562 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_memory_breakdown_print: |   - CUDA0 (GTX 1070)   |  8191 =    0 + (8878 =  4076 +    4500 +     301) + 17592186043729 |
2025-11-09 16:59:13,562 - DEBUG - root - llama.cpp[Dorado-WebSurf_Tool-ext.Q8_0]: llama_memory_breakdown_print: |   - Host               |                  461 =   394 +       0 +      67                   |
2025-11-09 16:59:13,994 - DEBUG - root - Log reader for Dorado-WebSurf_Tool-ext.Q8_0 cancelled.
2025-11-09 16:59:13,996 - INFO - llama_runner.metrics - Runner stopped
2025-11-09 16:59:13,997 - INFO - llama_runner.headless_service_manager - Runner Manager Event: Stopped Dorado-WebSurf_Tool-ext.Q8_0
2025-11-09 16:59:14,000 - INFO - root - Application exited with code 0.
2025-11-09 16:59:14,236 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 22:41:57,707 - INFO - root - App file logging to: logs\app.log
2025-11-09 22:41:57,708 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 22:41:57,710 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 22:41:57,711 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 22:41:57,711 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 22:41:57,712 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 22:41:57,712 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 22:41:57,712 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 22:41:57,712 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 22:41:57,713 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 22:41:57,714 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 22:41:57,714 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 22:41:57,714 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 22:41:57,715 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-09 22:41:57,715 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 22:41:57,715 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 22:41:57,715 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 22:41:57,715 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 22:41:57,715 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-09 22:41:57,716 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-09 22:41:57,716 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 22:41:57,716 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 22:41:57,716 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 22:41:57,716 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 22:45:03,260 - INFO - root - Application exited with code 0.
2025-11-09 22:45:03,286 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 22:47:18,382 - INFO - root - App file logging to: logs\app.log
2025-11-09 22:47:18,951 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 22:47:18,952 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 22:47:19,322 - INFO - root - LM Studio Proxy listening on http://127.0.0.1:1234
2025-11-09 22:47:19,374 - INFO - root - Ollama Proxy listening on http://127.0.0.1:11434
2025-11-09 22:54:59,080 - INFO - root - Ollama Proxy server shut down.
2025-11-09 22:54:59,299 - INFO - root - LM Studio Proxy server shut down.
2025-11-09 22:54:59,299 - INFO - root - Application exited with code 0.
2025-11-09 22:54:59,364 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 23:14:18,454 - INFO - root - App file logging to: logs\app.log
2025-11-09 23:14:18,456 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 23:14:18,457 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 23:14:18,459 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 23:14:18,460 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 23:14:18,460 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 23:14:18,460 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 23:14:18,461 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 23:14:18,461 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 23:14:18,462 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 23:14:18,462 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 23:14:18,463 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 23:14:18,463 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 23:14:18,463 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-09 23:14:18,464 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 23:14:18,464 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 23:14:18,464 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 23:14:18,464 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 23:14:18,465 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-09 23:14:18,465 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-09 23:14:18,465 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 23:14:18,466 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 23:14:18,466 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 23:14:18,466 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 23:16:14,285 - ERROR - root - File not found for size check: models/JanusCoderV-7B.i1-Q4_K_S.gguf
2025-11-09 23:16:14,285 - WARNING - root - Non-blocking error: Could not get size for models/JanusCoderV-7B.i1-Q4_K_S.gguf. Cannot use cache.
2025-11-09 23:16:22,845 - INFO - root - Application exited with code 0.
2025-11-09 23:16:22,871 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 23:27:32,165 - INFO - root - App file logging to: logs\app.log
2025-11-09 23:27:32,167 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 23:27:32,168 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 23:27:32,170 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 23:27:32,170 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 23:27:32,171 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 23:27:32,171 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 23:27:32,171 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 23:27:32,171 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 23:27:32,172 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 23:27:32,172 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 23:27:32,173 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 23:27:32,174 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 23:27:32,174 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-09 23:27:32,174 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 23:27:32,175 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 23:27:32,175 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 23:27:32,175 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 23:27:32,176 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-09 23:27:32,176 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-09 23:27:32,176 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 23:27:32,177 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 23:27:32,177 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 23:27:32,177 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-09 23:28:09,831 - INFO - root - Application exited with code 0.
2025-11-09 23:28:09,862 - DEBUG - qasync._windows._IocpProactor - Closing
2025-11-09 23:37:42,197 - INFO - root - App file logging to: logs\app.log
2025-11-09 23:37:42,198 - DEBUG - asyncio - Using proactor: _IocpProactor
2025-11-09 23:37:42,200 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-09 23:37:42,201 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-09 23:37:42,202 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-09 23:37:42,202 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-09 23:37:42,202 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-09 23:37:42,202 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-09 23:37:42,203 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-09 23:37:42,203 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-09 23:37:42,204 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-09 23:37:42,204 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-09 23:37:42,205 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-09 23:37:42,205 - ERROR - llama_runner.headless_service_manager - Error starting Llama Runner WebUI service: name 'HTMLResponse' is not defined
2025-11-09 23:37:42,205 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-09 23:37:42,205 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-09 23:37:42,206 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-09 23:37:42,206 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-09 23:37:42,206 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-09 23:37:42,206 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-09 23:37:42,206 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-09 23:37:42,206 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-09 23:37:42,207 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-09 23:37:42,207 - INFO - llama_runner.headless_service_manager - ============================================================

