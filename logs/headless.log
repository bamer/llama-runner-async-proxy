2025-11-11 14:49:50,531 - INFO - root - Logging configured for headless mode
2025-11-11 14:49:50,531 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-11 14:49:50,532 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 14:49:50,532 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 14:49:50,532 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 14:49:50,532 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 14:49:50,533 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 14:49:50,533 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 14:49:50,533 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 14:49:50,533 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-11 14:49:50,533 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-11 14:49:50,534 - INFO - root - Configuration loaded and validated successfully
2025-11-11 14:49:50,534 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-11 14:49:50,534 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-11 14:49:50,534 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 14:49:50,534 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 14:49:50,534 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 14:49:50,534 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 14:49:50,535 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 14:49:50,535 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 14:49:50,535 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 14:49:50,535 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-11 14:49:50,536 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-11 14:49:50,536 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-11 14:49:50,537 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-11 14:49:50,538 - INFO - root - 
============================================================
2025-11-11 14:49:50,538 - INFO - root - SERVICES ACCESSIBLES :
2025-11-11 14:49:50,538 - INFO - root - ============================================================
2025-11-11 14:49:50,538 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-11 14:49:50,538 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-11 14:49:50,538 - INFO - root - Dashboard Web: http://127.0.0.1:8035/
2025-11-11 14:49:50,538 - INFO - root - Metrics Server: http://127.0.0.1:8080/
2025-11-11 14:49:50,538 - INFO - root - ============================================================

2025-11-11 14:49:50,539 - INFO - root - Starting all services...
2025-11-11 14:49:50,539 - CRITICAL - root - Critical error in headless service: no running event loop
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\main.py", line 193, in main
    asyncio.create_task(hsm.start_services())
  File "C:\Users\theba\.pyenv\pyenv-win\versions\3.12.4\Lib\asyncio\tasks.py", line 417, in create_task
    loop = events.get_running_loop()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: no running event loop
2025-11-11 14:49:50,540 - INFO - root - Headless service shutting down
2025-11-11 14:57:10,349 - INFO - root - Logging configured for headless mode
2025-11-11 14:57:10,349 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-11 14:57:10,349 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 14:57:10,349 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 14:57:10,350 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 14:57:10,350 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 14:57:10,350 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 14:57:10,350 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 14:57:10,350 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 14:57:10,351 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-11 14:57:10,351 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-11 14:57:10,351 - INFO - root - Configuration loaded and validated successfully
2025-11-11 14:57:10,351 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-11 14:57:10,351 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-11 14:57:10,351 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 14:57:10,351 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 14:57:10,352 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 14:57:10,352 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 14:57:10,352 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 14:57:10,352 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 14:57:10,352 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 14:57:10,353 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-11 14:57:10,353 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-11 14:57:10,353 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-11 14:57:10,353 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-11 14:57:10,354 - INFO - root - 
============================================================
2025-11-11 14:57:10,354 - INFO - root - SERVICES ACCESSIBLES :
2025-11-11 14:57:10,354 - INFO - root - ============================================================
2025-11-11 14:57:10,355 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-11 14:57:10,355 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-11 14:57:10,355 - INFO - root - Dashboard Web: http://127.0.0.1:8035/
2025-11-11 14:57:10,355 - INFO - root - Metrics Server: http://127.0.0.1:8080/
2025-11-11 14:57:10,355 - INFO - root - ============================================================

2025-11-11 14:57:10,355 - INFO - root - Starting all services...
2025-11-11 14:57:10,355 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-11 14:57:10,356 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-11 14:57:10,356 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-11 14:57:10,356 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-11 14:57:10,357 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-11 14:57:10,357 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-11 14:57:10,357 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-11 14:57:10,358 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-11 14:57:10,358 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-11 14:57:10,358 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-11 14:57:10,358 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-11 14:57:10,359 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-11 14:57:10,359 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-11 14:57:10,359 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-11 14:57:10,359 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-11 14:57:10,360 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-11 14:57:10,360 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-11 14:57:10,360 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-11 14:57:10,360 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-11 17:56:21,734 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:21,735 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:21,735 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:21,735 - INFO - llama_runner.headless_service_manager - Stopping Ollama proxy...
2025-11-11 17:56:21,735 - INFO - llama_runner.headless_service_manager - Stopping LM Studio proxy...
2025-11-11 17:56:21,735 - INFO - llama_runner.headless_service_manager - Stopping Llama Runner WebUI...
2025-11-11 17:56:21,844 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:21,844 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:21,844 - INFO - root - All services stopped gracefully
2025-11-11 17:56:40,987 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:40,987 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:40,987 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:40,987 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:40,987 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:40,988 - INFO - root - All services stopped gracefully
2025-11-11 17:56:42,635 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:42,635 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:42,635 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:42,635 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:42,636 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:42,636 - INFO - root - All services stopped gracefully
2025-11-11 17:56:43,027 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:43,028 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:43,028 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:43,028 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:43,028 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:43,028 - INFO - root - All services stopped gracefully
2025-11-11 17:56:43,235 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:43,235 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:43,235 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:43,235 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:43,235 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:43,236 - INFO - root - All services stopped gracefully
2025-11-11 17:56:43,427 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:43,427 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:43,427 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:43,428 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:43,428 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:43,428 - INFO - root - All services stopped gracefully
2025-11-11 17:56:43,658 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:43,659 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:43,659 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:43,659 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:43,659 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:43,660 - INFO - root - All services stopped gracefully
2025-11-11 17:56:43,939 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:43,939 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:43,939 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:43,940 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:43,940 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:43,940 - INFO - root - All services stopped gracefully
2025-11-11 17:56:44,530 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:44,531 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:44,531 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:44,531 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:44,531 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:44,531 - INFO - root - All services stopped gracefully
2025-11-11 17:56:44,731 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 17:56:44,731 - INFO - root - Shutdown requested, stopping services...
2025-11-11 17:56:44,731 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 17:56:44,731 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 17:56:44,732 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 17:56:44,732 - INFO - root - All services stopped gracefully
2025-11-11 18:05:52,479 - INFO - root - Logging configured for headless mode
2025-11-11 18:05:52,479 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-11 18:05:52,479 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 18:05:52,479 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 18:05:52,480 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 18:05:52,480 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 18:05:52,480 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 18:05:52,480 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 18:05:52,481 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 18:05:52,481 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-11 18:05:52,481 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-11 18:05:52,481 - INFO - root - Configuration loaded and validated successfully
2025-11-11 18:05:52,481 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-11 18:05:52,481 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-11 18:05:52,482 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 18:05:52,482 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 18:05:52,482 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 18:05:52,482 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 18:05:52,482 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 18:05:52,482 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 18:05:52,482 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 18:05:52,483 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-11 18:05:52,483 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-11 18:05:52,483 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-11 18:05:52,484 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-11 18:05:52,485 - INFO - root - 
============================================================
2025-11-11 18:05:52,485 - INFO - root - SERVICES ACCESSIBLES :
2025-11-11 18:05:52,485 - INFO - root - ============================================================
2025-11-11 18:05:52,485 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-11 18:05:52,486 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-11 18:05:52,486 - INFO - root - Dashboard Web: http://127.0.0.1:8035/
2025-11-11 18:05:52,486 - INFO - root - Metrics Server: http://127.0.0.1:8080/
2025-11-11 18:05:52,486 - INFO - root - ============================================================

2025-11-11 18:05:52,486 - INFO - root - Starting all services...
2025-11-11 18:05:52,486 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-11 18:05:52,486 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-11 18:05:52,486 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-11 18:05:52,486 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-11 18:05:52,487 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-11 18:05:52,487 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-11 18:05:52,487 - INFO - llama_runner.headless_service_manager - Starting Llama Runner WebUI service on port 8081...
2025-11-11 18:05:52,488 - INFO - llama_runner.headless_service_manager - ‚úÖ Llama Runner WebUI service started on http://0.0.0.0:8081/
2025-11-11 18:05:52,488 - INFO - llama_runner.headless_service_manager -    üåê Access via: http://localhost:8081/
2025-11-11 18:05:52,488 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-11 18:05:52,488 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-11 18:05:52,488 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-11 18:05:52,488 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-11 18:05:52,489 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-11 18:05:52,489 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-11 18:05:52,489 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-11 18:05:52,489 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-11 18:05:52,489 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-11 18:05:52,489 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-11 18:12:58,067 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:12:58,068 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:12:58,068 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:12:58,068 - INFO - llama_runner.headless_service_manager - Stopping Ollama proxy...
2025-11-11 18:12:58,068 - INFO - llama_runner.headless_service_manager - Stopping LM Studio proxy...
2025-11-11 18:12:58,068 - INFO - llama_runner.headless_service_manager - Stopping Llama Runner WebUI...
2025-11-11 18:12:58,176 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:12:58,176 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:12:58,176 - INFO - root - All services stopped gracefully
2025-11-11 18:13:03,050 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:03,050 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:03,050 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:03,051 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:03,051 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:03,051 - INFO - root - All services stopped gracefully
2025-11-11 18:13:03,746 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:03,747 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:03,747 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:03,747 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:03,747 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:03,747 - INFO - root - All services stopped gracefully
2025-11-11 18:13:04,042 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:04,042 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:04,042 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:04,042 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:04,042 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:04,042 - INFO - root - All services stopped gracefully
2025-11-11 18:13:04,234 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:04,234 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:04,234 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:04,234 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:04,234 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:04,235 - INFO - root - All services stopped gracefully
2025-11-11 18:13:04,410 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:04,410 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:04,410 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:04,410 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:04,410 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:04,410 - INFO - root - All services stopped gracefully
2025-11-11 18:13:04,601 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:04,602 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:04,602 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:04,602 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:04,602 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:04,602 - INFO - root - All services stopped gracefully
2025-11-11 18:13:04,803 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:04,803 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:04,803 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:04,803 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:04,803 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:04,804 - INFO - root - All services stopped gracefully
2025-11-11 18:13:04,994 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:04,995 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:04,995 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:04,995 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:04,995 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:04,996 - INFO - root - All services stopped gracefully
2025-11-11 18:13:13,754 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:13,754 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:13,754 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:13,754 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:13,754 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:13,755 - INFO - root - All services stopped gracefully
2025-11-11 18:13:14,129 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:14,130 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:14,130 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:14,130 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:14,130 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:14,130 - INFO - root - All services stopped gracefully
2025-11-11 18:13:14,386 - INFO - root - Received signal 2, initiating shutdown
2025-11-11 18:13:14,386 - INFO - root - Shutdown requested, stopping services...
2025-11-11 18:13:14,386 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-11 18:13:14,387 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-11 18:13:14,387 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-11 18:13:14,387 - INFO - root - All services stopped gracefully
2025-11-11 23:41:26,309 - INFO - root - Logging configured for headless mode
2025-11-11 23:41:26,309 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-11 23:41:26,310 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 23:41:26,310 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 23:41:26,310 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 23:41:26,310 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 23:41:26,311 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 23:41:26,311 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 23:41:26,311 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 23:41:26,311 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-11 23:41:26,312 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-11 23:41:26,312 - INFO - root - Configuration loaded and validated successfully
2025-11-11 23:41:26,312 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-11 23:41:26,312 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-11 23:41:26,312 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-11 23:41:26,312 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-11 23:41:26,313 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-11 23:41:26,313 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-11 23:41:26,313 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-11 23:41:26,314 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-11 23:41:26,314 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-11 23:41:26,314 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-11 23:41:26,314 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-11 23:41:26,315 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-11 23:41:26,316 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-11 23:41:26,318 - INFO - root - 
============================================================
2025-11-11 23:41:26,318 - INFO - root - SERVICES ACCESSIBLES :
2025-11-11 23:41:26,318 - INFO - root - ============================================================
2025-11-11 23:41:26,318 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-11 23:41:26,318 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-11 23:41:26,319 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-11 23:41:26,319 - INFO - root - ============================================================

2025-11-11 23:41:26,319 - INFO - root - Starting all services...
2025-11-11 23:41:26,320 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-11 23:41:26,321 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-11 23:41:26,321 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-11 23:41:26,322 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-11 23:41:26,323 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-11 23:41:26,323 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-11 23:41:26,323 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-11 23:41:26,323 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-11 23:41:26,324 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-11 23:41:26,324 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-11 23:41:26,324 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-11 23:41:26,324 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-11 23:41:26,324 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-11 23:41:26,325 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-11 23:41:26,325 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-11 23:41:26,326 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-11 23:43:09,394 - ERROR - root - Error handling /v1/models: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\lmstudio_proxy_thread.py", line 811, in _list_openai_models_handler
    id_mapping = gguf_metadata.get_model_name_to_id_mapping(all_models_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 546, in get_model_name_to_id_mapping
    model_path = model_config.get("model_path")
                 ^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'

2025-11-11 23:45:47,723 - ERROR - root - Error handling /v1/models: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\lmstudio_proxy_thread.py", line 811, in _list_openai_models_handler
    id_mapping = gguf_metadata.get_model_name_to_id_mapping(all_models_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 546, in get_model_name_to_id_mapping
    model_path = model_config.get("model_path")
                 ^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'

2025-11-12 00:21:55,949 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 00:21:55,949 - INFO - root - Shutdown requested, stopping services...
2025-11-12 00:21:55,949 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 00:21:55,949 - INFO - llama_runner.headless_service_manager - Stopping Ollama proxy...
2025-11-12 00:21:55,949 - INFO - llama_runner.headless_service_manager - Stopping LM Studio proxy...
2025-11-12 00:21:56,057 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 00:21:56,058 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 00:21:56,058 - INFO - root - All services stopped gracefully
2025-11-12 00:56:47,313 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 00:56:47,314 - INFO - root - Shutdown requested, stopping services...
2025-11-12 00:56:47,314 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 00:56:47,314 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 00:56:47,314 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 00:56:47,314 - INFO - root - All services stopped gracefully
2025-11-12 00:58:44,145 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 00:58:44,145 - INFO - root - Shutdown requested, stopping services...
2025-11-12 00:58:44,145 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 00:58:44,146 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 00:58:44,146 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 00:58:44,146 - INFO - root - All services stopped gracefully
2025-11-12 01:00:36,568 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:00:36,569 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:00:36,569 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:00:36,569 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:00:36,569 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:00:36,570 - INFO - root - All services stopped gracefully
2025-11-12 01:01:39,813 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:01:39,813 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:01:39,813 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:01:39,814 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:01:39,814 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:01:39,814 - INFO - root - All services stopped gracefully
2025-11-12 01:05:00,889 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:05:00,890 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:05:00,890 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:05:00,890 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:05:00,890 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:05:00,890 - INFO - root - All services stopped gracefully
2025-11-12 01:06:54,280 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:06:54,280 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:06:54,281 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:06:54,281 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:06:54,281 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:06:54,281 - INFO - root - All services stopped gracefully
2025-11-12 01:07:32,643 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:07:32,644 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:07:32,644 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:07:32,644 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:07:32,644 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:07:32,644 - INFO - root - All services stopped gracefully
2025-11-12 01:08:15,326 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:08:15,326 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:08:15,326 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:08:15,327 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:08:15,327 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:08:15,327 - INFO - root - All services stopped gracefully
2025-11-12 01:13:26,533 - INFO - root - Logging configured for headless mode
2025-11-12 01:13:26,533 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 01:13:26,533 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 01:13:26,534 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 01:13:26,534 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 01:13:26,534 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 01:13:26,534 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 01:13:26,534 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 01:13:26,535 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 01:13:26,535 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 01:13:26,535 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 01:13:26,535 - INFO - root - Configuration loaded and validated successfully
2025-11-12 01:13:26,535 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 01:13:26,535 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 01:13:26,535 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 01:13:26,536 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 01:13:26,536 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 01:13:26,536 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 01:13:26,536 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 01:13:26,536 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 01:13:26,537 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 01:13:26,537 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 01:13:26,537 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 01:13:26,537 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 01:13:26,541 - DEBUG - asyncio - Using proactor: IocpProactor
2025-11-12 01:13:26,542 - INFO - root - 
============================================================
2025-11-12 01:13:26,543 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 01:13:26,543 - INFO - root - ============================================================
2025-11-12 01:13:26,543 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 01:13:26,543 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 01:13:26,543 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 01:13:26,543 - INFO - root - ============================================================

2025-11-12 01:13:26,543 - INFO - root - Starting all services...
2025-11-12 01:13:26,544 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 01:13:26,544 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 01:13:26,544 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 01:13:26,545 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 01:13:26,545 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 01:13:26,546 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 01:13:26,546 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 01:13:26,546 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 0.0.0.0:8585
2025-11-12 01:13:26,546 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://0.0.0.0:8585/
2025-11-12 01:13:26,546 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - üè† llama.cpp WebUI: http://localhost:8035/
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - no proxy needed
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - üè† Llama Runner WebUI: http://localhost:8081/
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 01:13:26,547 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 01:13:26,548 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 01:17:26,968 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:26,968 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:26,968 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:26,969 - INFO - llama_runner.headless_service_manager - Stopping Ollama proxy...
2025-11-12 01:17:26,969 - INFO - llama_runner.headless_service_manager - Stopping LM Studio proxy...
2025-11-12 01:17:26,969 - INFO - llama_runner.headless_service_manager - Stopping Dashboard API...
2025-11-12 01:17:27,078 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:27,078 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:27,079 - INFO - root - All services stopped gracefully
2025-11-12 01:17:37,581 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:37,581 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:37,581 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:37,581 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:37,581 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:37,581 - INFO - root - All services stopped gracefully
2025-11-12 01:17:39,960 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:39,961 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:39,961 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:39,961 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:39,961 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:39,961 - INFO - root - All services stopped gracefully
2025-11-12 01:17:40,335 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:40,335 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:40,335 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:40,335 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:40,335 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:40,336 - INFO - root - All services stopped gracefully
2025-11-12 01:17:40,581 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:40,581 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:40,581 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:40,581 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:40,581 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:40,581 - INFO - root - All services stopped gracefully
2025-11-12 01:17:40,792 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:40,792 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:40,792 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:40,792 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:40,792 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:40,793 - INFO - root - All services stopped gracefully
2025-11-12 01:17:40,972 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:40,972 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:40,972 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:40,972 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:40,972 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:40,973 - INFO - root - All services stopped gracefully
2025-11-12 01:17:41,185 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:41,185 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:41,185 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:41,186 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:41,186 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:41,186 - INFO - root - All services stopped gracefully
2025-11-12 01:17:41,390 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:41,390 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:41,391 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:41,391 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:41,391 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:41,391 - INFO - root - All services stopped gracefully
2025-11-12 01:17:41,580 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:41,581 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:41,581 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:41,581 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:41,581 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:41,581 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,086 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,087 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,087 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,087 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,087 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,087 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,114 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,114 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,114 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,114 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,115 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,115 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,144 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,144 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,145 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,145 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,145 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,145 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,177 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,177 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,177 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,177 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,177 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,177 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,206 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,207 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,207 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,207 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,207 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,207 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,235 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,235 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,235 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,235 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,235 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,236 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,265 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,266 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,266 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,266 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,266 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,266 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,297 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,297 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,297 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,298 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,298 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,298 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,326 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,326 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,326 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,326 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,326 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,327 - INFO - root - All services stopped gracefully
2025-11-12 01:17:42,357 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 01:17:42,357 - INFO - root - Shutdown requested, stopping services...
2025-11-12 01:17:42,357 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 01:17:42,357 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 01:17:42,358 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 01:17:42,358 - INFO - root - All services stopped gracefully
2025-11-12 02:12:49,109 - INFO - root - Received signal 2, initiating shutdown
2025-11-12 02:12:49,109 - INFO - root - Shutdown requested, stopping services...
2025-11-12 02:12:49,109 - INFO - llama_runner.headless_service_manager - Stopping all services...
2025-11-12 02:12:49,109 - INFO - llama_runner.headless_service_manager - Stopping all Llama runners...
2025-11-12 02:12:49,110 - INFO - llama_runner.headless_service_manager - ‚úÖ All headless services stopped successfully.
2025-11-12 02:12:49,110 - INFO - root - All services stopped gracefully
2025-11-12 14:02:31,857 - INFO - root - Logging configured for headless mode
2025-11-12 14:02:31,857 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 14:02:31,857 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 14:02:31,857 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 14:02:31,858 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 14:02:31,858 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 14:02:31,858 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 14:02:31,858 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 14:02:31,858 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 14:02:31,858 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 14:02:31,858 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 14:02:31,858 - INFO - root - Configuration loaded and validated successfully
2025-11-12 14:02:31,859 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 14:02:31,859 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 14:02:31,859 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 14:02:31,859 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 14:02:31,859 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 14:02:31,859 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 14:02:31,860 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 14:02:31,860 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 14:02:31,860 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 14:02:31,860 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 14:02:31,860 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 14:02:31,860 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 14:02:31,862 - INFO - root - 
============================================================
2025-11-12 14:02:31,863 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 14:02:31,863 - INFO - root - ============================================================
2025-11-12 14:02:31,863 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 14:02:31,863 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 14:02:31,863 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 14:02:31,863 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 14:02:31,863 - INFO - root - ============================================================

2025-11-12 14:02:31,863 - INFO - root - Starting all services...
2025-11-12 14:02:31,864 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 14:02:31,864 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 14:02:31,864 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 14:02:31,864 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 14:02:31,865 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 14:02:31,865 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 14:02:31,865 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 14:02:31,865 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 0.0.0.0:8585
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://0.0.0.0:8585/
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 14:02:31,866 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 14:02:31,867 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 14:02:31,867 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 14:02:31,867 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 14:02:46,612 - INFO - root - Headless service shutting down
2025-11-12 23:22:00,206 - INFO - root - Logging configured for headless mode
2025-11-12 23:22:00,207 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 23:22:00,207 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:22:00,207 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:22:00,207 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:22:00,208 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:22:00,208 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:22:00,208 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:22:00,208 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:22:00,209 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 23:22:00,209 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 23:22:00,209 - INFO - root - Configuration loaded and validated successfully
2025-11-12 23:22:00,209 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 23:22:00,210 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 23:22:00,210 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:22:00,210 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:22:00,210 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:22:00,211 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:22:00,211 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:22:00,211 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:22:00,211 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:22:00,212 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 23:22:00,212 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 23:22:00,212 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 23:22:00,214 - INFO - root - 
============================================================
2025-11-12 23:22:00,215 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 23:22:00,215 - INFO - root - ============================================================
2025-11-12 23:22:00,215 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 23:22:00,215 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 23:22:00,215 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 23:22:00,216 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 23:22:00,216 - INFO - root - ============================================================

2025-11-12 23:22:00,216 - INFO - root - Starting all services...
2025-11-12 23:22:00,216 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 23:22:00,216 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 23:22:00,217 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 23:22:00,217 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 23:22:00,218 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 23:22:00,218 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 23:22:00,218 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 23:22:00,219 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-12 23:22:00,219 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-12 23:22:00,220 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 23:22:00,220 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 23:22:00,220 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 23:22:00,220 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 23:22:00,220 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 23:22:00,221 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 23:22:00,221 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 23:22:00,221 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 23:22:00,221 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 23:22:00,221 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 23:22:24,471 - INFO - root - Headless service shutting down
2025-11-12 23:25:53,249 - INFO - root - Logging configured for headless mode
2025-11-12 23:25:53,250 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 23:25:53,250 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:25:53,251 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:25:53,251 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:25:53,251 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:25:53,252 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:25:53,252 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:25:53,253 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:25:53,253 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 23:25:53,253 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 23:25:53,254 - INFO - root - Configuration loaded and validated successfully
2025-11-12 23:25:53,254 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 23:25:53,254 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 23:25:53,255 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:25:53,255 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:25:53,256 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:25:53,256 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:25:53,257 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:25:53,257 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:25:53,257 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:25:53,258 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 23:25:53,258 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 23:25:53,258 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 23:25:53,261 - INFO - root - 
============================================================
2025-11-12 23:25:53,262 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 23:25:53,262 - INFO - root - ============================================================
2025-11-12 23:25:53,262 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 23:25:53,262 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 23:25:53,263 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 23:25:53,263 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 23:25:53,263 - INFO - root - ============================================================

2025-11-12 23:25:53,263 - INFO - root - Starting all services...
2025-11-12 23:25:53,263 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 23:25:53,263 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 23:25:53,264 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 23:25:53,264 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 23:25:53,265 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 23:25:53,265 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 23:25:53,265 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 23:25:53,266 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-12 23:25:53,266 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-12 23:25:53,266 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 23:25:53,266 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 23:25:53,267 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 23:25:53,267 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 23:25:53,267 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 23:25:53,267 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 23:25:53,267 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 23:25:53,268 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 23:25:53,268 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 23:25:53,268 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 23:30:39,744 - ERROR - root - Error handling /v1/models: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\lmstudio_proxy_thread.py", line 811, in _list_openai_models_handler
    id_mapping = gguf_metadata.get_model_name_to_id_mapping(all_models_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 546, in get_model_name_to_id_mapping
    model_path = model_config.get("model_path")
                 ^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'

2025-11-12 23:31:21,450 - INFO - root - Headless service shutting down
2025-11-12 23:36:48,649 - INFO - root - Logging configured for headless mode
2025-11-12 23:36:48,649 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 23:36:48,650 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:36:48,650 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:36:48,650 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:36:48,650 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:36:48,650 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:36:48,650 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:36:48,650 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:36:48,650 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 23:36:48,651 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 23:36:48,651 - INFO - root - Configuration loaded and validated successfully
2025-11-12 23:36:48,651 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 23:36:48,651 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 23:36:48,651 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:36:48,652 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:36:48,652 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:36:48,652 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:36:48,652 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:36:48,653 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:36:48,653 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:36:48,653 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 23:36:48,653 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 23:36:48,653 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 23:36:48,656 - INFO - root - 
============================================================
2025-11-12 23:36:48,656 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 23:36:48,657 - INFO - root - ============================================================
2025-11-12 23:36:48,657 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 23:36:48,657 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 23:36:48,657 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 23:36:48,657 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 23:36:48,658 - INFO - root - ============================================================

2025-11-12 23:36:48,658 - INFO - root - Starting all services...
2025-11-12 23:36:48,658 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 23:36:48,658 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 23:36:48,658 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 23:36:48,659 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 23:36:48,660 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 23:36:48,660 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 23:36:48,661 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 23:36:48,661 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-12 23:36:48,662 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-12 23:36:48,662 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 23:36:48,662 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 23:36:48,663 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 23:36:48,663 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 23:36:48,663 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 23:36:48,664 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 23:36:48,664 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 23:36:48,664 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 23:36:48,664 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 23:36:48,665 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 23:39:40,873 - ERROR - root - Error handling /v1/models: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\lmstudio_proxy_thread.py", line 811, in _list_openai_models_handler
    id_mapping = gguf_metadata.get_model_name_to_id_mapping(all_models_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 546, in get_model_name_to_id_mapping
    model_path = model_config.get("model_path")
                 ^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'

2025-11-12 23:39:54,071 - INFO - root - Headless service shutting down
2025-11-12 23:39:55,411 - INFO - root - Logging configured for headless mode
2025-11-12 23:39:55,411 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 23:39:55,412 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:39:55,412 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:39:55,412 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:39:55,413 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:39:55,413 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:39:55,413 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:39:55,413 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:39:55,414 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 23:39:55,414 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 23:39:55,414 - INFO - root - Configuration loaded and validated successfully
2025-11-12 23:39:55,414 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 23:39:55,415 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 23:39:55,415 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:39:55,415 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:39:55,415 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:39:55,416 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:39:55,416 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:39:55,416 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:39:55,416 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:39:55,417 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 23:39:55,417 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 23:39:55,417 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 23:39:55,419 - INFO - root - 
============================================================
2025-11-12 23:39:55,419 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 23:39:55,419 - INFO - root - ============================================================
2025-11-12 23:39:55,419 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 23:39:55,420 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 23:39:55,420 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 23:39:55,420 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 23:39:55,420 - INFO - root - ============================================================

2025-11-12 23:39:55,420 - INFO - root - Starting all services...
2025-11-12 23:39:55,421 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 23:39:55,421 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 23:39:55,421 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 23:39:55,422 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 23:39:55,422 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 23:39:55,422 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 23:39:55,423 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 23:39:55,423 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-12 23:39:55,423 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-12 23:39:55,423 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 23:39:55,424 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 23:39:55,424 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 23:39:55,424 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 23:39:55,424 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 23:39:55,425 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 23:39:55,425 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 23:39:55,425 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 23:39:55,425 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 23:39:55,425 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 23:41:04,473 - INFO - root - Headless service shutting down
2025-11-12 23:42:33,751 - INFO - root - Logging configured for headless mode
2025-11-12 23:42:33,751 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 23:42:33,752 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:42:33,752 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:42:33,753 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:42:33,753 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:42:33,754 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:42:33,754 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:42:33,755 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:42:33,755 - INFO - root - Configuration loaded and validated successfully
2025-11-12 23:42:33,756 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 23:42:33,756 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 23:42:33,756 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:42:33,757 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:42:33,757 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:42:33,757 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:42:33,758 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:42:33,758 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:42:33,759 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:42:33,759 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 23:42:33,760 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 23:42:33,760 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 23:42:33,764 - INFO - root - 
============================================================
2025-11-12 23:42:33,764 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 23:42:33,765 - INFO - root - ============================================================
2025-11-12 23:42:33,765 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 23:42:33,765 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 23:42:33,765 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 23:42:33,766 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 23:42:33,766 - INFO - root - ============================================================

2025-11-12 23:42:33,766 - INFO - root - Starting all services...
2025-11-12 23:42:33,766 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 23:42:33,766 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 23:42:33,767 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 23:42:33,767 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 23:42:33,768 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 23:42:33,769 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 23:42:33,769 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 23:42:33,770 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-12 23:42:33,770 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-12 23:42:33,770 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 23:42:33,771 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 23:42:33,771 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 23:42:33,771 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 23:42:33,771 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 23:42:33,772 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 23:42:33,772 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 23:42:33,772 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 23:42:33,773 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 23:42:33,773 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-12 23:44:23,757 - ERROR - root - Error handling /v1/models: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\lmstudio_proxy_thread.py", line 811, in _list_openai_models_handler
    id_mapping = gguf_metadata.get_model_name_to_id_mapping(all_models_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 546, in get_model_name_to_id_mapping
    model_path = model_config.get("model_path")
                 ^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'

2025-11-12 23:45:05,694 - INFO - root - Headless service shutting down
2025-11-12 23:53:52,123 - INFO - root - Logging configured for headless mode
2025-11-12 23:53:52,124 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-12 23:53:52,124 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:53:52,124 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:53:52,124 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:53:52,124 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:53:52,124 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:53:52,125 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:53:52,125 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:53:52,125 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-12 23:53:52,125 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-12 23:53:52,125 - INFO - root - Configuration loaded and validated successfully
2025-11-12 23:53:52,125 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-12 23:53:52,125 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-12 23:53:52,126 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-12 23:53:52,126 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-12 23:53:52,126 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-12 23:53:52,126 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-12 23:53:52,126 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-12 23:53:52,126 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-12 23:53:52,127 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-12 23:53:52,127 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-12 23:53:52,127 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-12 23:53:52,127 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-12 23:53:52,129 - INFO - root - 
============================================================
2025-11-12 23:53:52,129 - INFO - root - SERVICES ACCESSIBLES :
2025-11-12 23:53:52,129 - INFO - root - ============================================================
2025-11-12 23:53:52,130 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-12 23:53:52,130 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-12 23:53:52,130 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-12 23:53:52,130 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-12 23:53:52,130 - INFO - root - ============================================================

2025-11-12 23:53:52,130 - INFO - root - Starting all services...
2025-11-12 23:53:52,130 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-12 23:53:52,131 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-12 23:53:52,131 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-12 23:53:52,131 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-12 23:53:52,132 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-12 23:53:52,132 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-12 23:53:52,132 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-12 23:53:52,132 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-12 23:53:52,133 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-12 23:53:52,133 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-12 23:53:52,133 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-12 23:53:52,133 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-12 23:53:52,133 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-12 23:53:52,134 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-12 23:53:52,134 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-12 23:53:52,134 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-12 23:53:52,134 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-12 23:53:52,134 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-12 23:53:52,134 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-13 00:03:47,909 - ERROR - root - Error handling /v1/models: 'str' object has no attribute 'get'
Traceback (most recent call last):
  File "F:\llm\llama-runner-async-proxy\llama_runner\lmstudio_proxy_thread.py", line 811, in _list_openai_models_handler
    id_mapping = gguf_metadata.get_model_name_to_id_mapping(all_models_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "F:\llm\llama-runner-async-proxy\llama_runner\gguf_metadata.py", line 546, in get_model_name_to_id_mapping
    model_path = model_config.get("model_path")
                 ^^^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'

2025-11-13 00:04:37,734 - INFO - root - Headless service shutting down
2025-11-13 00:04:43,876 - INFO - root - Logging configured for headless mode
2025-11-13 00:04:43,876 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-13 00:04:43,877 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 00:04:43,877 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 00:04:43,877 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 00:04:43,877 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 00:04:43,878 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 00:04:43,878 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 00:04:43,878 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 00:04:43,878 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-13 00:04:43,878 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-13 00:04:43,878 - INFO - root - Configuration loaded and validated successfully
2025-11-13 00:04:43,879 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-13 00:04:43,879 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-13 00:04:43,879 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 00:04:43,879 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 00:04:43,879 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 00:04:43,879 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 00:04:43,880 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 00:04:43,880 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 00:04:43,880 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 00:04:43,881 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-13 00:04:43,881 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-13 00:04:43,881 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-13 00:04:43,883 - INFO - root - 
============================================================
2025-11-13 00:04:43,883 - INFO - root - SERVICES ACCESSIBLES :
2025-11-13 00:04:43,884 - INFO - root - ============================================================
2025-11-13 00:04:43,884 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-13 00:04:43,884 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-13 00:04:43,884 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-13 00:04:43,884 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-13 00:04:43,884 - INFO - root - ============================================================

2025-11-13 00:04:43,884 - INFO - root - Starting all services...
2025-11-13 00:04:43,885 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-13 00:04:43,885 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-13 00:04:43,885 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-13 00:04:43,886 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-13 00:04:43,886 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-13 00:04:43,886 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-13 00:04:43,886 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-13 00:04:43,887 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-13 00:04:43,887 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-13 00:04:43,887 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-13 00:04:43,887 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-13 00:04:43,887 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-13 00:04:43,888 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-13 00:07:17,199 - INFO - root - Headless service shutting down
2025-11-13 00:18:40,336 - INFO - root - Logging configured for headless mode
2025-11-13 00:18:40,337 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-13 00:18:40,337 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 00:18:40,337 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 00:18:40,337 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 00:18:40,338 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 00:18:40,338 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 00:18:40,338 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 00:18:40,338 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 00:18:40,338 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-13 00:18:40,339 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-13 00:18:40,339 - INFO - root - Configuration loaded and validated successfully
2025-11-13 00:18:40,339 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-13 00:18:40,339 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-13 00:18:40,339 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 00:18:40,339 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 00:18:40,340 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 00:18:40,340 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 00:18:40,340 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 00:18:40,340 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 00:18:40,340 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 00:18:40,341 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-13 00:18:40,341 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-13 00:18:40,341 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-13 00:18:40,343 - INFO - root - 
============================================================
2025-11-13 00:18:40,343 - INFO - root - SERVICES ACCESSIBLES :
2025-11-13 00:18:40,343 - INFO - root - ============================================================
2025-11-13 00:18:40,343 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-13 00:18:40,343 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-13 00:18:40,344 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-13 00:18:40,344 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-13 00:18:40,344 - INFO - root - ============================================================

2025-11-13 00:18:40,344 - INFO - root - Starting all services...
2025-11-13 00:18:40,344 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-13 00:18:40,344 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-13 00:18:40,344 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-13 00:18:40,345 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-13 00:18:40,345 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-13 00:18:40,346 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-13 00:18:40,346 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-13 00:18:40,346 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-13 00:18:40,347 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-13 00:18:40,347 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-13 00:18:40,347 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-13 00:18:40,347 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-13 00:18:40,347 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-13 00:18:40,347 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-13 00:18:40,348 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-13 00:18:40,348 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-13 00:18:40,348 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-13 00:18:40,348 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-13 00:18:40,348 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-13 00:27:08,346 - INFO - root - Headless service shutting down
2025-11-13 10:16:05,268 - INFO - root - Logging configured for headless mode
2025-11-13 10:16:05,269 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-13 10:16:05,269 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 10:16:05,269 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 10:16:05,269 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 10:16:05,269 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 10:16:05,270 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 10:16:05,270 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 10:16:05,270 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 10:16:05,271 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-13 10:16:05,271 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-13 10:16:05,271 - INFO - root - Configuration loaded and validated successfully
2025-11-13 10:16:05,271 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-13 10:16:05,272 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-13 10:16:05,272 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 10:16:05,272 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 10:16:05,272 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 10:16:05,272 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 10:16:05,273 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 10:16:05,273 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 10:16:05,273 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 10:16:05,273 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-13 10:16:05,274 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-13 10:16:05,274 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-13 10:16:05,276 - INFO - root - 
============================================================
2025-11-13 10:16:05,276 - INFO - root - SERVICES ACCESSIBLES :
2025-11-13 10:16:05,276 - INFO - root - ============================================================
2025-11-13 10:16:05,276 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-13 10:16:05,276 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-13 10:16:05,276 - INFO - root - Dashboard Web: http://127.0.0.1:8080/
2025-11-13 10:16:05,277 - INFO - root - Dashboard API: http://127.0.0.1:8585/
2025-11-13 10:16:05,277 - INFO - root - ============================================================

2025-11-13 10:16:05,277 - INFO - root - Starting all services...
2025-11-13 10:16:05,277 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-13 10:16:05,277 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-13 10:16:05,277 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-13 10:16:05,278 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-13 10:16:05,278 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-13 10:16:05,279 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-13 10:16:05,279 - INFO - llama_runner.headless_service_manager - Starting Dashboard API server...
2025-11-13 10:16:05,279 - INFO - llama_runner.services.dashboard_api - Starting Dashboard API server on 127.0.0.1:8585
2025-11-13 10:16:05,279 - INFO - llama_runner.headless_service_manager - ‚úÖ Dashboard API server started on http://127.0.0.1:8585/
2025-11-13 10:16:05,279 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-13 10:16:05,280 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-13 10:16:05,280 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-13 10:16:05,280 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-13 10:16:05,280 - INFO - llama_runner.headless_service_manager - üè† Dashboard Web: http://localhost:8080/
2025-11-13 10:16:05,281 - INFO - llama_runner.headless_service_manager -    ‚úÖ Direct access - Dashboard Web
2025-11-13 10:16:05,281 - INFO - llama_runner.headless_service_manager - üîó Dashboard API: http://localhost:8585/
2025-11-13 10:16:05,281 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-13 10:16:05,281 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-13 10:16:05,281 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-13 10:18:00,591 - INFO - root - Headless service shutting down
2025-11-13 12:45:37,270 - INFO - root - Logging configured for headless mode
2025-11-13 12:45:37,271 - INFO - root - === STARTING HEADLESS SERVICE ===
2025-11-13 12:45:37,271 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 12:45:37,271 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 12:45:37,271 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 12:45:37,272 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 12:45:37,272 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 12:45:37,272 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 12:45:37,272 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 12:45:37,273 - WARNING - llama_runner.services.config_validator - [WARNING] config: No models configured
2025-11-13 12:45:37,273 - WARNING - llama_runner.services.config_validator - Configuration validation passed with 1 warning(s)
2025-11-13 12:45:37,273 - INFO - root - Configuration loaded and validated successfully
2025-11-13 12:45:37,273 - INFO - llama_runner.headless_service_manager - Initializing services for headless mode...
2025-11-13 12:45:37,273 - WARNING - llama_runner.headless_service_manager - Audio section is missing or None in config. Using empty models dict.
2025-11-13 12:45:37,273 - INFO - root - üîÑ V√©rification de l'existence des fichiers de configuration...
2025-11-13 12:45:37,274 - INFO - root - ‚úÖ V√©rification des r√©pertoires n√©cessaires...
2025-11-13 12:45:37,274 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\config
2025-11-13 12:45:37,274 - INFO - root - ‚úÖ R√©pertoire d√©j√† existant : F:\llm\llama-runner-async-proxy\logs
2025-11-13 12:45:37,274 - INFO - root - üîç Fichier app_config.json existe : True
2025-11-13 12:45:37,274 - INFO - root - üîç Fichier models_config.json existe : True
2025-11-13 12:45:37,275 - INFO - root - ‚úÖ Les fichiers de configuration existent d√©j√†
2025-11-13 12:45:37,275 - INFO - llama_runner.headless_service_manager - LlamaRunnerManager initialized.
2025-11-13 12:45:37,275 - INFO - llama_runner.headless_service_manager - Ollama Proxy is enabled. Creating server...
2025-11-13 12:45:37,275 - INFO - llama_runner.headless_service_manager - LM Studio Proxy is enabled. Creating server...
2025-11-13 12:45:37,275 - INFO - root - 
============================================================
2025-11-13 12:45:37,276 - INFO - root - SERVICES ACCESSIBLES :
2025-11-13 12:45:37,276 - INFO - root - ============================================================
2025-11-13 12:45:37,276 - INFO - root - Ollama Proxy: http://127.0.0.1:11434/
2025-11-13 12:45:37,276 - INFO - root - LM Studio Proxy: http://127.0.0.1:1234/
2025-11-13 12:45:37,276 - INFO - root - ============================================================

2025-11-13 12:45:37,276 - INFO - root - Starting all services...
2025-11-13 12:45:37,276 - INFO - root - Headless service running. Press Ctrl+C to shutdown.
2025-11-13 12:45:37,276 - INFO - llama_runner.headless_service_manager - Starting all services...
2025-11-13 12:45:37,276 - INFO - llama_runner.headless_service_manager - Starting Ollama proxy server...
2025-11-13 12:45:37,277 - INFO - llama_runner.headless_service_manager - ‚úÖ Ollama Proxy server started on http://0.0.0.0:11434/
2025-11-13 12:45:37,278 - INFO - llama_runner.headless_service_manager - Starting LM Studio proxy server...
2025-11-13 12:45:37,278 - INFO - llama_runner.headless_service_manager - ‚úÖ LM Studio Proxy server started on http://0.0.0.0:1234/
2025-11-13 12:45:37,278 - INFO - llama_runner.headless_service_manager - ‚úÖ All services started successfully. Waiting for shutdown signal...
2025-11-13 12:45:37,278 - INFO - llama_runner.headless_service_manager - 
============================================================
2025-11-13 12:45:37,279 - INFO - llama_runner.headless_service_manager - üåê SERVICES ACCESSIBLES :
2025-11-13 12:45:37,279 - INFO - llama_runner.headless_service_manager - ============================================================
2025-11-13 12:45:37,279 - INFO - llama_runner.headless_service_manager - üîó Ollama Proxy: http://localhost:11434/
2025-11-13 12:45:37,279 - INFO - llama_runner.headless_service_manager - üîó LM Studio Proxy: http://localhost:1234/
2025-11-13 12:45:37,279 - INFO - llama_runner.headless_service_manager - ============================================================

2025-11-13 12:45:55,218 - INFO - root - Headless service shutting down
