2025-11-09 23:35:47,424 - INFO - üöÄ Starting component tests
2025-11-09 23:35:47,424 - INFO - === Testing Ollama Proxy ===
2025-11-09 23:35:48,122 - INFO - Ensured metadata cache directory exists: F:\llm\llama-runner-async-proxy\config\metadata_cache
2025-11-09 23:35:48,386 - INFO - Updated dynamic routing handlers for /v1/chat/completions, /v1/completions, /v1/embeddings to support conditional streaming.
2025-11-09 23:35:48,409 - INFO - Initializing services for headless mode...
2025-11-09 23:35:48,410 - INFO - LlamaRunnerManager initialized.
2025-11-09 23:35:48,410 - INFO - Ollama Proxy is enabled. Creating server...
2025-11-09 23:35:48,411 - INFO - LM Studio Proxy is enabled. Creating server...
2025-11-09 23:35:48,411 - INFO - Starting Ollama proxy service...
2025-11-09 23:35:48,412 - ERROR - ‚ùå Ollama proxy test failed: 'HeadlessServiceManager' object has no attribute '_start_ollama_proxy'
2025-11-09 23:35:48,412 - INFO - === Testing LM Studio Proxy ===
2025-11-09 23:35:48,413 - INFO - Initializing services for headless mode...
2025-11-09 23:35:48,413 - INFO - LlamaRunnerManager initialized.
2025-11-09 23:35:48,414 - INFO - Ollama Proxy is enabled. Creating server...
2025-11-09 23:35:48,414 - INFO - LM Studio Proxy is enabled. Creating server...
2025-11-09 23:35:48,414 - INFO - Starting LM Studio proxy service...
2025-11-09 23:35:48,414 - ERROR - ‚ùå LM Studio proxy test failed: 'HeadlessServiceManager' object has no attribute '_start_lmstudio_proxy'
2025-11-09 23:35:48,414 - INFO - 
=== TEST SUMMARY ===
2025-11-09 23:35:48,414 - INFO - Ollama proxy test: ‚ùå FAIL
2025-11-09 23:35:48,415 - INFO - LM Studio proxy test: ‚ùå FAIL
