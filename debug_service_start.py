import logging
import asyncio
import sys
from pathlib import Path

# Configuration du logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def start_services_step_by_step():
    logger.info("ğŸš€ DÃ©marrage des services Ã©tape par Ã©tape")
    
    try:
        logger.info("ğŸ“¥ Import des modules nÃ©cessaires...")
        from llama_runner.config_loader import load_config
        from llama_runner.headless_service_manager import HeadlessServiceManager
        logger.info("âœ… Imports rÃ©ussis")
        
        logger.info("ğŸ”„ Chargement de la configuration...")
        config = load_config()
        logger.info(f"âœ… Configuration chargÃ©e avec {len(config)} clÃ©s")
        
        # Extrait les modÃ¨les de la configuration
        models_config = config.get('models', {})
        logger.info(f"ğŸ“‹ Nombre de modÃ¨les configurÃ©s: {len(models_config)}")
        
        logger.info("ğŸ”§ Initialisation HeadlessServiceManager...")
        hsm = HeadlessServiceManager(config, models_config)
        logger.info("âœ… HeadlessServiceManager initialisÃ© avec succÃ¨s")
        
        logger.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info("ğŸ”§ Ã‰TAPE 1 : DÃ©marrage Ollama Proxy")
        logger.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # DÃ©marrage Ollama proxy uniquement
        if hsm.ollama_proxy and hsm.llama_runner_manager:
            logger.info("â–¶ï¸ DÃ©marrage Ollama proxy server...")
            try:
                from fastapi import FastAPI
                from uvicorn import Config, Server
                
                ollama_app = FastAPI()
                ollama_app.state.get_runner_port_callback = hsm.llama_runner_manager.get_runner_port
                ollama_app.state.request_runner_start_callback = hsm.llama_runner_manager.request_runner_start
                ollama_app.state.llama_runner_manager = hsm.llama_runner_manager
                
                config = Config(
                    app=ollama_app,
                    host="0.0.0.0",
                    port=11434,
                    log_level="debug"
                )
                server = Server(config)
                hsm.ollama_server = server
                
                # CrÃ©e une tÃ¢che pour le serveur
                server_task = asyncio.create_task(server.serve())
                logger.info("âœ… Ollama Proxy server dÃ©marrÃ© sur http://0.0.0.0:11434/")
                
                # Attends un peu pour voir si le serveur dÃ©marre correctement
                await asyncio.sleep(2)
                
                # VÃ©rifie si la tÃ¢che est toujours en cours
                if not server_task.done():
                    logger.info("âœ… Ollama Proxy server fonctionne correctement")
                    server_task.cancel()  # Annule la tÃ¢che pour passer Ã  l'Ã©tape suivante
                    await server_task
                    logger.info("ğŸ›‘ Ollama Proxy server arrÃªtÃ© pour test")
                else:
                    logger.error("âŒ Ollama Proxy server a Ã©chouÃ© au dÃ©marrage")
                    return False
                    
            except Exception as e:
                logger.error(f"âŒ Erreur lors du dÃ©marrage Ollama proxy: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
        
        logger.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info("ğŸ”§ Ã‰TAPE 2 : DÃ©marrage LM Studio Proxy")
        logger.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # DÃ©marrage LM Studio proxy uniquement
        if hsm.lmstudio_proxy and hsm.llama_runner_manager:
            logger.info("â–¶ï¸ DÃ©marrage LM Studio proxy server...")
            try:
                from fastapi import FastAPI
                from uvicorn import Config, Server
                
                lmstudio_app = FastAPI()
                lmstudio_app.state.all_models_config = models_config
                lmstudio_app.state.get_runner_port_callback = hsm.llama_runner_manager.get_runner_port
                lmstudio_app.state.runtimes_config = config.get("llama-runtimes", {})
                lmstudio_app.state.request_runner_start_callback = hsm.llama_runner_manager.request_runner_start
                lmstudio_app.state.is_model_running_callback = hsm.llama_runner_manager.is_llama_runner_running
                lmstudio_app.state.proxy_thread_instance = hsm.lmstudio_proxy
                lmstudio_app.state.llama_runner_manager = hsm.llama_runner_manager
                
                config = Config(
                    app=lmstudio_app,
                    host="0.0.0.0",
                    port=1234,
                    log_level="debug"
                )
                server = Server(config)
                hsm.lmstudio_server = server
                
                # CrÃ©e une tÃ¢che pour le serveur
                server_task = asyncio.create_task(server.serve())
                logger.info("âœ… LM Studio Proxy server dÃ©marrÃ© sur http://0.0.0.0:1234/")
                
                # Attends un peu pour voir si le serveur dÃ©marre correctement
                await asyncio.sleep(2)
                
                # VÃ©rifie si la tÃ¢che est toujours en cours
                if not server_task.done():
                    logger.info("âœ… LM Studio Proxy server fonctionne correctement")
                    server_task.cancel()  # Annule la tÃ¢che pour passer Ã  l'Ã©tape suivante
                    await server_task
                    logger.info("ğŸ›‘ LM Studio Proxy server arrÃªtÃ© pour test")
                else:
                    logger.error("âŒ LM Studio Proxy server a Ã©chouÃ© au dÃ©marrage")
                    return False
                    
            except Exception as e:
                logger.error(f"âŒ Erreur lors du dÃ©marrage LM Studio proxy: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return False
        
        logger.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        logger.info("ğŸ”§ Ã‰TAPE 3 : DÃ©marrage WebUI Service")
        logger.info("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # DÃ©marrage WebUI service
        logger.info("â–¶ï¸ DÃ©marrage Llama Runner WebUI service...")
        try:
            from fastapi import FastAPI
            from uvicorn import Config, Server
            from fastapi.responses import HTMLResponse
            
            webui_app = FastAPI()
            
            @webui_app.get("/", response_class=HTMLResponse)
            async def webui_root():
                return "<h1>Test WebUI</h1><p>Service fonctionnel</p>"
            
            config = Config(
                app=webui_app,
                host="0.0.0.0",
                port=8081,
                log_level="debug"
            )
            server = Server(config)
            hsm.webui_server = server
            
            # CrÃ©e une tÃ¢che pour le serveur
            server_task = asyncio.create_task(server.serve())
            logger.info("âœ… Llama Runner WebUI service dÃ©marrÃ© sur http://0.0.0.0:8081/")
            
            # Attends un peu pour voir si le serveur dÃ©marre correctement
            await asyncio.sleep(2)
            
            # VÃ©rifie si la tÃ¢che est toujours en cours
            if not server_task.done():
                logger.info("âœ… WebUI service fonctionne correctement")
                server_task.cancel()  # Annule la tÃ¢che
                await server_task
                logger.info("ğŸ›‘ WebUI service arrÃªtÃ©")
            else:
                logger.error("âŒ WebUI service a Ã©chouÃ© au dÃ©marrage")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Erreur lors du dÃ©marrage WebUI service: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
        
        logger.info("ğŸ‰ Tous les services ont dÃ©marrÃ© correctement dans les tests Ã©tape par Ã©tape!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erreur globale: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    logger.info("ğŸ SCRIPT DE DÃ‰BOGAGE DÃ‰TAILLÃ‰ DU DÃ‰MARRAGE DES SERVICES")
    logger.info(f"ğŸ Python version: {sys.version}")
    logger.info(f"ğŸ“ RÃ©pertoire courant: {Path.cwd()}")
    
    try:
        loop = asyncio.get_event_loop()
        success = loop.run_until_complete(start_services_step_by_step())
        logger.info(f"âœ¨ Test d'Ã©tape par Ã©tape {'rÃ©ussi' if success else 'Ã©chouÃ©'}")
        loop.close()
    except Exception as e:
        logger.error(f"âŒ Erreur fatale: {e}")
        import traceback
        logger.error(traceback.format_exc())