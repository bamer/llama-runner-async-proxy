{
  "proxies": {
    "ollama": {
      "enabled": true
    },
    "lmstudio": {
      "enabled": true
    }
  },
  "llama-runtimes": {
    "llama-server": {
      "runtime": "F:/llm/llama/llama-server.exe"
    }
  },
  "global_model_parameters": {
    "ctx_size": 16096,
    "n_gpu_layers": 100,
    "temp": 0.8,
    "top_p": 0.95,
    "top_k": 40,
    "repeat_penalty": 1.1,
    "repeat_last_n": 64,
    "batch_size": 2048,
    "ubatch_size": 1024,
    "threads": 10,
    "flash_attn": "auto",
    "mlock": true,
    "no_mmap": true,
    "no_mul_mat_q": false,
    "cache_type_k": "f16",
    "cache_type_v": "f16",
    "rope_freq_base": 0.0,
    "rope_freq_scale": 0.0,
    "yarn_ext_factor": -1.0,
    "yarn_attn_factor": 1.0,
    "yarn_beta_fast": 32.0,
    "yarn_beta_slow": 1.0,
    "yarn_orig_ctx": 0,
    "no_kv_offload": false,
    "main_gpu": 0,
    "rpc_secure": false,
    "reasoning_budget": -1,
    "jinja": true,
    "no_warmup": false,
    "no_context_shift": false,
    "n_cpu_moe": 1,
    "min_p": 0.0
  },
  "model_discovery": {
    "enabled": false,
    "base_path": "F:\\llm\\llama\\models",
    "auto_update": false
  },
  "audio": {
    "models": {
      "whisper-tiny": {
        "model_path": "tiny",
        "parameters": {
          "device": "cpu",
          "compute_type": "int8",
          "threads": 4,
          "language": null,
          "beam_size": 5
        }
      },
      "whisper-base": {
        "model_path": "base",
        "parameters": {
          "device": "cpu",
          "compute_type": "int8",
          "threads": 4,
          "language": null,
          "beam_size": 5
        }
      },
      "whisper-small": {
        "model_path": "small",
        "parameters": {
          "device": "cpu", 
          "compute_type": "int8",
          "threads": 4,
          "language": null,
          "beam_size": 5
        }
      }
    }
  },
  "models": {
    "Chroma1-HD-Flash-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Chroma1-HD-Flash-Q4_K_S-GGUF\\Chroma1-HD-Flash-Q4_K_S.gguf",
      "model_id": "Chroma1 HD Flash (Q4)",
      "mmproj": "F:\\llm\\llama\\models\\Chroma1-HD-Flash-Q4_K_S-GGUF\\mmproj-F32.gguf",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M-GGUF\\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
      "model_id": "DeepSeek R1 0528 Qwen3 8B (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Dorado-WebSurf_Tool-ext.Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\\Dorado-WebSurf_Tool-ext.Q8_0.gguf",
      "model_id": "Dorado WebSurf Tool (Q8)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "gemma-2-9b-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\gemma-2-9b-GGUF\\gemma-2-9b-Q8_0.gguf",
      "model_id": "Gemma 2 9B (Q8)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 1.0,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "top_p": 1.0,
        "top_k": 0
      }
    },
    "granite-4_0-h-tiny-IQ4_XS": {
      "model_path": "F:\\llm\\llama\\models\\granite-4_0-h-tiny-IQ4_XS\\model.gguf",
      "model_id": "Granite 4.0 H Tiny (IQ4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85
      }
    },
    "Jan-v1-4B-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Jan-v1-4B-Q4_K_M\\model.gguf",
      "model_id": "Jan v1 4B (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Magistral-Small-2509-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Magistral-Small-2509-Q4_K_M-GGUF\\Magistral-Small-2509-Q4_K_M.gguf",
      "model_id": "Magistral Small 2509 (Q4)",
      "mmproj": "F:\\llm\\llama\\models\\Magistral-Small-2509-Q4_K_M-GGUF\\mmproj-Magistral-Small-2509-F16.gguf",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "neutss-air-BF16": {
      "model_path": "F:\\llm\\llama\\models\\neutts-air\\neutss-air-BF16.gguf",
      "model_id": "Neutss Air (BF16)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85
      }
    },
    "OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1": {
      "model_path": "F:\\llm\\llama\\models\\OpenAI-20B-NEO-CODE-DI-Uncensored-Q5_1\\model.gguf",
      "model_id": "OpenAI 20B NEO CODE DI Uncensored (Q5)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Phi-4-mini-instruct_Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Phi-4-mini-instruct_Q4_K_M\\model.gguf",
      "model_id": "Phi 4 Mini Instruct (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85
      }
    },
    "Qwen2.5-7B-Instruct-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-7B-Instruct-Q4_K_M-GGUF\\Qwen2.5-7B-Instruct-Q4_K_M.gguf",
      "model_id": "Qwen2.5 7B Instruct (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen2.5-Coder-1.5B-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-Coder-1.5B-Q8_0-GGUF\\qwen2.5-coder-1.5b-q8_0.gguf",
      "model_id": "Qwen2.5 Coder 1.5B (Q8)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M-GGUF\\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M.gguf",
      "model_id": "Qwen2.5 Coder 7B Instruct Abliterated (Q5)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Qwen2.5-Omni-3B": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-Omni-3B-GGUF\\Qwen2.5-Omni-3B-Q8_0.gguf",
      "model_id": "Qwen2.5 Omni 3B",
      "mmproj": "F:\\llm\\llama\\models\\Qwen2.5-Omni-3B-GGUF\\mmproj-Qwen2.5-Omni-3B-f16.gguf",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen2.5-VL-7B-Instruct": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-VL-7B-Instruct-GGUF\\Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
      "model_id": "Qwen2.5 VL 7B Instruct (Q4)",
      "mmproj": "F:\\llm\\llama\\models\\Qwen2.5-VL-7B-Instruct-GGUF\\mmproj-F32.gguf",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 1.0,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "top_p": 1.0,
        "top_k": 0,
        "jinja": true
      }
    },
    "Qwen3-0.6B-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-0.6B-GGUF\\Qwen3-0.6B-Q4_K_S.gguf",
      "model_id": "Qwen3 0.6B Ultra Fast (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.2,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "jinja": true
      }
    },
    "Qwen3-0.6B-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-0.6B-GGUF\\Qwen3-0.6B-Q8_0.gguf",
      "model_id": "Qwen3 0.6B Ultra Fast (Q8)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.2,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "jinja": true
      }
    },
    "Qwen3-1.7B-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-1.7B-GGUF\\Qwen3-1.7B-Q4_K_S.gguf",
      "model_id": "Qwen3 1.7B Fast (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.2,
        "threads": 8,
        "mlock": true,
        "no_mmap": true
      }
    },
    "Qwen3-1.7B-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-1.7B-Q8_0-GGUF\\Qwen3-1.7B-Q8_0.gguf",
      "model_id": "Qwen3 1.7B Fast (Q8)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.2,
        "threads": 8,
        "mlock": true,
        "no_mmap": true
      }
    },
    "Qwen3-4B-Thinking-2507-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\\Qwen3-4B-Thinking-2507-Q4_K_S.gguf",
      "model_id": "Qwen3 4B Thinking (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_M-GGUF\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
      "model_id": "Qwen3 Coder 30B A3B Instruct (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Qwen3-Coder-30B-A3B-Instruct-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf",
      "model_id": "Qwen3 Coder 30B A3B Instruct (Q4-S)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Qwen3-VL-8B-Thinking.Q4_K": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-VL-8B-Thinking.Q4_K-GGUF\\Qwen3-VL-8B-Thinking.Q4_K.gguf",
      "model_id": "Qwen3 VL 8B Thinking (Q4)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 30,
        "jinja": true,
        "n_cpu_moe": 30,
        "no_warmup": "",
        "no_context_shift": ""
      }
    },
    "Qwen3-14B-128K-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-14B-128K-GGUF\\Qwen3-14B-128K-Q8_0.gguf",
      "model_id": "Qwen3 14B 128K (Q8)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 24000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true,
        "n_cpu_moe": 30,
        "no_warmup": true,
        "no_context_shift": true
      }
    },
    "gpt-oss-20b-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\gpt-oss-20b-GGUF\\gpt-oss-20b-Q4_K_S.gguf",
      "model_id": "GPT OSS 20B (Q4)",
      "llama_cpp_runtime": "default",
      "parameters": {
        "ctx_size": 32000,
        "temp": 1.0,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "top_p": 1.0,
        "top_k": 0,
        "jinja": true
      }
    },
    "gpt-oss-20b-MXFP4": {
      "model_path": "F:\\llm\\llama\\models\\gpt-oss-20b-MXFP4-GGUF\\gpt-oss-20b-MXFP4.gguf",
      "model_id": "GPT OSS 20B MXFP4",
      "llama_cpp_runtime": "default",
      "parameters": {
        "ctx_size": 32000,
        "temp": 1.0,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "top_p": 1.0,
        "top_k": 0,
        "jinja": true
      }
    },
    "Qwen3-14B-128K-UD-Q4_K_XL": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-14B-128K-GGUF\\Qwen3-14B-128K-UD-Q4_K_XL.gguf",
      "model_id": "Qwen3 14B 128K UD (Q4-XL)",
      "auto_update_model": true,
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true,
        "n_cpu_moe": 30
      }
    }
  }
}