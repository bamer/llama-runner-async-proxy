{
  "proxies": {
    "ollama": {
      "enabled": true,
      "port": 11434
    },
    "lmstudio": {
      "enabled": true,
      "port": 1234
    }
  },
  "llama-runtimes": {
    "llama-server": {
      "runtime": "F:\\llm\\llama\\llama-server.exe"
    }
  },
  "webui": {
    "enabled": true,
    "port": 8081,
    "host": "0.0.0.0"
  },
  "metrics": {
    "enabled": true,
    "port": 8080,
    "host": "0.0.0.0"
  },
  "audio": {
    "models": {
      "whisper-tiny": {
        "model_path": "tiny",
        "parameters": {
          "device": "cpu",
          "compute_type": "int8",
          "threads": 4,
          "language": null,
          "beam_size": 5
        }
      },
      "whisper-base": {
        "model_path": "base",
        "parameters": {
          "device": "cpu",
          "compute_type": "int8",
          "threads": 4,
          "language": null,
          "beam_size": 5
        }
      },
      "whisper-small": {
        "model_path": "small",
        "parameters": {
          "device": "cpu",
          "compute_type": "int8",
          "threads": 4,
          "language": null,
          "beam_size": 5
        }
      }
    }
  },
  "models": {
    "Chroma1-HD-Flash-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Chroma1-HD-Flash-Q4_K_S-GGUF\\Chroma1-HD-Flash-Q4_K_S.gguf",
      "mmproj": "F:\\llm\\llama\\models\\mmproj-F32.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "DeepSeek-R1-0528-Qwen3-8B-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M-GGUF\\DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Dorado-WebSurf_Tool-ext.Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Dorado-WebSurf_Tool-ext.Q8_0-GGUF\\Dorado-WebSurf_Tool-ext.Q8_0.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Qwen2.5-7B-Instruct-Q4_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-7B-Instruct-Q4_K_M-GGUF\\Qwen2.5-7B-Instruct-Q4_K_M.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen2.5-Coder-1.5B-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-Coder-1.5B-Q8_0-GGUF\\Qwen2.5-Coder-1.5B-Q8_0.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M-GGUF\\Qwen2.5-Coder-7B-Instruct-abliterated.Q5_K_M.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Qwen2.5-Omni-3B": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-Omni-3B-GGUF\\Qwen2.5-Omni-3B-Q8_0.gguf",
      "mmproj": "F:\\llm\\llama\\models\\mmproj-Qwen2.5-Omni-3B-f16.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "threads": 8,
        "mlock": true,
        "no_mmap": true,
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen2.5-VL-7B-Instruct": {
      "model_path": "F:\\llm\\llama\\models\\Qwen2.5-VL-7B-Instruct-GGUF\\Qwen2.5-VL-7B-Instruct-Q4_K_S.gguf",
      "mmproj": "F:\\llm\\llama\\models\\mmproj-F32.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 1.0,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "top_p": 1.0,
        "top_k": 0,
        "jinja": true
      }
    },
    "Qwen3-1.7B-Q8_0": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-1.7B-Q8_0-GGUF\\Qwen3-1.7B-Q8_0.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.2,
        "threads": 8,
        "mlock": true,
        "no_mmap": true
      }
    },
    "Qwen3-4B-Thinking-2507-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-4B-Thinking-2507-Q4_K_S-GGUF\\Qwen3-4B-Thinking-2507-Q4_K_S.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "jinja": true
      }
    },
    "Qwen3-Coder-30B-A3B-Instruct-Q4_K_S": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 85,
        "repeat_penalty": 1.05,
        "top_p": 0.8,
        "top_k": 20,
        "jinja": true
      }
    },
    "Qwen3-VL-8B-Thinking.Q4_K": {
      "model_path": "F:\\llm\\llama\\models\\Qwen3-VL-8B-Thinking.Q4_K-GGUF\\Qwen3-VL-8B-Thinking.Q4_K.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 1024,
        "ubatch_size": 512,
        "threads": 10,
        "mlock": true,
        "no_mmap": true,
        "flash_attn": "on",
        "n_gpu_layers": 30,
        "jinja": true,
        "n_cpu_moe": 30,
        "no-warmup": "",
        "no-context-shift": ""
      }
    }
  }
}