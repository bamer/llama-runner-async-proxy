# üìã Guide d'Installation - LlamaRunner Pro

**Version Phase 2 - Instructions claires et v√©rifi√©es**

## üéØ Pr√©requis Syst√®me

### Configuration minimale requise
- **Syst√®me d'exploitation** : Windows 10/11 ou Linux (Ubuntu 20.04+)
- **Processeur** : 4 c≈ìurs physiques minimum
- **M√©moire RAM** : 16 GB minimum (32 GB recommand√© pour les grands mod√®les)
- **Stockage** : 50 GB d'espace disque SSD (pour les mod√®les GGUF)
- **GPU** : NVIDIA avec 8+ GB VRAM (recommand√© pour l'inf√©rence acc√©l√©r√©e)

### Logiciels requis
- **Python 3.11+** ([T√©l√©charger Python](https://www.python.org/downloads/))
- **PowerShell 7+** ([T√©l√©charger PowerShell](https://github.com/PowerShell/PowerShell))
- **VS Code** (recommand√©) ([T√©l√©charger VS Code](https://code.visualstudio.com/))
- **Git** ([T√©l√©charger Git](https://git-scm.com/))

## üöÄ Installation Pas √† Pas

### 1. Cloner le d√©p√¥t
```powershell
git clone https://github.com/votre-repo/llama-runner-async-proxy.git
cd llama-runner-async-proxy
```

### 2. Configurer l'environnement Python

#### Option A : Virtualenv (recommand√©)
```powershell
# Cr√©er l'environnement virtuel
python -m venv dev-venv

# Activer l'environnement
.\dev-venv\Scripts\Activate.ps1

# Installer les d√©pendances
pip install -r requirements.txt
```

#### Option B : Anaconda
```powershell
# Cr√©er l'environnement conda
conda create -n llama python=3.11 -y

# Activer l'environnement
conda activate llama

# Installer les d√©pendances
pip install -r requirements.txt
```

### 3. V√©rifier l'installation Python
```powershell
python --version
# Devrait afficher : Python 3.11.x

pip list
# Devrait afficher toutes les d√©pendances install√©es
```

### 4. Configurer les dossiers de travail
Le projet cr√©era automatiquement les dossiers n√©cessaires :
- `config/` - Configuration
- `logs/` - Fichiers de log
- `models/` - Mod√®les GGUF
- `tools/` - Outils externes

### 5. T√©l√©charger les outils externes (optionnel mais recommand√©)

#### Pour Windows :
1. T√©l√©chargez `llama-server.exe` depuis [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases)
2. Placez-le dans le dossier `tools/`
3. Configurez le chemin dans `config/config.json`

#### Pour Linux :
```powershell
# Compiler llama.cpp (optionnel)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j$(nproc)
cp ./bin/server ../tools/llama-server
```

## üîß Configuration Initiale

### 1. G√©n√©rer la configuration par d√©faut
La premi√®re ex√©cution cr√©era une configuration par d√©faut :
```powershell
.\LaunchMenu.ps1
```

### 2. Configurer les mod√®les
√âditez le fichier `config/config.json` pour ajouter vos mod√®les :

```json
{
  "models": {
    "mon-modele-7b": {
      "model_path": "models/mon-modele-7b.Q4_K_M.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "n_gpu_layers": 45,
        "port": 8035
      }
    }
  },
  "default_model": "mon-modele-7b"
}
```

### 3. Configurer les proxies
Dans `config/config.json`, configurez les ports des proxies :

```json
{
  "proxies": {
    "ollama": {
      "enabled": true,
      "port": 11434
    },
    "lmstudio": {
      "enabled": true,
      "port": 1234
    }
  }
}
```

## üß™ Validation de l'Installation

### 1. Ex√©cuter le script de validation
```powershell
.\scripts\validate_system.ps1
```

### 2. R√©sultats attendus
‚úÖ **Succ√®s** :
```
üéâ VALIDATION R√âUSSIE - PHASE 2 !
‚úÖ Le syst√®me est pr√™t √† d√©marrer avec la nouvelle structure.
üöÄ Utilisez: ..\LaunchMenu.ps1 pour d√©marrer l'application.
```

‚ùå **√âchec** - Probl√®mes courants et solutions :

#### Probl√®me : Environnement Python non trouv√©
```powershell
‚ùå Python non trouv√©: F:\llm\llama-runner-async-proxy\dev-venv\Scripts\python.exe
```
**Solution** :
```powershell
# Recr√©er l'environnement virtuel
python -m venv dev-venv
.\dev-venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

#### Probl√®me : Modules Python manquants
```powershell
‚ùå Modules manquants: psutil, websockets, fastapi
```
**Solution** :
```powershell
pip install psutil websockets fastapi uvicorn
```

#### Probl√®me : Ports occup√©s
```powershell
‚ö†Ô∏è  Port 1234 d√©j√† utilis√©
```
**Solution** :
```powershell
# Configurer de nouveaux ports
.\scripts\port_config.ps1
```

## üéÆ D√©marrage Initial

### 1. Lancer le menu interactif
```powershell
.\LaunchMenu.ps1
```

### 2. Choisir le mode de d√©marrage
Pour la premi√®re utilisation, choisissez **"üöÄ Mode Proxy (Serveur principal)"**

### 3. V√©rifier les logs
Les logs sont √©crits dans :
- `logs/app.log` - Logs de l'application
- `logs/launch_menu.log` - Logs du menu de lancement

## ‚öôÔ∏è Configuration Avanc√©e

### Configuration GPU
Pour les utilisateurs NVIDIA :
```json
{
  "models": {
    "mon-modele": {
      "parameters": {
        "n_gpu_layers": 45,
        "cuda_visible_devices": "0"
      }
    }
  }
}
```

### Configuration multi-mod√®les
```json
{
  "concurrentRunners": 2,
  "models": {
    "modele-a": { ... },
    "modele-b": { ... }
  }
}
```

### Configuration audio (Whisper)
```json
{
  "audio": {
    "models": {
      "whisper-large-v3": {
        "model_path": "models/whisper-large-v3",
        "parameters": {
          "device": "cuda",
          "compute_type": "float16"
        }
      }
    }
  }
}
```

## üõ† D√©pannage Courant

### Probl√®me : Import error apr√®s installation
**Erreur** :
```python
ImportError: cannot import name 'calculate_system_fingerprint' from 'llama_runner.config_loader'
```
**Solution** : Cela a √©t√© corrig√© dans la Phase 2. Assurez-vous d'utiliser la derni√®re version du code.

### Probl√®me : D√©marrage du serveur √©choue
**Solution** :
1. V√©rifiez les logs dans `logs/app.log`
2. Ex√©cutez la validation : `.\scripts\validate_system.ps1`
3. Essayez le mode debug : `.\LaunchMenu.ps1` ‚Üí "üîß Mode D√©veloppement"

### Probl√®me : Mod√®les non d√©tect√©s
**Solution** :
1. V√©rifiez que les chemins dans `config.json` sont corrects
2. Utilisez des chemins relatifs : `models/mon-modele.gguf` au lieu de `F:\llm\...\mon-modele.gguf`
3. Red√©marrez l'application apr√®s modification de la configuration

## üîÑ Mise √† Jour

Pour mettre √† jour le projet :
```powershell
# Sauvegarder votre configuration
copy config\config.json config\config_backup.json

# Mettre √† jour le code
git pull

# Mettre √† jour les d√©pendances
pip install -r requirements.txt --upgrade

# Relancer la validation
.\scripts\validate_system.ps1
```

## üìû Support

Si vous rencontrez des probl√®mes persistants :
- V√©rifiez les [issues GitHub](https://github.com/votre-repo/llama-runner-async-proxy/issues)
- Cr√©ez une nouvelle issue avec :
  - Votre syst√®me d'exploitation
  - La version de Python
  - Le contenu de `logs/app.log`
  - Les √©tapes pour reproduire le probl√®me

---

**‚úÖ Installation r√©ussie !**  
Vous √™tes maintenant pr√™t √† utiliser LlamaRunner Pro avec la structure Phase 2 stabilis√©e.