# ğŸ“– Guide d'Utilisation - LlamaRunner Pro

**Version Phase 2 - Instructions claires et modes d'emploi**

## ğŸ¯ Modes de Fonctionnement Disponibles

LlamaRunner Pro propose plusieurs modes d'opÃ©ration accessibles via le menu interactif :

### ğŸš€ Mode Proxy (Serveur principal)
- **Description** : Proxy asynchrone pour tous les modÃ¨les d'IA
- **Ports** : 1234 (LM Studio API), 11434 (Ollama API)
- **Commande** : `.\LaunchMenu.ps1` â†’ Choisir "ğŸš€ Mode Proxy (Serveur principal)"
- **Cas d'usage** : Utilisation comme interface unifiÃ©e pour multiple backends d'IA

### ğŸ¦™ Mode Llama.cpp seul
- **Description** : DÃ©marrage direct du serveur llama.cpp avec un modÃ¨le spÃ©cifique
- **Ports** : 8035 (serveur llama.cpp)
- **Commande** : `.\LaunchMenu.ps1` â†’ Choisir "ğŸ¦™ Mode Llama.cpp seul"
- **Cas d'usage** : Tests de performance d'un modÃ¨le unique, dÃ©veloppement

### ğŸŒ Mode Proxy + WebUI
- **Description** : Proxy avec interface web pour le contrÃ´le des modÃ¨les
- **Ports** : 1234, 11434 + 8081 (interface web)
- **Commande** : `.\LaunchMenu.ps1` â†’ Choisir "ğŸŒ Mode Proxy + WebUI"
- **Cas d'usage** : Utilisation interactive via navigateur web

### ğŸ“Š Mode Proxy + WebUI + Dashboard
- **Description** : Proxy complet avec monitoring temps rÃ©el des mÃ©triques systÃ¨me
- **Ports** : 1234, 11434 + 8081 (web UI) + 8035 (dashboard)
- **Commande** : `.\LaunchMenu.ps1` â†’ Choisir "ğŸ“Š Mode Proxy + WebUI + Dashboard"
- **Cas d'usage** : Monitoring production, visualisation des performances

### ğŸ”§ Mode DÃ©veloppement (Debug)
- **Description** : Mode avec logs dÃ©taillÃ©s et debugging activÃ©
- **Ports** : Tous les ports activÃ©s
- **Commande** : `.\LaunchMenu.ps1` â†’ Choisir "ğŸ”§ Mode DÃ©veloppement (Debug)"
- **Cas d'usage** : DÃ©veloppement, debugging, tests avancÃ©s

## ğŸ–¥ Interface Utilisateur

### Menu Interactif (LaunchMenu.ps1)
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    ğŸ¦™ MENU INTERACTIF - PHASE 2 STABLE    â•‘
â•‘      Structure corrigÃ©e et simplifiÃ©e     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Ã‰TAT ACTUEL DU PROJET :
   ğŸ“ RÃ©pertoire projet: F:\llm\llama-runner-async-proxy
   ğŸ Python: .\dev-venv\Scripts\python.exe

  > ğŸš€ Mode Proxy (Serveur principal)
    ğŸ¦™ Mode Llama.cpp seul
    ğŸŒ Mode Proxy + WebUI
    ğŸ“Š Mode Proxy + WebUI + Dashboard
    ğŸ”§ Mode DÃ©veloppement (Debug)
    ğŸ§ª Tests du systÃ¨me
    ğŸ“¦ Installation des dÃ©pendances
    âš™ï¸  Configuration des ports
    ğŸ” Validation complÃ¨te du systÃ¨me
    ğŸ¤– Gestion des modÃ¨les
    ğŸ”„ Mise Ã  jour config
    âŒ Quitter
```

**Commandes de navigation** :
- **FlÃ¨ches haut/bas** : SÃ©lectionner une option
- **EntrÃ©e** : ExÃ©cuter l'option sÃ©lectionnÃ©e
- **Ã‰chap** : Quitter l'application

### Interface Web (Mode WebUI)
AccÃ©dez Ã  l'interface web via : `http://localhost:8081`

**FonctionnalitÃ©s disponibles** :
- ğŸ“‹ Liste des modÃ¨les disponibles
- â–¶ï¸ DÃ©marrage/arrÃªt des modÃ¨les
- âš™ï¸ Configuration des paramÃ¨tres
- ğŸ“Š Visualisation des mÃ©triques basiques
- ğŸ”§ Gestion des ports et configuration

### Tableau de Bord (Mode Dashboard)
AccÃ©dez au dashboard via : `http://localhost:8035`

**MÃ©triques affichÃ©es en temps rÃ©el** :
- ğŸ“ˆ Utilisation CPU (%)
- ğŸ’¾ Utilisation mÃ©moire (GB)
- ğŸ® Utilisation GPU (% et mÃ©moire)
- âš¡ Latence des requÃªtes (ms)
- ğŸ”„ Taux de requÃªtes par seconde
- ğŸ“Š Ã‰tat des modÃ¨les (dÃ©marrÃ©/arrÃªtÃ©)

## ğŸ¤– Gestion des ModÃ¨les

### Via le menu interactif
```powershell
.\LaunchMenu.ps1 â†’ ğŸ¤– Gestion des modÃ¨les
```

**OpÃ©rations disponibles** :
- ğŸ” **DÃ©couverte automatique** : Scan du dossier `models/` pour nouveaux modÃ¨les
- ğŸ“¥ **Import manuel** : Ajout d'un modÃ¨le existant
- ğŸ—‘ï¸ **Suppression** : Retrait d'un modÃ¨le de la configuration
- ğŸ”„ **Mise Ã  jour** : Actualisation des mÃ©tadonnÃ©es d'un modÃ¨le
- âš™ï¸ **Configuration** : Modification des paramÃ¨tres d'un modÃ¨le

### Manuellement via configuration
Ã‰ditez le fichier `config/config.json` :

```json
{
  "models": {
    "mon-nouveau-modele": {
      "model_path": "models/mon-nouveau-modele.Q4_K_M.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 32000,
        "temp": 0.7,
        "batch_size": 512,
        "n_gpu_layers": 45,
        "port": 8036  // Port unique par modÃ¨le
      },
      "display_name": "Mon Nouveau ModÃ¨le",
      "auto_discovered": false
    }
  },
  "default_model": "mon-nouveau-modele"
}
```

**AprÃ¨s modification** : RedÃ©marrez l'application pour appliquer les changements.

## ğŸ”Œ API et IntÃ©grations

### LM Studio Compatible API
**Endpoint** : `http://localhost:1234`

**Exemple cURL** :
```bash
curl http://localhost:1234/api/v0/models
curl http://localhost:1234/api/v0/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mon-modele",
    "messages": [{"role": "user", "content": "Bonjour!"}],
    "stream": false
  }'
```

### Ollama Compatible API
**Endpoint** : `http://localhost:11434`

**Exemple cURL** :
```bash
curl http://localhost:11434/api/tags
curl http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mon-modele",
    "prompt": "Bonjour!",
    "stream": false
  }'
```

### OpenAI Compatible API
**Endpoint** : `http://localhost:1234/v1`

**Exemple cURL** :
```bash
curl http://localhost:1234/v1/models
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mon-modele",
    "messages": [{"role": "user", "content": "Bonjour!"}]
  }'
```

## ğŸµ FonctionnalitÃ©s Audio (Whisper)

### Transcription audio
**Endpoint** : `http://localhost:1234/v1/audio/transcriptions`

**Exemple Python** :
```python
import requests

url = "http://localhost:1234/v1/audio/transcriptions"
files = {"file": open("audio.wav", "rb")}
data = {"model": "whisper-tiny"}

response = requests.post(url, files=files, data=data)
print(response.json())
```

### Traduction audio
**Endpoint** : `http://localhost:1234/v1/audio/translations`

**Exemple Python** :
```python
url = "http://localhost:1234/v1/audio/translations"
files = {"file": open("audio.wav", "rb")}
data = {"model": "whisper-tiny", "language": "en"}

response = requests.post(url, files=files, data=data)
print(response.json())
```

## ğŸ“Š Monitoring et Logs

### Fichiers de logs
- `logs/app.log` : Logs principaux de l'application
- `logs/launch_menu.log` : Logs du menu de lancement
- `logs/validation.log` : Logs de validation systÃ¨me
- `logs/model_management.log` : Logs de gestion des modÃ¨les

### Format des logs
```
[2025-11-09 18:58:24] [INFO] DÃ©marrage du proxy LM Studio sur le port 1234
[2025-11-09 18:58:25] [DEBUG] ModÃ¨le 'JanusCoderV-7B.i1-Q4_K_S' chargÃ© avec succÃ¨s
[2025-11-09 18:58:30] [WARNING] Port 8035 dÃ©jÃ  utilisÃ©, utilisation du port alternatif 8036
[2025-11-09 18:58:35] [ERROR] Erreur de dÃ©marrage du runner pour le modÃ¨le 'autre-modele'
```

### Surveillance systÃ¨me
Utilisez la commande PowerShell pour surveiller les ressources :
```powershell
# Surveillance CPU et mÃ©moire
Get-Process python | Select-Object CPU, WS, PM, VM

# Surveillance des ports
Get-NetTCPConnection -LocalPort @(1234,11434,8035,8081) | Select-Object LocalPort, State
```

## ğŸ§ª Tests et Validation

### ExÃ©cuter les tests unitaires
```powershell
pytest tests/
```

### ExÃ©cuter les tests d'intÃ©gration
```powershell
pytest tests/integration/
```

### Validation systÃ¨me complÃ¨te
```powershell
.\scripts\validate_system.ps1
```

## âš™ï¸ Maintenance et Mise Ã  Jour

### Sauvegarde de configuration
```powershell
# Sauvegarde manuelle
copy config\config.json config\config_backup_$(Get-Date -Format "yyyyMMdd_HHmmss").json
```

### Mise Ã  jour des dÃ©pendances
```powershell
pip install -r requirements.txt --upgrade
```

### Nettoyage des caches
```powershell
# Nettoyer les caches Python
rm -rf __pycache__
rm -rf *.pyc

# Nettoyer les logs anciens
Get-ChildItem logs\*.log -Recurse | Where-Object {$_.LastWriteTime -lt (Get-Date).AddDays(-7)} | Remove-Item
```

## ğŸš¨ ProblÃ¨mes Courants et Solutions

### Erreur au dÃ©marrage
**SymptÃ´me** : L'application ne dÃ©marre pas, logs vides
**Solutions** :
1. VÃ©rifiez l'environnement Python : `.\dev-venv\Scripts\python.exe --version`
2. ExÃ©cutez la validation : `.\scripts\validate_system.ps1`
3. Essayez le mode debug : `.\LaunchMenu.ps1` â†’ "ğŸ”§ Mode DÃ©veloppement"

### ModÃ¨le ne dÃ©marre pas
**SymptÃ´me** : Timeout au dÃ©marrage du modÃ¨le
**Solutions** :
1. VÃ©rifiez le chemin du fichier GGUF dans `config.json`
2. Assurez-vous que le runtime (`llama-server.exe`) est dans `tools/`
3. RÃ©duisez `n_gpu_layers` si vous n'avez pas assez de VRAM
4. Augmentez le timeout dans le code source (si nÃ©cessaire)

### ProblÃ¨mes de performance
**SymptÃ´me** : RÃ©ponses lentes, latence Ã©levÃ©e
**Solutions** :
1. RÃ©duisez `ctx_size` pour les modÃ¨les trop gros
2. Ajustez `batch_size` et `ubatch_size`
3. VÃ©rifiez l'utilisation GPU avec `nvidia-smi`
4. Fermez d'autres applications consommatrices de ressources

### ProblÃ¨mes de mÃ©moire
**SymptÃ´me** : Erreurs de mÃ©moire insuffisante
**Solutions** :
1. Utilisez des modÃ¨les quantifiÃ©s (Q4_K_M au lieu de Q8_0)
2. RÃ©duisez `ctx_size`
3. Fermez d'autres modÃ¨les en cours d'exÃ©cution
4. Ajoutez de la mÃ©moire SWAP si nÃ©cessaire

## ğŸ“ Support et CommunautÃ©

### Canaux de support
- **Issues GitHub** : Pour les bugs et fonctionnalitÃ©s
- **Discussions** : Pour les questions et aide
- **Documentation** : Toujours la premiÃ¨re rÃ©fÃ©rence

### Contribuer
1. Fork le dÃ©pÃ´t
2. CrÃ©ez une branche pour votre fonctionnalitÃ©
3. Soumettez une pull request
4. Suivez les conventions de code et les tests

### Rapporter un bug
Fournissez toujours :
- Votre environnement (OS, Python version)
- Les Ã©tapes pour reproduire
- Le contenu des logs pertinents
- Le rÃ©sultat attendu vs le rÃ©sultat rÃ©el

---

**âœ… Vous Ãªtes prÃªt !**  
Avec ce guide, vous devriez Ãªtre en mesure d'utiliser pleinement LlamaRunner Pro dans tous ses modes. N'hÃ©sitez pas Ã  consulter les autres documents de documentation pour des informations plus dÃ©taillÃ©es.