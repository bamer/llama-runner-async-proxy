{
    "name": "llama-runner-async-proxy",
    "version": "1.0.0",
    "description": "Node.js async proxy for Llama models",
    "main": "backend/src/server.js",
    "scripts": {
        "postinstall": "npm --prefix frontend install",
        "start": "npm --prefix frontend install && npm --prefix frontend run build && node backend/src/server.js",
        "dev": "nodemon backend/src/server.js",
        "start:prod": "npm --prefix frontend install && npm --prefix frontend run build && node backend/src/server.js",
        "testall": "echo \"Error: no test specified\" && exit 1"
    },
    "dependencies": {
        "axios": "^1.6.0",
        "chart.js": "^4.5.1",
        "cors": "^2.8.5",
        "express": "^4.18.2",
        "node-fetch": "^3.3.2",
        "react-chartjs-2": "^5.3.1"
    },
    "devDependencies": {
        "nodemon": "^3.0.1"
    }
}