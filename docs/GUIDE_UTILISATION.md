# ğŸš€ Guide d'utilisation - LlamaRunner Pro

## ğŸ“‹ Vue d'ensemble

Le script `Launch-LlamaRunner.ps1` est le lanceur principal de LlamaRunner Pro. Il offre plusieurs modes de lancement avec une interface interactive intuitive et des options en ligne de commande.

---

## ğŸ® Mode Interactif (RecommandÃ©)

### Lancement sans paramÃ¨tre

```powershell
.\Launch-LlamaRunner.ps1
```

Cela affiche un menu interactif avec les options suivantes :

1. **ğŸƒâ€â™‚ï¸ Proxy uniquement** - Lance le proxy (LM Studio + Ollama)
2. **ğŸŒ Proxy + Interface Web** - Lance le proxy avec l'interface web
3. **ğŸ“Š Proxy + Web UI + Dashboard MÃ©triques** - Mode complet avec monitoring
4. **ğŸ”§ Mode DÃ©veloppement** - Avec logs dÃ©taillÃ©s pour le debugging
5. **ğŸ–¥ï¸ Mode Headless** - Serveur sans interface graphique
6. **ğŸ§ª Lancer les tests** - Tests de validation du systÃ¨me
7. **ğŸ“¦ Installer les dÃ©pendances** - Installation/mise Ã  jour Python
8. **âŒ Quitter** - Sortie du programme

---

## âš¡ Lancement en ligne de commande

### Options principales

```powershell
# Installation des dÃ©pendances
.\Launch-LlamaRunner.ps1 -Install

# Lancement du proxy uniquement
.\Launch-LlamaRunner.ps1 -Proxy

# Proxy + interface web
.\Launch-LlamaRunner.ps1 -WebUI

# Mode complet avec dashboard mÃ©triques
.\Launch-LlamaRunner.ps1 -Metrics

# Mode dÃ©veloppement avec logs dÃ©taillÃ©s
.\Launch-LlamaRunner.ps1 -Dev

# Mode headless (serveur sans GUI)
.\Launch-LlamaRunner.ps1 -Headless

# Tests de validation
.\Launch-LlamaRunner.ps1 -Test
```

### Options de configuration

```powershell
# SpÃ©cifier un fichier de configuration
.\Launch-LlamaRunner.ps1 -Proxy -Config "ma_config.json"

# Niveau de log personnalisÃ©
.\Launch-LlamaRunner.ps1 -Proxy -LogLevel "DEBUG"

# Ports personnalisÃ©s
.\Launch-LlamaRunner.ps1 -Metrics -MetricsPort 8080 -LmStudioPort 1235

# Mode combine
.\Launch-LlamaRunner.ps1 -Metrics -Headless -LogLevel "WARNING"
```

---

## ğŸ“Š Modes de fonctionnement dÃ©taillÃ©s

### 1. Proxy uniquement (`-Proxy`)

- **LM Studio API** : <http://localhost:1234>
- **Ollama API** : <http://localhost:11434>
- **Usage** : IntÃ©gration avec IDEs et outils externes

### 2. Proxy + Interface Web (`-WebUI`)

- **Dashboard Web** : <http://localhost:8081>
- **LM Studio API** : <http://localhost:1234>
- **Ollama API** : <http://localhost:11434>
- **Usage** : Interface utilisateur graphique complÃ¨te

### 3. Mode Complet (`-Metrics`)

- **Dashboard MÃ©triques** : <http://localhost:8080> â­
- **Interface Web** : <http://localhost:8081>
- **LM Studio API** : <http://localhost:1234>
- **Ollama API** : <http://localhost:11434>
- **Usage** : Monitoring en temps rÃ©el + interface complÃ¨te

### 4. Mode DÃ©veloppement (`-Dev`)

- Logs dÃ©taillÃ©s activÃ©s (niveau DEBUG)
- **LM Studio API** : <http://localhost:1234>
- **Ollama API** : <http://localhost:11434>
- **Usage** : DÃ©bogage et dÃ©veloppement

### 5. Mode Headless (`-Headless`)

- Serveur sans interface graphique
- **LM Studio API** : <http://localhost:1234>
- **Ollama API** : <http://localhost:11434>
- **Usage** : Serveurs, Docker, CI/CD

---

## ğŸ“ˆ Dashboard de monitoring en temps rÃ©el

Le mode `-Metrics` active le dashboard de monitoring accessible sur **<http://localhost:8585>**

### FonctionnalitÃ©s du dashboard

- ğŸ“Š **Graphiques en temps rÃ©el** : CPU, mÃ©moire, disque, rÃ©seau
- ğŸ›¡ï¸ **Circuit Breaker Status** : Ã‰tat des protections de rÃ©silience
- ğŸš¨ **Alertes automatiques** : Notifications de performance
- ğŸ“ˆ **MÃ©triques des modÃ¨les** : Temps de rÃ©ponse, taux d'erreur
- ğŸ“¡ **MÃ©triques API** : Endpoints, statistiques d'utilisation

---

## âš™ï¸ Configuration avancÃ©e

### Variables d'environnement supportÃ©es

- `LLAMA_RUNNER_CONFIG` : Fichier de configuration par dÃ©faut
- `LLAMA_RUNNER_LOG_LEVEL` : Niveau de log global
- `LLAMA_RUNNER_METRICS_PORT` : Port du dashboard (dÃ©faut: 8585)

### Fichiers de configuration

- `config.json` : Configuration principale
- `config_prefilled.json` : Configuration prÃ©-remplie
- `config_prefilled_enhanced.jsonc` : Configuration avancÃ©e

---

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes courants

**1. Erreur "Python non trouvÃ©"**

```powershell
.\Launch-LlamaRunner.ps1 -Install
```

**2. Modules manquants**

```powershell
# RÃ©installer les dÃ©pendances
.\Launch-LlamaRunner.ps1 -Install
```

**3. Port dÃ©jÃ  utilisÃ©**

```powershell
# Utiliser des ports diffÃ©rents
.\Launch-LlamaRunner.ps1 -Metrics -MetricsPort 8080
```

**4. ProblÃ¨mes de performance**

```powershell
# Mode dÃ©veloppement pour diagnostic
.\Launch-LlamaRunner.ps1 -Dev
```

### Logs de diagnostic

- Logs dans la console en temps rÃ©el
- Niveau DEBUG pour dÃ©veloppement
- Circuit breaker stats dans le dashboard

---

## ğŸ¯ Cas d'usage recommandÃ©s

### Pour les dÃ©veloppeurs

```powershell
# Mode dÃ©veloppement avec logs
.\Launch-LlamaRunner.ps1 -Dev
```

### Pour la production (serveurs)

```powershell
# Mode headless avec monitoring
.\Launch-LlamaRunner.ps1 -Metrics -Headless
```

### Pour les tests

```powershell
# Tests de validation
.\Launch-LlamaRunner.ps1 -Test
```

### Pour l'utilisation quotidienne

```powershell
# Mode complet avec interface
.\Launch-LlamaRunner.ps1 -WebUI
```

---

## ğŸ’¡ Tips et astuces

1. **DÃ©marrage rapide** : Utilisez le mode interactif pour explorer les options
2. **Monitoring continu** : Le mode `-Metrics` offre le meilleur contrÃ´le
3. **DÃ©veloppement** : Le mode `-Dev` facilite le dÃ©bogage
4. **Production** : Combinez `-Metrics -Headless` pour un serveur robuste
5. **Tests** : Lancez `-Test` rÃ©guliÃ¨rement pour valider le systÃ¨me

---

## ğŸ”’ SÃ©curitÃ© et bonnes pratiques

- âš¡ **Ports** : Les ports par dÃ©faut (1234, 11434, 8585) sont sÃ©curisÃ©s
- ğŸ›¡ï¸ **Circuit Breaker** : Protection automatique contre les dÃ©faillances
- ğŸ“Š **Monitoring** : Surveillance continue des performances
- ğŸ” **Local** : Toutes les donnÃ©es restent locales par dÃ©faut

Le script gÃ¨re automatiquement l'arrÃªt propre de tous les services avec Ctrl+C.
