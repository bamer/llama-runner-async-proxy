# ğŸ§­ Technical & Structural Overview â€” *LlamaRunner Pro / Async Proxy System*

> **Purpose:** A complete guide for new developers joining the project, explaining the technical framework, file organization, and coding standards. Updated to reflect the latest project configuration and environment.

---

## âš™ï¸ 1. Project Overview

**Project Name:** `LlamaRunner Pro â€“ Async Proxy System`  
**Goal:** Provide a unified interface between various AI models (LM Studio, Ollama, etc.) through an **asynchronous Python proxy**, paired with a **Vue.js web dashboard** and real-time monitoring tools.

The system runs on **Windows and Linux** platforms, designed for **autonomous, modular, and extensible** operation.

---

## ğŸ§© 2. Technology Stack

### ğŸ–¥ Backend
| Component | Technology | Role |
|------------|-------------|------|
| **Language** | Python 3.11+ | Proxy, orchestration, monitoring |
| **Web Framework** | FastAPI / uvicorn | API and asynchronous request handling |
| **Realtime Layer** | WebSocket | Live communication |
| **GUI Framework** | PySide6 | Optional desktop interface |
| **System Monitoring** | psutil | CPU, memory, and GPU metrics collection |
| **Packaging / Build** | PyInstaller | Executable generation |

### ğŸŒ Frontend / Dashboard
| Component | Technology | Role |
|------------|-------------|------|
| **Framework** | Vue.js 3 | Web dashboard |
| **UI Library** | Element Plus | UI components |
| **Charts** | Chart.js | Real-time monitoring visualization |
| **Build Tool** | Vite | Compilation and bundling |
| **Styles** | SCSS | Advanced styling |

### ğŸ§± Development Tools
- **PowerShell 7+** for the launch menu (`LaunchMenu.ps1`)
- **VS Code** as the primary IDE  
- **pytest** and **unittest** for testing  
- **Git** for version control  
- **Linters / Formatters:** black, flake8, eslint  
- **Environments:** Virtualenv (`venv`) or **Anaconda** (no Docker)

---

## ğŸ—‚ 3. Project File Structure

```
llama-runner-async-proxy/
â”œâ”€â”€ LaunchMenu.ps1               # Main interactive menu must stay there and only script
â”œâ”€â”€ main.py                      # Python entry point
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ app_config.json          # General aplication configuration
â”‚   â”œâ”€â”€ models_config.json       # list of models and Model-specific settings
â”œâ”€â”€ logs/                        # Rotating logs all log related files must go there
â”œâ”€â”€ scripts/                     # PowerShell tools
â”‚   â”œâ”€â”€ model_management.ps1     # Model (.gguf) management
â”‚   â”œâ”€â”€ Validate-System.ps1      # System validation
â”‚   â”œâ”€â”€ PortConfig.ps1           # Network configuration
â”‚   â””â”€â”€ Debug-Launch.ps1         # Debug launch mode
â”œâ”€â”€ tests/                       # Unit & integration tests
â”‚   â”œâ”€â”€ test_implementation_validation.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_config_updater.py
â”‚   â”‚   â”œâ”€â”€ test_llama_runner_manager.py
â”‚   â”‚   â””â”€â”€ test_metrics_validation.py
â”œâ”€â”€ docs/                        # Documentation (formerly documentation/)
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ INSTALLATION.md
â”‚   â””â”€â”€ USAGE.md
â”œâ”€â”€ dashboard/                   # Vue.js + Chart.js frontend
â”œâ”€â”€ tests/                       # All tests related files must be there
â””â”€â”€ llama_runner/                # Core Python backend
```

the llama-server path is "F:\llm\llama\llama-server.exe" and must never change
The directory containing all llm models is "F:\llm\llama\models" each model are in a subdirectory Ex:
model name : JanusCoderV-7B.i1-Q4_K_S.gguf is in directory : 
"F:\llm\llama\models\JanusCoderV-7B-i1-GGUF\JanusCoderV-7B.i1-Q4_K_S.gguf"
other exemple :
neutss-air-BF16.gguf
"F:\llm\llama\models\neutts-air\neutss-air-BF16.gguf"
---

## ğŸ§  4. Code Structure & Modules

### `llama_runner/` (Backend Core)
- **`main.py`** â€” main FastAPI server entry point.
- **`proxy_manager.py`** â€” coordinates LM Studio, Ollama, and other local models.
- **`config_loader.py`** â€” reads and validates configuration files from `/config`.
- **`metrics_collector.py`** â€” collects system metrics via psutil.
- **`websocket_manager.py`** â€” sends live data to the dashboard.
- **`runner_manager.py`** â€” handles subprocesses and inter-process communication.

---

## ğŸ§¾ 5. Coding Standards (from `code_conventions.md`)

### Style & Typing
- All functions must include **type hints**.
- Strict **snake_case / PascalCase** naming conventions.
- Enforce **UTF-8** encoding across I/O, logs, and configs.
- Log both to **console and file**.
- Non-critical issues â†’ `warning`, critical issues â†’ `exception`.

### Structure
- One class or concept per Python file.
- Loaders, runners, and proxies are logically separated.
- Centralized validation via `config_loader.py`.
- separation of concerns is a priority and mandatory way of 
---


## ğŸ” 6. Security & Configuration

- Sensitive files have restricted permissions.
- **Standard ports:**
  | Service | Port |
  |----------|------|
  | LM Studio API | 1234 |
  | Ollama API | 11434 |
  | Dashboard Web | 8035 |

- Deployment is local only, with **venv or Anaconda**, no Docker.

---

## ğŸ§­ 7. Key Principles for Contributors

1. **Never delete files without prior analysis.**  
2. **Use atomic, meaningful commits.**  
3. **Never ignore high-severity diagnostics.**  
4. **Document every functional change.**  
5. **Follow type safety and conventions 100%.**  
6. **Run all tests locally before committing.**

---


---

### âœ… End of Document

