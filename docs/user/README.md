# ğŸ¦™ LlamaRunner Pro - Proxy IA Asynchrone

## ğŸ¯ **PrÃ©sentation**

LlamaRunner Pro est un proxy asynchrone avancÃ© pour modÃ¨les de langage IA, offrant une interface unifiÃ©e pour LM Studio, Ollama et d'autres services. ConÃ§u pour Ãªtre portable, sÃ©curisÃ© et Ã©volutif.

## ğŸš€ **FonctionnalitÃ©s Principales**

- **ğŸ”„ Proxy Multi-Plateforme** : Support natif LM Studio (port 1234) et Ollama (port 11434)
- **ğŸŒ Interface Web** : Dashboard interactif sur port 8081
- **ğŸ“Š Monitoring Temps RÃ©el** : MÃ©triques sur port 8080
- **ğŸ¤– Gestion Intelligente des ModÃ¨les** : Scan et configuration automatique des modÃ¨les GGUF
- **ğŸ”§ Mode DÃ©veloppement** : Logs dÃ©taillÃ©s et outils de debugging
- **âš¡ Performance OptimisÃ©e** : Support GPU, gestion mÃ©moire avancÃ©e
- **ğŸ”’ SÃ©curitÃ© RenforcÃ©e** : Isolation des processus, droits restreints

## ğŸ“¦ **Architecture du Projet**

```
llama-runner-async-proxy/
â”œâ”€â”€ LaunchMenu.ps1               # Point d'entrÃ©e unique (menu interactif)
â”œâ”€â”€ main.py                      # Application principale
â”œâ”€â”€ config.json                  # Configuration principale
â”œâ”€â”€ logs/                        # Dossiers des logs
â”œâ”€â”€ config/                      # Fichiers de configuration
â”œâ”€â”€ scripts/                     # Scripts utilitaires
â”‚   â”œâ”€â”€ model_management.ps1     # : Gestion robuste des modÃ¨les
â”‚   â”œâ”€â”€ Validate-System.ps1      # Validation complÃ¨te
â”‚   â”œâ”€â”€ PortConfig.ps1           # Configuration des ports
â”‚   â””â”€â”€ Debug-Launch.ps1         # Mode debug avancÃ©
â”œâ”€â”€ tests/                       # Tests unitaires et d'intÃ©gration
â”‚   â””â”€â”€ test_implementation_validation.py  # âœ… Mis Ã  jour
â”œâ”€â”€ documentation/               # Documentation complÃ¨te
â”‚   â”œâ”€â”€ README.md                # âœ… Ce fichier
â”‚   â”œâ”€â”€ INSTALLATION.md          # Guide d'installation
â”‚   â””â”€â”€ USAGE.md                 # Guide d'utilisation
â”œâ”€â”€â”€dashborad /                  # Dashboard avec graph et monitoring temps reel vu.js
â””â”€â”€ llama_runner/                # Code source Python
```

## **ğŸ”§ Cross-Platform Technical Stack:**

```
ğŸŒ Frontend (Cross-Platform):
â”œâ”€â”€ Vue.js 3 (JavaScript - Universal)
â”œâ”€â”€ Element Plus (React-based components - Universal)  
â”œâ”€â”€ Chart.js (Universal charting)
â”œâ”€â”€ Vite (Universal build tool)
â””â”€â”€ SCSS (Universal styling)

ğŸ’» Backend (Cross-Platform):
â”œâ”€â”€ Python 3.11+ (Universal)
â”œâ”€â”€ PySide6 (Universal GUI framework)
â”œâ”€â”€ FastAPI/uvicorn (Universal web server)
â”œâ”€â”€ WebSocket (Universal real-time)
â””â”€â”€ PSUtil (Universal system monitoring)

ğŸ³ Deployment (Cross-Platform):
â”œâ”€â”€ Docker (Universal containerization)
â”œâ”€â”€ Electron (Universal desktop app)
â”œâ”€â”€ PyInstaller (Universal executable)
â””â”€â”€ pip (Universal package manager)


## âš™ï¸ **Configuration par DÃ©faut (Ports Standards)**

| Service | Port | URL |
|---------|------|-----|
| **LM Studio API** | 1234 | http://localhost:1234 |
| **Ollama API** | 11434 | http://localhost:11434 |
| ****Dashboard Interface Web** | 8035 | http://localhost:8035 |

## ğŸš€ **DÃ©marrage Rapide**

### 1. **PrÃ©requis**
- Python 3.11+
- PowerShell 7+
- AccÃ¨s Ã  `F:\llm\llama\llama-server.exe`

### 2. **Premier dÃ©marrage**
```powershell
.\LaunchMenu.ps1 
```

### 3. **Configuration des ModÃ¨les (OPTIONNEL mais recommandÃ©)**
```powershell
# Dans le menu, sÃ©lectionnez :
# "ğŸ¤– Gestion des modÃ¨les"
```

### 4. **Lancement du proxy**
```powershell
# Dans le menu, sÃ©lectionnez :
# "ğŸš€ Mode Proxy (Serveur principal)"
```

## ğŸ§ª **ExÃ©cution des Tests**

### Depuis le menu interactif :
```powershell
.\LaunchMenu.ps1
# SÃ©lectionnez "ğŸ§ª Tests du systÃ¨me"
```

### En ligne de commande :
```powershell
.\dev-venv\Scripts\python.exe tests\test_implementation_validation.py
```

## ğŸ”§ **RÃ©solution des ProblÃ¨mes Courants**

### ProblÃ¨me : "null key is not allowed in a hash literal"
**Solution** : âœ… dans la version actuelle
- Le script de gestion des modÃ¨les gÃ©nÃ¨re maintenant des noms valides
- Configuration minimale de secours si nÃ©cessaire

### ProblÃ¨me : "usage: main.py [-h] [--log-level...] arguments invalides"
**Solution** : âœ… dans la version actuelle
- Le menu utilise maintenant `main.py` original avec arguments compatibles
- Plus d'utilisation de `main_fixed.py` cassÃ©

### ProblÃ¨me : "Ports occupÃ©s"
**Solution** : âœ… dans la version actuelle
- Le menu vÃ©rifie automatiquement la disponibilitÃ© des ports
- LibÃ©ration sÃ©curisÃ©e avec confirmation utilisateur

### ProblÃ¨me : "Aucun modÃ¨le valide trouvÃ©"
**Solution** :
1. VÃ©rifiez que vos fichiers `.gguf` sont dans `F:\llm\llama\models\`
2. Assurez-vous qu'ils font plus de 100MB
3. Le script gÃ©nÃ©rera des noms par dÃ©faut si nÃ©cessaire

## ğŸ“ **Contributions et Maintenance**

- **Tests** : Toute nouvelle fonctionnalitÃ© doit inclure des tests unitaires
- **Documentation** : Mettre Ã  jour la documentation pour chaque changement majeur
- **SÃ©curitÃ©** : Les droits d'accÃ¨s doivent Ãªtre restreints sur les fichiers sensibles

## ğŸ¯ **Statut Actuel**

- âœ… **Menu interactif fonctionnel** : Point d'entrÃ©e unique
- âœ… **Gestion des modÃ¨les corrigÃ©e** : Plus d'erreurs de clÃ©s nulles
- âœ… **CompatibilitÃ© main.py** : Arguments corrects, ports standards
- âœ… **Configuration minimale sÃ©curisÃ©e** : Fonctionne mÃªme sans modÃ¨les
- âœ… **Tests fonctionnels** : Validation complÃ¨te du systÃ¨me
- âœ… **SÃ©curitÃ© renforcÃ©e** : Droits restreints sur les fichiers critiques

## ğŸš€ **Prochaines Ã‰tapes**

- [ ] Ajouter plus de tests unitaires
- [ ] AmÃ©liorer la documentation des API
- [ ] Ajouter des exemples d'utilisation
- [ ] Optimiser les performances GPU

---

**ğŸš€ Statut** : **OPÃ‰RATIONNEL ET STABLE**  
**ğŸ”§ Version** : 1.0 Pro 
**ğŸ“… DerniÃ¨re mise Ã  jour** : 2025-11-07  
**âš¡ Temps de dÃ©marrage** : < 5 secondes