# ğŸ—ï¸ DOCS - Documentation CentralisÃ©e Llama Runner

## ğŸ” Vue d'ensemble

Projet : Llama Runner Async Proxy  
Objectif : Interface unifiÃ©e pour modÃ¨les IA (Ollama, LM Studio) avec dashboard moderne  
Principe : Separation of Concerns, code documentÃ©, tests inclus

---


## ğŸ§© Architecture

### Composants principaux

| Composant | Emplacement | Description |
|----------|-------------|-------------|
| **Backend Python** | `/llama_runner` | Gestion des runners, proxies, modÃ¨les |
| **Dashboard Web** | `/dashboard` | Interface Vue.js pour gestion et monitoring |
| **Scripts** | `/scripts` | Outils d'automatisation |
| **Configuration** | `/config` | Fichiers de configuration JSON |
| **Logs** | `/logs` | Journaux d'exÃ©cution |
| **ModÃ¨les** | `F:\\llm\\models` | Stockage des fichiers GGUF |

### Communication

```
[Vue.js Dashboard] <---> [Backend Python] <---> [Llama.cpp Runners]
     (Port 8080)           (Port 8585)           (Ports dynamiques)
         |                       |                      |
         | HTTP/WS API          | API REST               | Processus locaux
         |----------------------|------------------------|
```

---


## ğŸš€ Lancement

### Lancement complet (recommandÃ©)

```bash
python launch_dashboard.py
```

> âš ï¸ Cela lance **automatiquement** le backend Python **et** le dashboard Vue.js
> âœ… Le dashboard est accessible sur http://localhost:8080
> âœ… Ctrl+C arrÃªte proprement les deux services

### Lancement manuel

#### Backend seulement
```bash
python main.py --log-level INFO
```

> âœ… Proxies Ollama (11434) et LM Studio (1234) dÃ©marrÃ©s
> âœ… API Dashboard sur port 8585
> âœ… Dashboard Web sur port 8080 (nÃ©cessite lancement sÃ©parÃ© du dashboard)

#### Dashboard seulement
```bash
cd dashboard && npm run dev
```

> âœ… Dashboard accessible sur http://localhost:8080
> âœ… Communique avec le backend sur http://localhost:8585

---


## ğŸ”Œ API Endpoints

| Endpoint | MÃ©thode | Description |
|---------|---------|-------------|
| `/v1/models` | GET | Liste des modÃ¨les |
| `/v1/chat/completions` | POST | Chat avec modÃ¨le |
| `/v1/audio/transcriptions` | POST | Transcription audio |
| `/v1/audio/translations` | POST | Traduction audio |
| `/api/status` | GET | Statut du systÃ¨me (dashboard) |
| `/api/health` | GET | Statut de santÃ© (dashboard) |
| `/health` | GET | Statut du systÃ¨me |


### Ports
| Service | Port | URL |
|--------|------|-----|
| **Dashboard Web** | 8080 | http://localhost:8080 |
| **Dashboard API** | 8585 | http://localhost:8585 |
| **Ollama Proxy** | 11434 | http://localhost:11434 |
| **LM Studio Proxy** | 1234 | http://localhost:1234 |

---


## ğŸ›  Configuration

### Fichiers clÃ©s

- `config/app_config.json` : Configuration globale
- `config/models_config.json` : Liste des modÃ¨les et paramÃ¨tres

### Structure models_config.json
```json
{
  "default_model": "qwen2.5-7b-instruct",
  "models": {
    "qwen2.5-7b-instruct": {
      "model_path": "..\\models\\qwen2.5-7b-instruct-q4_k_m.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 16000,
        "n_gpu_layers": 50,
        "temp": 0.6
      },
      "auto_discovered": true,
      "auto_update_model": false
    }
  }
}
```

> âœ… La configuration se fait **via le dashboard**, pas manuellement.
> âœ… Les nouveaux modÃ¨les sont **auto-dÃ©couverts** sans Ã©craser les paramÃ¨tres existants.

---


## ğŸ“ Structure projet

```
llama-runner-async-proxy/
â”œâ”€â”€ launch_dashboard.py      # Lance backend + dashboard
â”œâ”€â”€ main.py                  # Backend seulement
â”œâ”€â”€ DOCS.md                  # Documentation centralisÃ©e
â”œâ”€â”€ ARCHITECTURE.md          # Architecture dÃ©taillÃ©e
â”œâ”€â”€ config/                  # Fichiers de configuration
â”œâ”€â”€ logs/                    # Journaux
â”œâ”€â”€ scripts/                 # Scripts utilitaires
â”œâ”€â”€ dashboard/               # Interface Vue.js
â””â”€â”€ llama_runner/            # Backend Python
    â”œâ”€â”€ headless_service_manager.py
    â”œâ”€â”€ config_loader.py
    â”œâ”€â”€ ollama_proxy_thread.py
    â”œâ”€â”€ lmstudio_proxy_thread.py
    â”œâ”€â”€ model_discovery.py
    â””â”€â”€ services/
        â”œâ”€â”€ config_validator.py
        â”œâ”€â”€ config_updater.py
        â”œâ”€â”€ metrics_collector.py
        â””â”€â”€ dashboard_api.py
```

---


## ğŸ§ª Tests

### RÃ©pertoires
- `/tests` : Tous les tests unitaires et d'intÃ©gration

### Commandes
```bash
# ExÃ©cuter tous les tests
python -m pytest tests/

# Validation systÃ¨me
powershell .\\scripts\\validate_system.ps1
```

---


## ğŸ”§ Environnement

### PrÃ©requis
- Python 3.11+
- Node.js 16+ (pour le dashboard)
- PowerShell 7+
- VS Code

### Installation
```powershell
# CrÃ©er l'environnement virtuel
python -m venv dev-venv

# Activer
dev-venv\\Scripts\\Activate.ps1

# Installer les dÃ©pendances
pip install -r requirements.txt

# Installer les dÃ©pendances du dashboard
cd dashboard && npm install
```

---


## ğŸ”„ Maintenance

### DÃ©couverte automatique
- Les nouveaux modÃ¨les GGUF sont **auto-dÃ©couverts** dans `F:\\llm\\models`
- Les **paramÃ¨tres existants sont prÃ©servÃ©s**
- Seuls les **nouveaux modÃ¨les** sont ajoutÃ©s

### Nettoyage
- Suppression automatique des fichiers de cache
- Rotation des logs

### Sauvegarde
- Configurations sauvegardÃ©es automatiquement avant modification
- Backup manuel possible via copie du dossier `config/`

---


## ğŸ¯ Principes de dÃ©veloppement

1. **Separation of Concerns** : Chaque composant a une responsabilitÃ© unique
2. **Code documentÃ©** : Commentaires prÃ©cis, typage strict, variables explicites
3. **Tests inclus** : Assurer la fiabilitÃ© et la maintenance
4. **Interface utilisateur** : Dashboard Vue.js comme point central de gestion
5. **Ã‰viter les actions manuelles** : Automatiser les tÃ¢ches rÃ©pÃ©titives
6. **ArrÃªt propre** : Ctrl+C arrÃªte tous les services correctement

---


### âœ… Version actuelle : 2025-11-12

Documentation mise Ã  jour aprÃ¨s correction complÃ¨te des problÃ¨mes.