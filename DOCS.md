# ğŸ—ï¸ DOCS - Documentation CentralisÃ©e Llama Runner

## ğŸ” Vue d'ensemble

Projet : Llama Runner Async Proxy  
Objectif : Interface unifiÃ©e pour modÃ¨les IA (Ollama, LM Studio) avec dashboard moderne  
Principe : Separation of Concerns, code documentÃ©, tests inclus

---


## ğŸ§© Architecture

### Composants principaux

| Composant | Emplacement | Description |
|----------|-------------|-------------|
| **Backend Python** | `/llama_runner` | Gestion des runners, proxies, modÃ¨les |
| **Dashboard Web** | `/dashboard` | Interface Vue.js pour gestion et monitoring |
| **Scripts** | `/scripts` | Outils d'automatisation |
| **Configuration** | `/config` | Fichiers de configuration JSON |
| **Logs** | `/logs` | Journaux d'exÃ©cution |
| **ModÃ¨les** | `F:\\llm\\models` | Stockage des fichiers GGUF |

### Communication

```
[Vue.js Dashboard] <---> [Backend Python] <---> [Llama.cpp Runners]
     (Port 8080)           (Port 8585)           (Ports dynamiques)
         |                       |                      |
         | HTTP/WS API          | API REST               | Processus locaux
         |----------------------|------------------------|
```

---


## ğŸš€ Lancement

### Script simplifiÃ© (`LaunchMenu.ps1`)

- **ğŸš€ Mode Proxy (Serveur principal)** : Proxies Ollama + LM Studio
- **ğŸ¦™ Mode Llama.cpp seul** : Direct llama-server.exe
- **ğŸŒ Mode Proxy + WebUI** : Avec interface web
- **ğŸ”§ Mode DÃ©veloppement (Debug)** : Logs dÃ©taillÃ©s
- **ğŸ” Validation systÃ¨me** : VÃ©rification d'intÃ©gritÃ©
- **âŒ Quitter** : Sortie du menu

> âš ï¸ Toute la configuration se fait via le **dashboard**, pas via PowerShell.

---


## ğŸ”Œ API Endpoints

| Endpoint | MÃ©thode | Description |
|---------|---------|-------------|
| `/v1/models` | GET | Liste des modÃ¨les |
| `/v1/chat/completions` | POST | Chat avec modÃ¨le |
| `/v1/audio/transcriptions` | POST | Transcription audio |
| `/v1/audio/translations` | POST | Traduction audio |
| `/health` | GET | Statut du systÃ¨me |


### Ports
| Service | Port | URL |
|--------|------|-----|
| **Dashboard Web** | 8080 | http://localhost:8080 |
| **Backend API** | 8585 | http://localhost:8585 |
| **Ollama Proxy** | 11434 | http://localhost:11434 |
| **LM Studio Proxy** | 1234 | http://localhost:1234 |
| **llama-server (direct)** | 8035 | http://localhost:8035 |

---


## ğŸ›  Configuration

### Fichiers clÃ©s

- `config/app_config.json` : Configuration globale
- `config/models_config.json` : Liste des modÃ¨les et paramÃ¨tres

### Structure models_config.json
```json
{
  "default_model": "qwen2.5-7b-instruct",
  "models": {
    "qwen2.5-7b-instruct": {
      "model_path": "..\\models\\qwen2.5-7b-instruct-q4_k_m.gguf",
      "llama_cpp_runtime": "llama-server",
      "parameters": {
        "ctx_size": 16000,
        "n_gpu_layers": 50,
        "temp": 0.6
      }
    }
  }
}
```

> âœ… La configuration se fait **via le dashboard**, pas manuellement.

---


## ğŸ“ Structure projet

```
llama-runner-async-proxy/
â”œâ”€â”€ LaunchMenu.ps1           # Lanceur simplifiÃ©
â”œâ”€â”€ main.py                  # Point d'entrÃ©e principal
â”œâ”€â”€ DOCS.md                  # Documentation centralisÃ©e
â”œâ”€â”€ ARCHITECTURE.md          # Architecture dÃ©taillÃ©e
â”œâ”€â”€ config/                  # Fichiers de configuration
â”œâ”€â”€ logs/                    # Journaux
â”œâ”€â”€ scripts/                 # Scripts utilitaires
â”œâ”€â”€ dashboard/               # Interface Vue.js
â””â”€â”€ llama_runner/            # Backend Python
    â”œâ”€â”€ headless_service_manager.py
    â”œâ”€â”€ config_loader.py
    â”œâ”€â”€ ollama_proxy_thread.py
    â”œâ”€â”€ lmstudio_proxy_thread.py
    â””â”€â”€ model_discovery.py
```

---


## ğŸ§ª Tests

### RÃ©pertoires
- `/tests` : Tous les tests unitaires et d'intÃ©gration

### Commandes
```bash
# ExÃ©cuter tous les tests
python -m pytest tests/

# Validation systÃ¨me
powershell .\\scripts\\validate_system.ps1
```

---


## ğŸ”§ Environnement

### PrÃ©requis
- Python 3.11+
- Node.js 16+ (pour le dashboard)
- PowerShell 7+
- VS Code

### Installation
```powershell
# CrÃ©er l'environnement virtuel
python -m venv dev-venv

# Activer
dev-venv\\Scripts\\Activate.ps1

# Installer les dÃ©pendances
pip install -r requirements.txt

# DÃ©marrer le backend
python main.py --headless

# DÃ©marrer le dashboard
cd dashboard
npm run dev
```

---


## ğŸ”„ Maintenance

### Nettoyage
- Suppression automatique des fichiers de cache
- Rotation des logs

### Sauvegarde
- Configurations sauvegardÃ©es automatiquement avant modification
- Backup manuel possible via copie du dossier `config/`

---


## ğŸ¯ Principes de dÃ©veloppement

1. **Separation of Concerns** : Chaque composant a une responsabilitÃ© unique
2. **Code documentÃ©** : Commentaires prÃ©cis, typage strict, variables explicites
3. **Tests inclus** : Assurer la fiabilitÃ© et la maintenance
4. **Interface utilisateur** : Dashboard Vue.js comme point central de gestion
5. **Ã‰viter les actions manuelles** : Automatiser les tÃ¢ches rÃ©pÃ©titives

---


### âœ… Version actuelle : 2025-11-11

Documentation mise Ã  jour aprÃ¨s restructuration complÃ¨te.