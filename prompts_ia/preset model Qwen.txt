set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\JanusCoderV-7B-i1-GGUF\JanusCoderV-7B.i1-Q4_K_S.gguf --alias JanusCoderV-7B.i1-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 1024 --ubatch-size 512 --threads 6 --flash-attn on --mlock --no-mmap --jinja --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 30 --n-gpu-layers 85 --ctx-size 32000
 
 
set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 256 --ubatch-size 128 --threads 6 --jinja  --kv-unified -fa on  --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 30 --n-gpu-layers 15 --ctx-size 256000 --no-warmup
 
 --no-mmap -np
 
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --mlock  --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 20 --n-gpu-layers 15 -np 4 --ctx-size 128000 --kv-unified
 
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/rpc-server -H 0.0.0.0 -p 50052 -c -d 
 set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; .\rpc-server -H 0.0.0.0 -p 50053 -c 
 
 export CUDA_VISIBLE_DEVICE=0 export CUDA_USE_TENSOR_CORES=1 export LLAMA_SET_ROWS=1; .\rpc-server -H 0.0.0.0 -p 50053 -c 
 
  set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 2048 --ubatch-size 1024 --threads 6 --jinja  --kv-unified -fa on --no-mmap --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 99 --n-gpu-layers 35 --ctx-size 128000 --no-warmup --rpc "192.168.1.3:50052,192.168.1.194:50054"
 
 --rpc "192.168.1.3:50052,192.16810.194:50054"
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --batch-size 2048 --ubatch-size 1024 --threads 6 --jinja  -fa on --kv-unified --context-shift --no-mmap -sm row -ts 80,20 --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 60 --n-gpu-layers 48 --ctx-size 128000 --no-warmup --rpc "192.168.1.3:50052,192.168.1.205:50054" --cache-reuse 128 --timeout 6000
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set GGML_CUDA_FORCE_CUBLAS=ON; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --batch-size 2048 --ubatch-size 1024 --threads 6 --jinja --kv-unified -fa on --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 40 --n-gpu-layers 47 --ctx-size 96000 --no-warmup --rpc "192.168.1.3:50052,192.168.1.194:50054"
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  -fa on --kv-unified --context-shift -sm row -ts 35,65 --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 34 --n-gpu-layers 99 --ctx-size 128000 --no-warmup --rpc "192.168.1.205:50054" --timeout 6000
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0 --host 127.0.0.1 --port 8134 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  -fa on --kv-unified --context-shift -sm row -ts 35,65 --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 34 --n-gpu-layers 40 --ctx-size 128000 --no-warmup --rpc "192.168.1.205:50054" --timeout 6000
 
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0 --host 127.0.0.1 --port 8134 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  -fa on --kv-unified --context-shift -sm row -ts 30,70 --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-cpu-moe 25 --n-gpu-layers 25 --ctx-size 128000 --no-warmup --rpc "192.168.1.205:50054" --timeout 6000
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0 --host 127.0.0.1 --port 8134 --batch-size 512 --ubatch-size 256 --threads 10 --jinja  -fa on --kv-unified --context-shift -sm row -ts 30,70 --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 999 --ctx-size 112000 --no-warmup --rpc "192.168.1.81:50054" --timeout 6000 --no-mmap --mlock --log-colors on --n-cpu-moe 30
 
 
--cpu-moe 
--no-warmup

set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf --alias Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0 --host 127.0.0.1 --port 8134 --batch-size 1024 --ubatch-size 1024 --threads 10 --jinja -fa on --context-shift -sm row --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 30 --ctx-size 96000 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap  --device CUDA0,RPC0
 
 
export CUDA_VISIBLE_DEVICE=0 ; export LLAMA_SET_ROWS=1 ; ./llama-server --model "/media/bamer/crucial MX300/llm/llama/models/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S-gguf/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S.gguf" --alias Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S --host 127.0.0.1 --port 8134  --threads 10 --jinja -fa on -sm row --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 48 --ctx-size 0 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 99 --no-mmap --cache-type-k q8_0 --cache-type-v q8_0 --device CUDA0,RPC0,RPC1
 
 
 export CUDA_VISIBLE_DEVICE=0 ; export LLAMA_SET_ROWS=1 ; ./llama-server --model "/media/bamer/crucial MX300/llm/llama/models/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S-gguf/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S.gguf" --alias Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S --host 127.0.0.1 --port 8134  --threads 10 --jinja -fa on --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 48 --ctx-size 128000 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 99 --no-mmap --device CUDA0,RPC1,RPC0
 
Qwen3-Coder-30B-A3B-Instruct-1M-MXFP4_MOE.gguf
 
-ts 22,78 
--cpu-moe 
--no-warmup 
--kv-unified 
--parallel 2
 
export CUDA_VISIBLE_DEVICE=0 ; export LLAMA_SET_ROWS=1 ; ./llama-server --model "/media/bamer/crucial MX300/llm/llama/models/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S-gguf/Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S.gguf" --alias Qwen3-Coder-30B-A3B-Instruct-128x1.8B-Q2_K_S --host 127.0.0.1 --port 8134  --threads 10 --jinja -fa on --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 44 --ctx-size 128000 --timeout 7000000 --log-colors on --n-cpu-moe 99 --no-mmap --cache-reuse 256
 
 
 
 
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-MXFP4_MOE.gguf --alias Qwen3-Coder-30B-A3B-Instruct-1M-MXFP4_MOE.gguf --host 127.0.0.1 --port 8134 --threads 6 --jinja -fa on --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 30 --ctx-size 96000 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap --device CUDA0,RPC0
 
set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0-gguf\Qwen3-Coder-30B-A3B-Instruct-1M-MXFP4_MOE.gguf --alias Qwen3-Coder-30B-A3B-Instruct-1M-MXFP4_MOE.gguf --host 127.0.0.1 --port 8134 --threads 6 --jinja -fa on --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 33 --ctx-size 256000 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap --device RPC0,CUDA0 
 
 
Qwen3-14B-Q4_K_S.gguf

set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Seed-Coder-8B-Reasoning-GGUF\Seed-Coder-8B-Reasoning-UD-Q5_K_XL.gguf --alias Seed-Coder-8B-Reasoning.gguf --host 127.0.0.1 --port 8134 --threads 6 --jinja -fa on --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 --n-gpu-layers 99 --ctx-size 256000 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap --device RPC0,CUDA0 
 
 
 miromind-ai_MiroThinker-v1.0-8B-Q6_K_L-gguf
 
set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\miromind-ai_MiroThinker-v1.0-8B-Q6_K_L-gguf\miromind-ai_MiroThinker-v1.0-8B-Q6_K_L.gguf --alias miromind-ai_MiroThinker-v1.0-8B-Q6_K_L --host 127.0.0.1 --port 8134 --threads 6 --jinja -fa on --temp 1.0  --top-p 0.95 --repeat-penalty 1.05 --n-gpu-layers 99 --ctx-size 262144 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap -sm row --device CUDA0,RPC0

set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\miromind-ai_MiroThinker-v1.0-8B-Q6_K_L-gguf\miromind-ai_MiroThinker-v1.0-8B-Q6_K_L.gguf --alias miromind-ai_MiroThinker-v1.0-8B-Q6_K_L --host 127.0.0.1 --port 8134 --threads 8 --jinja -fa on --temp 1.0  --top-p 0.95 --repeat-penalty 1.05 --n-gpu-layers 10 --ctx-size 0 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap -sm row --reasoning-budget 0 --device CUDA0,RPC0 



Olmo-3-7B-Think-SFT.Q6_K.gguf

set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Olmo-3-7B-Think-SFT.Q6_K-gguf\Olmo-3-7B-Think-SFT.Q6_K.gguf --alias Olmo-3-7B-Think-SFT.Q6_K.gguf --host 127.0.0.1 --port 8134 --threads 6 --jinja -fa on --temp 1.0  --top-p 0.95 --repeat-penalty 1.05 --n-gpu-layers 99 --ctx-size 0 --rpc "192.168.1.81:50054" --timeout 7000000 --log-colors on --n-cpu-moe 55 --no-mmap -sm row --device CUDA0,RPC0

 
 
 
 
 
 
 
 
 
 set CUDA_VISIBLE_DEVICE=0 ; set CUDA_USE_TENSOR_CORES=1; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\gpt-oss-20b-Q4_K_S-gguf\gpt-oss-20b-Q4_K_S.gguf --alias gpt-oss-20b-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --mlock  --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 20 --n-gpu-layers 15 -np 4 --ctx-size 128000 
 
  set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\gpt-oss-20b-Q4_K_S-gguf\gpt-oss-20b-Q4_K_S.gguf --alias gpt-oss-20b-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --no-context-shift --n-cpu-moe 20 --n-gpu-layers 60 -np 4 --ctx-size 512000 --swa-full  --cache-reuse 256 --reasoning-format high
 
  set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\GPT-OSS-Code-Reasoning-20B.Q8_0-gguf\GPT-OSS-Code-Reasoning-20B.Q8_0.gguf --alias GPT-OSS-Code-Reasoning-20B.Q8_0 --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 256 --ubatch-size 128 --threads 6 --jinja --kv-unified --flash-attn on  --n-cpu-moe 20 --n-gpu-layers 60 --np 2 --ctx-size 512000  --cache-reuse 128 --chat-template-kwargs '{"reasoning_effort": "high"}' --no-warmup  
 
 
set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\DeepSeek-Coder-V2-Lite-Instruct.Q4_K-gguf\DeepSeek-Coder-V2-Lite-Instruct.Q4_K.gguf --alias DeepSeek-Coder-V2-Lite-Instruct.Q4_K --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 256 --ubatch-size 128 --threads 6 --jinja --kv-unified -fa on -b 2048 -ub 2048 --n-cpu-moe 20 --n-gpu-layers 10 --cache-reuse 128 --temp 0.6 --top-p 0.95 --min-p 0.01 --ctx-size 163840 --seed 3407 --no-warmup 
 
 
 
set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\JanusCoderV-7B-i1-GGUF\JanusCoderV-7B.i1-Q4_K_S.gguf --alias JanusCoderV-7B.i1-Q4_K_S.gguf --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 256 --ubatch-size 128 --threads 6 --jinja  --flash-attn on --no-context-shift --n-cpu-moe 10 --n-gpu-layers 70  --cache-reuse 128 -np 2 --ctx-size 128000
 
 set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\JanusCoderV-7B-i1-GGUF\JanusCoderV-7B.i1-Q4_K_S.gguf --host 127.0.0.1 --port 8134 --main-gpu 0 --threads 6 --jinja --flash-attn on --n-cpu-moe 10 --n-gpu-layers 70 --ctx-size 64000
 
 
 use error-analysis agent to find and fix the aplication then use  multi-agent-review agent for ensure the code quality follow best practice and project guideline
 
 --port 0 
 --ctx-size 32000 
 --n-gpu-layers 85 
 --temp 0.7 
 --top-p 0.95 
 --top-k 20 
 --repeat-penalty 1.2 
 --presence-penalty 1.5 
 --repeat-last-n 64 
 --batch-size 1024 
 --ubatch-size 512 
 --threads 10 
 --flash-attn on 
 --mlock 
 --no-mmap 
 --cache-type-k f16 
 --cache-type-v f16 
 --rope-freq-base 0.0 
 --rope-freq-scale 0.0 
 --yarn-ext-factor -1.0 
 --yarn-attn-factor 1.0 
 --yarn-beta-fast 32.0 
 --yarn-beta-slow 1.0 
 --yarn-orig-ctx 0 
 --main-gpu 0 
 --reasoning-budget -1 
 --jinja 
 --no-context-shift 
 --n-cpu-moe 30   
 --min-p 0.0 
 --mirostat 0 
 --predict -1  
 --reasoning-format deepseek

 
 
 
 Profil SAFE+ (S√©curit√© maximale + Optimisations performance)
{

 "ctx_size": 32000,
  "n_gpu_layers": 85,
  "temp": 0.6,
  "top_p": 0.95,
  "top_k": 20,
  "repeat_penalty": 1.2,
  "presence_penalty": 1.5,
  "repeat_last_n": 64,
  "batch_size": 2048,
  "ubatch_size": 1024,
  "threads": 12,
  "flash_attn": "on",
  "mlock": true,
  "no_mmap": true,
  "no_mul_mat_q": false,
  "cache_type_k": "f16",
  "cache_type_v": "f16",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": -1.0,
  "no_kv_offload": false,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": -1,
  "jinja": true,
  "no_warmup": false,
  "no_context_shift": true,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "pooling_type": 0,
  "min_p": 0.0,
  "tfs_z": 1.0,
  "typical_p": 1.0,
  "mirostat": 0,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": true,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false,
  "predict": true,
  "reasoning_format": "deepseek"
}

Caract√©ristiques ULTRA TURBO :

‚úÖ Contexte r√©duit (8K) pour m√©moire minimale
‚úÖ Cache en q8_0 pour vitesse maximale
‚úÖ No warmup et no context shift activ√©s
‚úÖ Reasoning budget limit√© pour r√©ponse rapide
‚úÖ Defrag threshold optimis√© pour performance
‚úÖ Batch size maximal pour throughput
{
  "ctx_size": 8192,
  "n_gpu_layers": 85,
  "temp": 0.2,
  "top_p": 0.8,
  "top_k": 15,
  "repeat_penalty": 1.05,
  "repeat_last_n": 32,
  "batch_size": 4096,
  "ubatch_size": 2048,
  "threads": 16,
  "flash_attn": "on",
  "mlock": false,
  "no_mmap": true,
  "cache_type_k": "q8_0",
  "cache_type_v": "q8_0",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": 0.8,
  "no_kv_offload": true,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": 2048,
  "jinja": true,
  "no_warmup": true,
  "no_context_shift": true,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 256,
  "pooling_type": 0,
  "min_p": 0.1,
  "tfs_z": 0.9,
  "typical_p": 0.9,
  "mirostat": 0,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": true,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false
}
Caract√©ristiques PERFORMANCE :

‚úÖ Temp√©rature √©quilibr√©e (0.7) pour cr√©ativit√© contr√¥l√©e
‚úÖ Flash Attention activ√© pour vitesse maximale
‚úÖ Batch size augment√© pour throughput
‚úÖ Min-P et TFS pour qualit√© de g√©n√©ration
‚úÖ Mirostat activ√© pour contr√¥le adaptatif
üöÄ Profil ULTRA TURBO (Vitesse maximale)
{
  "ctx_size": 32000,
  "n_gpu_layers": 85,
  "temp": 0.7,
  "top_p": 0.95,
  "top_k": 40,
  "repeat_penalty": 1.1,
  "repeat_last_n": 64,
  "batch_size": 2048,
  "ubatch_size": 1024,
  "threads": 12,
  "flash_attn": "on",
  "mlock": true,
  "no_mmap": true,
  "cache_type_k": "f16",
  "cache_type_v": "f16",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": -1.0,
  "no_kv_offload": false,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": -1,
  "jinja": true,
  "no_warmup": false,
  "no_context_shift": false,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "pooling_type": 0,
  "min_p": 0.05,
  "tfs_z": 0.95,
  "typical_p": 0.95,
  "mirostat": 2,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": true,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false
}




 Temp√©rature basse (0.3) pour des r√©ponses coh√©rentes
‚úÖ R√©p√©tition p√©nalis√©e (1.2) pour √©viter les boucles
‚úÖ Top-k limit√© (20) pour la stabilit√©
‚úÖ Cache en f16 pour pr√©cision maximale
‚úÖ Tous les param√®tres de s√©curit√© activ√©s

{
  "ctx_size": 32000,
  "n_gpu_layers": 85,
  "temp": 0.3,
  "top_p": 0.9,
  "top_k": 20,
  "repeat_penalty": 1.2,
  "repeat_last_n": 64,
  "batch_size": 1024,
  "ubatch_size": 512,
  "threads": 8,
  "flash_attn": "auto",
  "mlock": true,
  "no_mmap": true,
  "cache_type_k": "f16",
  "cache_type_v": "f16",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": -1.0,
  "no_kv_offload": false,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": -1,
  "jinja": true,
  "no_warmup": false,
  "no_context_shift": false,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "pooling_type": 0,
  "min_p": 0.0,
  "tfs_z": 1.0,
  "typical_p": 1.0,
  "mirostat": 0,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": false,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false
}
