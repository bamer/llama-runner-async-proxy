set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\JanusCoderV-7B-i1-GGUF\JanusCoderV-7B.i1-Q4_K_S.gguf --alias JanusCoderV-7B.i1-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 1024 --ubatch-size 512 --threads 6 --flash-attn on --mlock --no-mmap --jinja --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 30 --n-gpu-layers 85 --ctx-size 32000
 
 
set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --mlock  --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 0 --n-gpu-layers 85 -np 3 --ctx-size 128000 --kv-unified true
 
 --no-mmap
 
 
 set PYTHONIOENCODING=utf-8 ; set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S-GGUF\Qwen3-Coder-30B-A3B-Instruct-Q4_K_S.gguf --alias Qwen3-Coder-30B-A3B-Instruct-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --mlock  --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 20 --n-gpu-layers 15 -np 4 --ctx-size 128000 --kv-unified
 
 
 set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\gpt-oss-20b-Q4_K_S-gguf\gpt-oss-20b-Q4_K_S.gguf --alias gpt-oss-20b-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --mlock  --no-context-shift --predict -1 --temp 0.7 --top-p 0.95 --top-k 20 --repeat-penalty 1.2 --repeat-last-n 64 --n-cpu-moe 20 --n-gpu-layers 15 -np 4 --ctx-size 128000 
 
  set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\gpt-oss-20b-Q4_K_S-gguf\gpt-oss-20b-Q4_K_S.gguf --alias gpt-oss-20b-Q4_K_S --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 512 --ubatch-size 256 --threads 6 --jinja  --flash-attn on --no-context-shift --n-cpu-moe 20 --n-gpu-layers 60 -np 4 --ctx-size 512000 --swa-full  --cache-reuse 256 --reasoning-format high
 
  set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\GPT-OSS-Code-Reasoning-20B.Q8_0-gguf\GPT-OSS-Code-Reasoning-20B.Q8_0.gguf --alias GPT-OSS-Code-Reasoning-20B.Q8_0 --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 1024 --ubatch-size 512 --threads 6 --jinja  --flash-attn on --no-context-shift --n-cpu-moe 20 --n-gpu-layers 60 --np 2 --ctx-size 512000 --swa-full  --cache-reuse 256 --chat-template-kwargs '{"reasoning_effort": "high"}'
 
 
   set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\DeepSeek-Coder-V2-Lite-Instruct.Q4_K-gguf\DeepSeek-Coder-V2-Lite-Instruct.Q4_K.gguf --alias GPT-OSS-Code-Reasoning-20B.Q8_0 --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 1024 --ubatch-size 512 --threads 6 --jinja  --flash-attn on --no-context-shift --n-cpu-moe 20 --n-gpu-layers 60 --ctx-size 512000 --cache-reuse 256 
 
set PYTHONIOENCODING=utf-8 ;set CUDA_VISIBLE_DEVICE=0 ; set LLAMA_SET_ROWS=1 ; F:/llm/llama/llama-server.exe --model F:\llm\llama\models\JanusCoderV-7B-i1-GGUF\JanusCoderV-7B.i1-Q4_K_S.gguf --alias JanusCoderV-7B.i1-Q4_K_S.gguf --host 127.0.0.1 --port 8134 --main-gpu 0 --batch-size 256 --ubatch-size 128 --threads 6 --jinja  --flash-attn on --no-context-shift --n-cpu-moe 10 --n-gpu-layers 70  --cache-reuse 128 -np 2 --ctx-size 128000
 
 
 use error-analysis agent to find and fix the aplication then use  multi-agent-review agent for ensure the code quality follow best practice and project guideline
 
 --port 0 
 --ctx-size 32000 
 --n-gpu-layers 85 
 --temp 0.7 
 --top-p 0.95 
 --top-k 20 
 --repeat-penalty 1.2 
 --presence-penalty 1.5 
 --repeat-last-n 64 
 --batch-size 1024 
 --ubatch-size 512 
 --threads 10 
 --flash-attn on 
 --mlock 
 --no-mmap 
 --cache-type-k f16 
 --cache-type-v f16 
 --rope-freq-base 0.0 
 --rope-freq-scale 0.0 
 --yarn-ext-factor -1.0 
 --yarn-attn-factor 1.0 
 --yarn-beta-fast 32.0 
 --yarn-beta-slow 1.0 
 --yarn-orig-ctx 0 
 --main-gpu 0 
 --reasoning-budget -1 
 --jinja 
 --no-context-shift 
 --n-cpu-moe 30   
 --min-p 0.0 
 --mirostat 0 
 --predict -1  
 --reasoning-format deepseek

 
 
 
 Profil SAFE+ (S√©curit√© maximale + Optimisations performance)
{

 "ctx_size": 32000,
  "n_gpu_layers": 85,
  "temp": 0.6,
  "top_p": 0.95,
  "top_k": 20,
  "repeat_penalty": 1.2,
  "presence_penalty": 1.5,
  "repeat_last_n": 64,
  "batch_size": 2048,
  "ubatch_size": 1024,
  "threads": 12,
  "flash_attn": "on",
  "mlock": true,
  "no_mmap": true,
  "no_mul_mat_q": false,
  "cache_type_k": "f16",
  "cache_type_v": "f16",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": -1.0,
  "no_kv_offload": false,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": -1,
  "jinja": true,
  "no_warmup": false,
  "no_context_shift": true,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "pooling_type": 0,
  "min_p": 0.0,
  "tfs_z": 1.0,
  "typical_p": 1.0,
  "mirostat": 0,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": true,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false,
  "predict": true,
  "reasoning_format": "deepseek"
}

Caract√©ristiques ULTRA TURBO :

‚úÖ Contexte r√©duit (8K) pour m√©moire minimale
‚úÖ Cache en q8_0 pour vitesse maximale
‚úÖ No warmup et no context shift activ√©s
‚úÖ Reasoning budget limit√© pour r√©ponse rapide
‚úÖ Defrag threshold optimis√© pour performance
‚úÖ Batch size maximal pour throughput
{
  "ctx_size": 8192,
  "n_gpu_layers": 85,
  "temp": 0.2,
  "top_p": 0.8,
  "top_k": 15,
  "repeat_penalty": 1.05,
  "repeat_last_n": 32,
  "batch_size": 4096,
  "ubatch_size": 2048,
  "threads": 16,
  "flash_attn": "on",
  "mlock": false,
  "no_mmap": true,
  "cache_type_k": "q8_0",
  "cache_type_v": "q8_0",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": 0.8,
  "no_kv_offload": true,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": 2048,
  "jinja": true,
  "no_warmup": true,
  "no_context_shift": true,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 256,
  "pooling_type": 0,
  "min_p": 0.1,
  "tfs_z": 0.9,
  "typical_p": 0.9,
  "mirostat": 0,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": true,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false
}
Caract√©ristiques PERFORMANCE :

‚úÖ Temp√©rature √©quilibr√©e (0.7) pour cr√©ativit√© contr√¥l√©e
‚úÖ Flash Attention activ√© pour vitesse maximale
‚úÖ Batch size augment√© pour throughput
‚úÖ Min-P et TFS pour qualit√© de g√©n√©ration
‚úÖ Mirostat activ√© pour contr√¥le adaptatif
üöÄ Profil ULTRA TURBO (Vitesse maximale)
{
  "ctx_size": 32000,
  "n_gpu_layers": 85,
  "temp": 0.7,
  "top_p": 0.95,
  "top_k": 40,
  "repeat_penalty": 1.1,
  "repeat_last_n": 64,
  "batch_size": 2048,
  "ubatch_size": 1024,
  "threads": 12,
  "flash_attn": "on",
  "mlock": true,
  "no_mmap": true,
  "cache_type_k": "f16",
  "cache_type_v": "f16",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": -1.0,
  "no_kv_offload": false,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": -1,
  "jinja": true,
  "no_warmup": false,
  "no_context_shift": false,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "pooling_type": 0,
  "min_p": 0.05,
  "tfs_z": 0.95,
  "typical_p": 0.95,
  "mirostat": 2,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": true,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false
}




 Temp√©rature basse (0.3) pour des r√©ponses coh√©rentes
‚úÖ R√©p√©tition p√©nalis√©e (1.2) pour √©viter les boucles
‚úÖ Top-k limit√© (20) pour la stabilit√©
‚úÖ Cache en f16 pour pr√©cision maximale
‚úÖ Tous les param√®tres de s√©curit√© activ√©s

{
  "ctx_size": 32000,
  "n_gpu_layers": 85,
  "temp": 0.3,
  "top_p": 0.9,
  "top_k": 20,
  "repeat_penalty": 1.2,
  "repeat_last_n": 64,
  "batch_size": 1024,
  "ubatch_size": 512,
  "threads": 8,
  "flash_attn": "auto",
  "mlock": true,
  "no_mmap": true,
  "cache_type_k": "f16",
  "cache_type_v": "f16",
  "rope_freq_base": 0.0,
  "rope_freq_scale": 0.0,
  "yarn_ext_factor": -1.0,
  "yarn_attn_factor": 1.0,
  "yarn_beta_fast": 32.0,
  "yarn_beta_slow": 1.0,
  "yarn_orig_ctx": 0,
  "defrag_thold": -1.0,
  "no_kv_offload": false,
  "split_mode": 0,
  "main_gpu": 0,
  "reasoning_budget": -1,
  "jinja": true,
  "no_warmup": false,
  "no_context_shift": false,
  "n_cpu_moe": -1,
  "grp_attn_n": 1,
  "grp_attn_w": 512,
  "pooling_type": 0,
  "min_p": 0.0,
  "tfs_z": 1.0,
  "typical_p": 1.0,
  "mirostat": 0,
  "mirostat_tau": 5.0,
  "mirostat_eta": 0.1,
  "penalize_nl": false,
  "ignore_eos": false,
  "rope_scaling_factor": 0.0,
  "rope_scaling_orig_ctx_len": 0,
  "rope_scaling_finetuned": false
}