# ğŸ—ï¸ Architecture du Projet Llama Runner

## Vue d'ensemble

Le projet Llama Runner est structurÃ© selon le principe de **sÃ©paration des prÃ©occupations (Separation of Concerns)** avec une interface utilisateur moderne basÃ©e sur Vue.js.

## Composants principaux

### 1. Backend Python (`/llama_runner`)

- **ResponsabilitÃ©** : Gestion des runners, proxies, modÃ¨les et services
- **Technologies** : Python 3, FastAPI, asyncio
- **Ports** : 8585 (API principale), 11434 (Ollama proxy), 1234 (LM Studio proxy)
- **FonctionnalitÃ©s** :
  - Gestion des runners llama.cpp
  - Proxies Ollama et LM Studio
  - Service de dÃ©couverte et gestion des modÃ¨les
  - Validation et gestion de configuration

### 2. Dashboard Web (`/dashboard`)

- **ResponsabilitÃ©** : Interface utilisateur moderne pour la gestion du systÃ¨me
- **Technologies** : Vue.js 3, Element Plus, Chart.js, Vite
- **Port** : 8080
- **FonctionnalitÃ©s** :
  - Dashboard temps rÃ©el avec mÃ©triques et graphiques
  - Gestion des modÃ¨les (ajout, suppression, configuration)
  - ContrÃ´le des services et proxies
  - Interface de configuration graphique
  - Monitoring des performances
  - Journalisation systÃ¨me

### 3. Scripts utilitaires (`/scripts`)

- **ResponsabilitÃ©** : Outils d'automatisation et de maintenance
- **FonctionnalitÃ©s** :
  - Validation du systÃ¨me
  - Gestion de la configuration
  - Maintenance des ports

## Communication entre composants

```
[Vue.js Dashboard] <---> [Backend Python] <---> [Llama.cpp Runners]
     (Port 8080)           (Port 8585)           (Ports dynamiques)
         |                       |                      |
         | HTTP/WS API          | API REST/WS          | Processus locaux
         |----------------------|----------------------|
```

## Flux d'interaction

1. **Lancement du systÃ¨me** :
   - ExÃ©cuter `LaunchMenu.ps1` pour dÃ©marrer le backend
   - AccÃ©der Ã  `http://localhost:8080` pour le dashboard

2. **Gestion via le dashboard** :
   - Configuration des modÃ¨les
   - ContrÃ´le des services
   - Surveillance des performances
   - Journalisation

3. **AccÃ¨s aux services** :
   - Ollama Proxy : `http://localhost:11434`
   - LM Studio Proxy : `http://localhost:1234`
   - Dashboard : `http://localhost:8080`

## Structure du projet

```
llama-runner-async-proxy/
â”œâ”€â”€ llama_runner/          # Backend Python
â”‚   â”œâ”€â”€ controllers/       # ContrÃ´leurs API
â”‚   â”œâ”€â”€ models/           # ModÃ¨les de donnÃ©es
â”‚   â”œâ”€â”€ repositories/     # AccÃ¨s aux donnÃ©es
â”‚   â”œâ”€â”€ services/         # Services mÃ©tier
â”‚   â””â”€â”€ ...               # Autres composants
â”œâ”€â”€ dashboard/            # Interface Vue.js
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # Composants rÃ©utilisables
â”‚   â”‚   â”œâ”€â”€ views/       # Vues principales
â”‚   â”‚   â””â”€â”€ ...          # Autres fichiers
â”œâ”€â”€ scripts/              # Scripts utilitaires
â”œâ”€â”€ config/               # Fichiers de configuration
â”œâ”€â”€ logs/                 # Fichiers de journalisation
â”œâ”€â”€ models/               # ModÃ¨les GGUF
â””â”€â”€ main.py              # Point d'entrÃ©e principal
```

## Ports utilisÃ©s

| Port | Service | Description |
|------|---------|-------------|
| 8080 | Dashboard | Interface utilisateur Vue.js |
| 8585 | Backend API | API REST et WebSocket |
| 11434 | Ollama Proxy | Compatible avec clients Ollama |
| 1234 | LM Studio Proxy | Compatible avec clients LM Studio |
| 8035 | llama-server | Interface Web directe (optionnelle) |

## Mise Ã  jour de configuration

- **Ancien systÃ¨me** : Configuration via menus PowerShell
- **Nouveau systÃ¨me** : Configuration via l'interface du dashboard
- **Avantages** :
  - Interface graphique intuitive
  - Validation en temps rÃ©el
  - Historique des modifications
  - Sauvegarde et restauration automatiques

## Composants obsolÃ¨tes supprimÃ©s

- **Metrics Server** : RemplacÃ© par le dashboard Vue.js
- **Menus de configuration PowerShell** : DÃ©placÃ©s vers le dashboard

## DÃ©veloppement

Pour le dÃ©veloppement, exÃ©cutez :

1. Backend : `python main.py`
2. Dashboard : `cd dashboard && npm run dev`

Le dashboard utilise un proxy pour accÃ©der aux API backend pendant le dÃ©veloppement.
